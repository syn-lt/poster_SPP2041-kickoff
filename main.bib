
@article{Butz2014,
  title = {Homeostatic Structural Plasticity Increases the Efficiency of Small-World Networks},
  volume = {6},
  issn = {1663-3563},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3978244/},
  doi = {10.3389/fnsyn.2014.00007},
  abstract = {In networks with small-world topology, which are characterized by a high clustering coefficient and a short characteristic path length, information can be transmitted efficiently and at relatively low costs. The brain is composed of small-world networks, and evolution may have optimized brain connectivity for efficient information processing. Despite many studies on the impact of topology on information processing in neuronal networks, little is known about the development of network topology and the emergence of efficient small-world networks. We investigated how a simple growth process that favors short-range connections over long-range connections in combination with a synapse formation rule that generates homeostasis in post-synaptic firing rates shapes neuronal network topology. Interestingly, we found that small-world networks benefited from homeostasis by an increase in efficiency, defined as the averaged inverse of the shortest paths through the network. Efficiency particularly increased as small-world networks approached the desired level of electrical activity. Ultimately, homeostatic small-world networks became almost as efficient as random networks. The increase in efficiency was caused by the emergent property of the homeostatic growth process that neurons started forming more long-range connections, albeit at a low rate, when their electrical activity was close to the homeostatic set-point. Although global network topology continued to change when neuronal activities were around the homeostatic equilibrium, the small-world property of the network was maintained over the entire course of development. Our results may help understand how complex systems such as the brain could set up an efficient network topology in a self-organizing manner. Insights from our work may also lead to novel techniques for constructing large-scale neuronal networks by self-organization.},
  journaltitle = {Frontiers in Synaptic Neuroscience},
  shortjournal = {Front Synaptic Neurosci},
  urldate = {2014-10-06},
  date = {2014-04-01},
  author = {Butz, Markus and Steenbuck, Ines D. and van Ooyen, Arjen},
  options = {useprefix=true},
  file = {/home/fh/lib/articles/Butz2014.pdf},
  eprinttype = {pmid},
  eprint = {24744727},
  pmcid = {PMC3978244}
}

@article{Vogels2011,
  langid = {english},
  title = {Inhibitory {{Plasticity Balances Excitation}} and {{Inhibition}} in {{Sensory Pathways}} and {{Memory Networks}}},
  volume = {334},
  issn = {0036-8075, 1095-9203},
  url = {http://www.sciencemag.org/content/334/6062/1569},
  doi = {10.1126/science.1211095},
  abstract = {Cortical neurons receive balanced excitatory and inhibitory synaptic currents. Such a balance could be established and maintained in an experience-dependent manner by synaptic plasticity at inhibitory synapses. We show that this mechanism provides an explanation for the sparse firing patterns observed in response to natural stimuli and fits well with a recently observed interaction of excitatory and inhibitory receptive field plasticity. The introduction of inhibitory plasticity in suitable recurrent networks provides a homeostatic mechanism that leads to asynchronous irregular network states. Further, it can accommodate synaptic memories with activity patterns that become indiscernible from the background state but can be reactivated by external stimuli. Our results suggest an essential role of inhibitory plasticity in the formation and maintenance of functional cortical circuitry.},
  number = {6062},
  journaltitle = {Science},
  shortjournal = {Science},
  urldate = {2014-10-06},
  date = {2011-12-16},
  pages = {1569--1573},
  author = {Vogels, T. P. and Sprekeler, H. and Zenke, F. and Clopath, C. and Gerstner, W.},
  file = {/home/fh/lib/articles/Vogels2011.pdf},
  eprinttype = {pmid},
  eprint = {22075724}
}

@article{Kuljis2010,
  title = {Integrative Understanding of Emergent Brain Properties, Quantum Brain Hypotheses, and Connectome Alterations in Dementia Are Key Challenges to Conquer {{Alzheimer}}'s Disease},
  volume = {1},
  url = {http://journal.frontiersin.org/Journal/10.3389/fneur.2010.00015/full},
  doi = {10.3389/fneur.2010.00015},
  abstract = {The biological substrate for cognition remains a challenge as much as defining this function of living beings. Here, we examine some of the difficulties to understand normal and disordered cognition in humans. We use aspects of Alzheimer's disease and related disorders to illustrate how the wealth of information at many conceptually separate, even intellectually decoupled, physical scales \textendash{} in particular at the Molecular Neuroscience versus Systems Neuroscience/Neuropsychology levels \textendash{} presents a challenge in terms of true interdisciplinary integration towards a coherent understanding. These unresolved dilemmas include critically the as yet untested quantum brain hypothesis, and the embryonic attempts to develop and define the so-called connectome in humans and in non-human models of disease. To mitigate these challenges, we propose a scheme incorporating the vast array of scales of the space and time (space\textendash{}time) manifold from at least the subatomic through cognitive-behavioral dimensions of inquiry, to achieve a new understanding of both normal and disordered cognition, that is essential for a new era of progress in the Generative Sciences and its application to translational efforts for disease prevention and treatment.},
  journaltitle = {Dementia},
  shortjournal = {Front. Neur},
  urldate = {2014-11-21},
  date = {2010},
  pages = {15},
  keywords = {Connectome,Alzheimer’s disease,cognition,mesoscale,quantum brain},
  author = {Kulji{\v s}, Rodrigo O.},
  file = {/home/fh/lib/articles/Kuljiš2010.pdf}
}

@online{zotero-null-4,
  title = {{{LaTeX}}-Examples},
  url = {https://github.com/MartinThoma/LaTeX-examples},
  abstract = {LaTeX-examples - Examples for the usage of LaTeX},
  journaltitle = {GitHub},
  urldate = {2014-01-21}
}

@article{Park2013,
  langid = {english},
  title = {Structural and {{Functional Brain Networks}}: {{From Connections}} to {{Cognition}}},
  volume = {342},
  issn = {0036-8075, 1095-9203},
  url = {http://www.sciencemag.org/content/342/6158/1238411},
  doi = {10.1126/science.1238411},
  shorttitle = {Structural and {{Functional Brain Networks}}},
  abstract = {How rich functionality emerges from the invariant structural architecture of the brain remains a major mystery in neuroscience. Recent applications of network theory and theoretical neuroscience to large-scale brain networks have started to dissolve this mystery. Network analyses suggest that hierarchical modular brain networks are particularly suited to facilitate local (segregated) neuronal operations and the global integration of segregated functions. Although functional networks are constrained by structural connections, context-sensitive integration during cognition tasks necessarily entails a divergence between structural and functional networks. This degenerate (many-to-one) function-structure mapping is crucial for understanding the nature of brain networks. The emergence of dynamic functional networks from static structural connections calls for a formal (computational) approach to neuronal information processing that may resolve this dialectic between structure and function.
Background The human brain presents a puzzling and challenging paradox: Despite a fixed anatomy, characterized by its connectivity, its functional repertoire is vast, enabling action, perception, and cognition. This contrasts with organs like the heart that have a dynamic anatomy but just one function. The resolution of this paradox may reside in the brain's network architecture, which organizes local interactions to cope with diverse environmental demands\textemdash{}ensuring adaptability, robustness, resilience to damage, efficient message passing, and diverse functionality from a fixed structure. This review asks how recent advances in understanding brain networks elucidate the brain's many-to-one (degenerate) function-structure relationships. In other words, how does diverse function arise from an apparently static neuronal architecture? We conclude that the emergence of dynamic functional connectivity, from static structural connections, calls for formal (computational) approaches to neuronal information processing that may resolve the dialectic between structure and function.
Schematic of the multiscale hierarchical organization of brain networks. Brain function or cognition can be described as the global integration of local (segregated) neuronal operations that underlies hierarchical message passing among cortical areas, and which is facilitated by hierarchical modular network architectures.
Advances Much of our understanding of brain connectivity rests on the way that it is measured and modeled. We consider two complementary approaches: the first has its basis in graph theory that aims to describe the network topology of (undirected) connections of the sort measured by noninvasive brain imaging of anatomical connections and functional connectivity (correlations) between remote sites. This is compared with model-based definitions of context-sensitive (directed) effective connectivity that are grounded in the biophysics of neuronal interactions. Recent topological network analyses of brain circuits suggest that modular and hierarchical structural networks are particularly suited for the functional integration of local (functionally specialized) neuronal operations that underlie cognition. Measurements of spontaneous activity reveal functional connectivity patterns that are similar to structural connectivity, suggesting that structural networks constrain functional networks. However, task-related responses that require context-sensitive integration disclose a divergence between function and structure that appears to rest mainly on long-range connections. In contrast to methods that describe network topology phenomenologically, model-based theoretical and computational approaches focus on the mechanisms of neuronal interactions that accommodate the dynamic reconfiguration of effective connectivity. We highlight the consilience between hierarchical topologies (based on structural and functional connectivity) and the effective connectivity that would be required for hierarchical message passing of the sort suggested by computational neuroscience.
Outlook In summary, neuronal interactions represent dynamics on a fixed structural connectivity that underlie cognition and behavior. Such divergence of function from structure is, perhaps, the most intriguing property of the brain and invites intensive future research. By studying the dynamics and self-organization of functional networks, we may gain insight into the true nature of the brain as the embodiment of the mind. The repertoire of functional networks rests upon the (hidden) structural architecture of connections that enables hierarchical functional integration. Understanding these networks will require theoretical models of neuronal processing that underlies cognition.},
  number = {6158},
  journaltitle = {Science},
  shortjournal = {Science},
  urldate = {2014-06-16},
  date = {2013-01-11},
  pages = {1238411},
  author = {Park, Hae-Jeong and Friston, Karl},
  file = {/home/fh/lib/articles/Park2013.pdf},
  eprinttype = {pmid},
  eprint = {24179229}
}

@article{Pernice2011,
  title = {How {{Structure Determines Correlations}} in {{Neuronal Networks}}},
  volume = {7},
  url = {http://dx.doi.org/10.1371/journal.pcbi.1002059},
  doi = {10.1371/journal.pcbi.1002059},
  abstract = {Author Summary
Many biological systems have been described as networks whose complex properties influence the behaviour of the system. Correlations of activity in such networks are of interest in a variety of fields, from gene-regulatory networks to neuroscience. Due to novel experimental techniques allowing the recording of the activity of many pairs of neurons and their importance with respect to the functional interpretation of spike data, spike train correlations in neural networks have recently attracted a considerable amount of attention. Although origin and function of these correlations is not known in detail, they are believed to have a fundamental influence on information processing and learning. We present a detailed explanation of how recurrent connectivity induces correlations in local neural networks and how structural features affect their size and distribution. We examine under which conditions network characteristics like distance dependent connectivity, hubs or patches markedly influence correlations and population signals.},
  number = {5},
  journaltitle = {PLoS Comput Biol},
  shortjournal = {PLoS Comput Biol},
  urldate = {2014-01-23},
  date = {2011-05-19},
  pages = {e1002059},
  author = {Pernice, Volker and Staude, Benjamin and Cardanobile, Stefano and Rotter, Stefan},
  file = {/home/fh/lib/articles/Pernice2011.pdf}
}

@book{Bang-Jensen2002,
  langid = {english},
  title = {Digraphs},
  isbn = {1-85233-611-0},
  publisher = {{Springer}},
  date = {2002-08-05},
  author = {Bang-Jensen, Jorgen and Gutin, Gregory Z. and Gutin, Gregory},
  file = {/home/fh/lib/books/Bang-Jensen2002_Digraphs.pdf}
}

@article{Roxin2011a,
  title = {The {{Role}} of {{Degree Distribution}} in {{Shaping}} the {{Dynamics}} in {{Networks}} of {{Sparsely Connected Spiking Neurons}}},
  volume = {5},
  issn = {1662-5188},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3058136/},
  doi = {10.3389/fncom.2011.00008},
  abstract = {Neuronal network models often assume a fixed probability of connection between neurons. This assumption leads to random networks with binomial in-degree and out-degree distributions which are relatively narrow. Here I study the effect of broad degree distributions on network dynamics by interpolating between a binomial and a truncated power-law distribution for the in-degree and out-degree independently. This is done both for an inhibitory network (I network) as well as for the recurrent excitatory connections in a network of excitatory and inhibitory neurons (EI network). In both cases increasing the width of the in-degree distribution affects the global state of the network by driving transitions between asynchronous behavior and oscillations. This effect is reproduced in a simplified rate model which includes the heterogeneity in neuronal input due to the in-degree of cells. On the other hand, broadening the out-degree distribution is shown to increase the fraction of common inputs to pairs of neurons. This leads to increases in the amplitude of the cross-correlation (CC) of synaptic currents. In the case of the I network, despite strong oscillatory CCs in the currents, CCs of the membrane potential are low due to filtering and reset effects, leading to very weak CCs of the spike-count. In the asynchronous regime of the EI network, broadening the out-degree increases the amplitude of CCs in the recurrent excitatory currents, while CC of the total current is essentially unaffected as are pairwise spiking correlations. This is due to a dynamic balance between excitatory and inhibitory synaptic currents. In the oscillatory regime, changes in the out-degree can have a large effect on spiking correlations and even on the qualitative dynamical state of the network.},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front Comput Neurosci},
  urldate = {2013-12-18},
  date = {2011-03-08},
  author = {Roxin, Alex},
  file = {/home/fh/lib/articles/Roxin2011.pdf},
  eprinttype = {pmid},
  eprint = {21556129},
  pmcid = {PMC3058136}
}

@book{Mardia2000a,
  langid = {english},
  location = {{Chichester; New York}},
  title = {Directional Statistics},
  isbn = {0-471-95333-4 978-0-471-95333-3},
  publisher = {{J. Wiley}},
  date = {2000},
  author = {Mardia, K. V and Jupp, Peter E},
  file = {/home/fh/lib/books/Mardia2000_Directional-statistics.pdf}
}

@article{Song2005,
  title = {Highly {{Nonrandom Features}} of {{Synaptic Connectivity}} in {{Local Cortical Circuits}}},
  volume = {3},
  url = {http://dx.doi.org/10.1371/journal.pbio.0030068},
  doi = {10.1371/journal.pbio.0030068},
  abstract = {A dataset of hundreds of recordings in which four neurons were simultaneously monitored reveals clustered connectivity patterns among cortical neurons.},
  number = {3},
  journaltitle = {PLoS Biol},
  shortjournal = {PLoS Biol},
  urldate = {2013-12-18},
  date = {2005-03-01},
  pages = {e68},
  keywords = {bidirs},
  author = {Song, Sen and Sj{\"o}str{\"o}m, Per Jesper and Reigl, Markus and Nelson, Sacha and Chklovskii, Dmitri B},
  file = {/home/fh/lib/articles/Song2005.pdf}
}

@online{zotero-null-14,
  title = {Python, Extract File Name from Path, No Matter What the Os/Path Format},
  url = {http://stackoverflow.com/questions/8384737/python-extract-file-name-from-path-no-matter-what-the-os-path-format},
  abstract = {Which Python library can I use to extract file names from paths, no matter what the operating system or path format could be ?

For example, I'd like all of these paths to return me "c" : 

a/b/c/
...},
  urldate = {2014-05-11}
}

@article{Ko2011,
  langid = {english},
  title = {Functional Specificity of Local Synaptic Connections in Neocortical Networks},
  volume = {473},
  issn = {0028-0836},
  url = {http://www.nature.com/nature/journal/v473/n7345/full/nature09880.html?WT.ec_id=NATURE-20110505},
  doi = {10.1038/nature09880},
  abstract = {Neuronal connectivity is fundamental to information processing in the brain. Therefore, understanding the mechanisms of sensory processing requires uncovering how connection patterns between neurons relate to their function. On a coarse scale, long-range projections can preferentially link cortical regions with similar responses to sensory stimuli. But on the local scale, where dendrites and axons overlap substantially, the functional specificity of connections remains unknown. Here we determine synaptic connectivity between nearby layer 2/3 pyramidal neurons in vitro, the response properties of which were first characterized in mouse visual cortex in vivo. We found that connection probability was related to the similarity of visually driven neuronal activity. Neurons with the same preference for oriented stimuli connected at twice the rate of neurons with orthogonal orientation preferences. Neurons responding similarly to naturalistic stimuli formed connections at much higher rates than those with uncorrelated responses. Bidirectional synaptic connections were found more frequently between neuronal pairs with strongly correlated visual responses. Our results reveal the degree of functional specificity of local synaptic connections in the visual cortex, and point to the existence of fine-scale subnetworks dedicated to processing related sensory information.},
  number = {7345},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2014-05-23},
  date = {2011-05-05},
  pages = {87--91},
  keywords = {Neuroscience},
  author = {Ko, Ho and Hofer, Sonja B. and Pichler, Bruno and Buchanan, Katherine A. and Sj{\"o}str{\"o}m, P. Jesper and Mrsic-Flogel, Thomas D.},
  file = {/home/fh/lib/articles/Ko2011.pdf}
}

@article{Sporns2007,
  title = {Brain Connectivity},
  volume = {2},
  issn = {1941-6016},
  url = {http://www.scholarpedia.org/article/Brain_connectivity},
  doi = {10.4249/scholarpedia.4695},
  number = {10},
  journaltitle = {Scholarpedia},
  urldate = {2013-12-18},
  date = {2007},
  pages = {4695},
  author = {Sporns, Olaf}
}

@article{Petersen2003,
  langid = {english},
  title = {Spatiotemporal {{Dynamics}} of {{Sensory Responses}} in {{Layer}} 2/3 of {{Rat Barrel Cortex Measured In Vivo}} by {{Voltage}}-{{Sensitive Dye Imaging Combined}} with {{Whole}}-{{Cell Voltage Recordings}} and {{Neuron Reconstructions}}},
  volume = {23},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/23/4/1298},
  abstract = {The spatiotemporal dynamics of the sensory response in layer 2/3 of primary somatosensory cortex evoked by a single brief whisker deflection was investigated by simultaneous voltage-sensitive dye (VSD) imaging and whole-cell (WC) voltage recordings in the anesthetized rat combined with reconstructions of dendritic and axonal arbors of L2/3 pyramids. Single and dual WC recordings from pyramidal cells indicated a strong correlation between the local VSD population response and the simultaneously measured subthreshold postsynaptic potential changes in both amplitude and time course. The earliest VSD response was detected 10\textendash{}12 msec after whisker deflection centered above the barrel isomorphic to the stimulated principal whisker. It was restricted horizontally to the size of a single barrel-column coextensive with the dendritic arbor of barrel-column-related pyramids in L2/3. The horizontal spread of excitation remained confined to a single barrel-column with weak whisker deflection. With intermediate deflections, excitation spread into adjacent barrel-columns, propagating twofold more rapidly along the rows of the barrel field than across the arcs, consistent with the preferred axonal arborizations in L2/3 of reconstructed pyramidal neurons. Finally, larger whisker deflections evoked excitation spreading over the entire barrel field within $\sim$50 msec before subsiding over the next $\sim$250 msec. Thus the subthreshold cortical map representing a whisker deflection is dynamic on the millisecond time scale and strongly depends on stimulus strength. The sequential spatiotemporal activation of the excitatory neuronal network in L2/3 by a simple sensory stimulus can thus be accounted for primarily by the columnar restriction of L4 to L2/3 excitatory connections and the axonal field of barrel-related pyramids.},
  number = {4},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2014-03-09},
  date = {2003-02-15},
  pages = {1298--1309},
  keywords = {barrel cortex,imaging,in vivo,layer 2/3,sensory response,voltage-sensitive dye},
  author = {Petersen, Carl C. H. and Grinvald, Amiram and Sakmann, Bert},
  file = {/home/fh/lib/articles/Petersen2003.pdf},
  eprinttype = {pmid},
  eprint = {12598618}
}

@article{Perin2011,
  langid = {english},
  title = {A Synaptic Organizing Principle for Cortical Neuronal Groups},
  volume = {108},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/108/13/5419},
  doi = {10.1073/pnas.1016051108},
  abstract = {Neuronal circuitry is often considered a clean slate that can be dynamically and arbitrarily molded by experience. However, when we investigated synaptic connectivity in groups of pyramidal neurons in the neocortex, we found that both connectivity and synaptic weights were surprisingly predictable. Synaptic weights follow very closely the number of connections in a group of neurons, saturating after only 20\% of possible connections are formed between neurons in a group. When we examined the network topology of connectivity between neurons, we found that the neurons cluster into small world networks that are not scale-free, with less than 2 degrees of separation. We found a simple clustering rule where connectivity is directly proportional to the number of common neighbors, which accounts for these small world networks and accurately predicts the connection probability between any two neurons. This pyramidal neuron network clusters into multiple groups of a few dozen neurons each. The neurons composing each group are surprisingly distributed, typically more than 100 $\mu$m apart, allowing for multiple groups to be interlaced in the same space. In summary, we discovered a synaptic organizing principle that groups neurons in a manner that is common across animals and hence, independent of individual experiences. We speculate that these elementary neuronal groups are prescribed Lego-like building blocks of perception and that acquired memory relies more on combining these elementary assemblies into higher-order constructs.},
  number = {13},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  urldate = {2013-12-18},
  date = {2011-03-29},
  pages = {5419--5424},
  keywords = {Edelman,Hebb,brain development,cell assemblies,learning,bidirs},
  author = {Perin, Rodrigo and Berger, Thomas K. and Markram, Henry},
  file = {/home/fh/lib/articles/Perin2011_commentary.pdf;/home/fh/lib/articles/Perin2011_SI.pdf;/home/fh/lib/articles/Perin2011.pdf},
  eprinttype = {pmid},
  eprint = {21383177}
}

@article{Sporns2004,
  title = {Motifs in {{Brain Networks}}},
  volume = {2},
  url = {http://dx.doi.org/10.1371/journal.pbio.0020369},
  doi = {10.1371/journal.pbio.0020369},
  abstract = {Analysis of characteristic patterns of connectivity in neuroanatomical datasets suggests that nervous systems evolved to maximize functional repertoires and support highly efficient integration of information.},
  number = {11},
  journaltitle = {PLoS Biol},
  shortjournal = {PLoS Biol},
  urldate = {2013-12-18},
  date = {2004-10-26},
  pages = {e369},
  author = {Sporns, Olaf and K{\"o}tter, Rolf},
  file = {/home/fh/lib/articles/Sporns2004.pdf}
}

@article{Watt2010,
  title = {Homeostatic {{Plasticity}} and {{STDP}}: {{Keeping}} a {{Neuron}}'s {{Cool}} in a {{Fluctuating World}}},
  volume = {2},
  issn = {1663-3563},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3059670/},
  doi = {10.3389/fnsyn.2010.00005},
  shorttitle = {Homeostatic {{Plasticity}} and {{STDP}}},
  abstract = {Spike-timing-dependent plasticity (STDP) offers a powerful means of forming and modifying neural circuits. Experimental and theoretical studies have demonstrated its potential usefulness for functions as varied as cortical map development, sharpening of sensory receptive fields, working memory, and associative learning. Even so, it is unlikely that STDP works alone. Unless changes in synaptic strength are coordinated across multiple synapses and with other neuronal properties, it is difficult to maintain the stability and functionality of neural circuits. Moreover, there are certain features of early postnatal development (e.g., rapid changes in sensory input) that threaten neural circuit stability in ways that STDP may not be well placed to counter. These considerations have led researchers to investigate additional types of plasticity, complementary to STDP, that may serve to constrain synaptic weights and/or neuronal firing. These are collectively known as ``homeostatic plasticity'' and include schemes that control the total synaptic strength of a neuron, that modulate its intrinsic excitability as a function of average activity, or that make the ability of synapses to undergo Hebbian modification depend upon their history of use. In this article, we will review the experimental evidence for homeostatic forms of plasticity and consider how they might interact with STDP during development, and learning and memory.},
  journaltitle = {Frontiers in Synaptic Neuroscience},
  shortjournal = {Front Synaptic Neurosci},
  urldate = {2014-02-20},
  date = {2010-06-07},
  author = {Watt, Alanna J. and Desai, Niraj S.},
  file = {/home/fh/lib/articles/Watt2010.pdf},
  eprinttype = {pmid},
  eprint = {21423491},
  pmcid = {PMC3059670}
}

@book{Batschelet1981,
  langid = {english},
  location = {{London; New York}},
  title = {Circular Statistics in Biology},
  isbn = {0-12-081050-6 978-0-12-081050-5},
  publisher = {{Academic Press}},
  date = {1981},
  author = {Batschelet, Edward}
}

@article{Zalesky2012,
  title = {On the Use of Correlation as a Measure of Network Connectivity},
  volume = {60},
  issn = {1053-8119},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811912001784},
  doi = {10.1016/j.neuroimage.2012.02.001},
  abstract = {Numerous studies have demonstrated that brain networks derived from neuroimaging data have nontrivial topological features, such as small-world organization, modular structure and highly connected hubs. In these studies, the extent of connectivity between pairs of brain regions has often been measured using some form of statistical correlation. This article demonstrates that correlation as a measure of connectivity in and of itself gives rise to networks with non-random topological features. In particular, networks in which connectivity is measured using correlation are inherently more clustered than random networks, and as such are more likely to be small-world networks. Partial correlation as a measure of connectivity also gives rise to networks with non-random topological features. Partial correlation networks are inherently less clustered than random networks. Network measures in correlation networks should be benchmarked against null networks that respect the topological structure induced by correlation measurements. Prevalently used random rewiring algorithms do not yield appropriate null networks for some network measures. Null networks are proposed to explicitly normalize for the inherent topological structure found in correlation networks, resulting in more conservative estimates of small-world organization. A number of steps may be needed to normalize each network measure individually and control for distinct features (e.g. degree distribution). The main conclusion of this article is that correlation can and should be used to measure connectivity, however appropriate null networks should be used to benchmark network measures in correlation networks.},
  number = {4},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  urldate = {2014-02-03},
  date = {2012-05-01},
  pages = {2096--2106},
  keywords = {Brain connectivity,Connectome,Correlation,Partial correlation,Small-world network,Transitivity},
  author = {Zalesky, Andrew and Fornito, Alex and Bullmore, Ed},
  file = {/home/fh/lib/articles/Zalesky2012.pdf}
}

@article{Zhao2011,
  title = {Synchronization from {{Second Order Network Connectivity Statistics}}},
  volume = {5},
  issn = {1662-5188},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3134837/},
  doi = {10.3389/fncom.2011.00028},
  abstract = {We investigate how network structure can influence the tendency for a neuronal network to synchronize, or its synchronizability, independent of the dynamical model for each neuron. The synchrony analysis takes advantage of the framework of second order networks, which defines four second order connectivity statistics based on the relative frequency of two-connection network motifs. The analysis identifies two of these statistics, convergent connections, and chain connections, as highly influencing the synchrony. Simulations verify that synchrony decreases with the frequency of convergent connections and increases with the frequency of chain connections. These trends persist with simulations of multiple models for the neuron dynamics and for different types of networks. Surprisingly, divergent connections, which determine the fraction of shared inputs, do not strongly influence the synchrony. The critical role of chains, rather than divergent connections, in influencing synchrony can be explained by their increasing the effective coupling strength. The decrease of synchrony with convergent connections is primarily due to the resulting heterogeneity in firing rates.},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front Comput Neurosci},
  urldate = {2013-12-18},
  date = {2011-07-08},
  author = {Zhao, Liqiong and Beverlin, Bryce and Netoff, Theoden and Nykamp, Duane Q.},
  file = {/home/fh/lib/articles/Zhao2011.pdf},
  eprinttype = {pmid},
  eprint = {21779239},
  pmcid = {PMC3134837}
}

@article{Kriener2009,
  langid = {english},
  title = {Correlations in Spiking Neuronal Networks with Distance Dependent Connections},
  volume = {27},
  issn = {0929-5313, 1573-6873},
  url = {http://link.springer.com/article/10.1007/s10827-008-0135-1},
  doi = {10.1007/s10827-008-0135-1},
  abstract = {Can the topology of a recurrent spiking network be inferred from observed activity dynamics? Which statistical parameters of network connectivity can be extracted from firing rates, correlations and related measurable quantities? To approach these questions, we analyze distance dependent correlations of the activity in small-world networks of neurons with current-based synapses derived from a simple ring topology. We find that in particular the distribution of correlation coefficients of subthreshold activity can tell apart random networks from networks with distance dependent connectivity. Such distributions can be estimated by sampling from random pairs. We also demonstrate the crucial role of the weight distribution, most notably the compliance with Dales principle, for the activity dynamics in recurrent networks of different types.},
  number = {2},
  journaltitle = {Journal of Computational Neuroscience},
  shortjournal = {J Comput Neurosci},
  urldate = {2014-01-21},
  date = {2009-10-01},
  pages = {177--200},
  keywords = {Distribution of correlation coefficients,Human Genetics,Neurology,Neurosciences,Pairwise correlations,Small-world networks,Spiking neural networks,Theory of Computation},
  author = {Kriener, Birgit and Helias, Moritz and Aertsen, Ad and Rotter, Stefan},
  file = {/home/fh/lib/articles/Kriener2009.pdf}
}

@article{Weckstrom2010,
  title = {Intracellular Recording},
  volume = {5},
  issn = {1941-6016},
  url = {http://www.scholarpedia.org/article/Intracellular_recording},
  doi = {10.4249/scholarpedia.2224},
  number = {8},
  journaltitle = {Scholarpedia},
  urldate = {2013-12-18},
  date = {2010},
  pages = {2224},
  author = {Weckstrom, Matti}
}

@book{Janson2000a,
  langid = {english},
  location = {{New York; Chichester}},
  title = {Theory of Random Graphs},
  isbn = {0-471-17541-2 978-0-471-17541-4},
  publisher = {{John Wiley \& Sons}},
  date = {2000},
  author = {Janson, Svante and {\L}uczak, Tomasz and Ruci{\'n}ski, Andrzej}
}

@book{Peters1984,
  langid = {english},
  location = {{New York}},
  title = {Cerebral Cortex},
  isbn = {0-306-41544-5 978-0-306-41544-9 0-306-43635-3 978-0-306-43635-2 0-306-45727-X 978-0-306-45727-2 0-306-44605-7 978-0-306-44605-4 0-306-45530-7 978-0-306-45530-8},
  publisher = {{Plenum Press}},
  date = {1984},
  author = {Peters, Alan and Jones, Edward G}
}

@article{Dong2007,
  title = {Random Graph Theory Based Connectivity Analysis in Wireless Sensor Networks with {{Rayleigh}} Fading Channels},
  doi = {10.1109/APCC.2007.4433515},
  abstract = {Connectivity is an essential merit of wireless sensor networks. There has been great interest in exploring the minimum density of sensor nodes that is needed to achieve a connected wireless network. This becomes difficult when uncertain features increase, such as Rayleigh fading channels. In this paper, we describe a range-dependent model for sensor networks by using random graph theory, and study the connectivity problem with this model. We calculate the probability of an arbitrary node being isolated, and thus obtain the probability of the whole network being connected. By giving the required minimum density, our work can guide in designing of the wireless sensor networks with fading channels. Moreover, the numerical results shows that the fading effect would degrade the connectivity of the wireless sensor networks.},
  journaltitle = {Asia-Pacific Conference on Communications, 2007. APCC 2007},
  date = {2007},
  pages = {123--126},
  keywords = {Degradation,Fading,Lattices,Mobile communication,Monitoring,Probability,Rayleigh channels,Rayleigh fading channels,Sensor phenomena and characterization,Shadow mapping,arbitrary node probability,graph theory,random graph theory,range-dependent model,sensor nodes,wireless sensor networks},
  author = {Dong, Jingbo and Chen, Qing and Niu, Zhisheng},
  file = {/home/fh/lib/articles/Dong2007.pdf}
}

@book{Braitenberg1998,
  langid = {english},
  location = {{Berlin; New York}},
  title = {Cortex: Statistics and Geometry of Neuronal Connectivity},
  edition = {2nd edition},
  isbn = {3-540-63816-4 978-3-540-63816-2},
  shorttitle = {Cortex},
  publisher = {{Springer}},
  date = {1998},
  author = {Braitenberg, Valentino and Sch{\"u}z, A and Braitenberg, Valentino},
  file = {/home/fh/lib/books/Braitenberg1998_Cortex-statistics-and-geometry-of-neuronal-connectivity.pdf}
}

@book{Brette2012,
  langid = {english},
  location = {{Cambridge}},
  title = {Handbook of Neural Activity Measurement},
  isbn = {978-1-139-54906-6 1-139-54906-5 1-139-55156-6 978-1-139-55156-4 978-0-511-97995-8 0-511-97995-9 1-139-55402-6 978-1-139-55402-2},
  abstract = {"Neuroscientists employ many different techniques to observe the activity of the brain, from single-channel recording to functional imaging (fMRI). Many practical books explain how to use these techniques, but in order to extract meaningful information from the results it is necessary to understand the physical and mathematical principles underlying each measurement. This book covers an exhaustive range of techniques, with each chapter focusing on one in particular. Each author, a leading expert, explains exactly which quantity is being measured, the underlying principles at work, and most importantly the precise relationship between the signals measured and neural activity. The book is an important reference for neuroscientists who use these techniques in their own experimental protocols and need to interpret their results precisely; for computational neuroscientists who use such experimental results in their models; and for scientists who want to develop new measurement techniques or enhance existing ones"--Provided by publisher.},
  publisher = {{Cambridge University Press}},
  date = {2012},
  author = {Brette, Romain and Destexhe, Alain},
  file = {/home/fh/lib/books/Brette2012_Handbook-of-neural-activity-measurement.pdf}
}

@book{Burgi2008,
  langid = {english},
  title = {Structure {{Correlation}}},
  isbn = {978-3-527-61608-4},
  abstract = {This book leaves the conventional view of chemical structures far behind: it demonstrates how a wealth of valuable, but hitherto unused information can be extracted from available structural data. For example, a single structure determination does not reveal much about a reaction pathway, but a sufficiently large number of comparable structures does. Finding the 'right' question is as important as is the intelligent use of crystallographic databases.Contributions by F.H. Allen, T.L. Blundell, I.D. Brown, H.B. B{\"u}rgi, J.D. Dunitz, L. Leiserowitz and others, authoritatively discuss the structure correlation method as well as illustrative results in detail, covering such apparently unrelated subjects as * Bond strength relations in soldis* Crystal structure prediction* Reaction pathways of organic molecules* Ligand/receptor interactions and enzyme mechanismsThis book will be useful to the academic and industrial reader alike. It offers both fundamental aspects and diverse applications of what will surely become a powerful branch of structural chemistry.},
  pagetotal = {922},
  publisher = {{John Wiley \& Sons}},
  date = {2008-07-11},
  keywords = {Science / Chemistry / Physical & Theoretical},
  author = {B{\"u}rgi, Hans-Beat and Dunitz, Jack D.}
}

@article{Rickert2009,
  langid = {english},
  title = {Dynamic {{Encoding}} of {{Movement Direction}} in {{Motor Cortical Neurons}}},
  volume = {29},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/29/44/13870},
  doi = {10.1523/JNEUROSCI.5441-08.2009},
  abstract = {When we perform a skilled movement such as reaching for an object, we can make use of prior information, for example about the location of the object in space. This helps us to prepare the movement, and we gain improved accuracy and speed during movement execution. Here, we investigate how prior information affects the motor cortical representation of movements during preparation and execution. We trained two monkeys in a delayed reaching task and provided a varying degree of prior information about the final target location. We decoded movement direction from multiple single-unit activity recorded from M1 (primary motor cortex) in one monkey and from PMd (dorsal premotor cortex) in a second monkey. Our results demonstrate that motor cortical cells in both areas exhibit individual encoding characteristics that change dynamically in time and dependent on prior information. On the population level, the information about movement direction is at any point in time accurately represented in a neuronal ensemble of time-varying composition. We conclude that movement representation in the motor cortex is not a static one, but one in which neurons dynamically allocate their computational resources to meet the demands defined by the movement task and the context of the movement. Consequently, we find that the decoding accuracy decreases if the precise task time, or the previous information that was available to the monkey, were disregarded in the decoding process. An optimal strategy for the readout of movement parameters from motor cortex should therefore take into account time and contextual parameters.},
  number = {44},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2013-12-18},
  date = {2009-04-11},
  pages = {13870--13882},
  author = {Rickert, J{\"o}rn and Riehle, Alexa and Aertsen, Ad and Rotter, Stefan and Nawrot, Martin P.},
  file = {/home/fh/lib/articles/Rickert2009.pdf},
  eprinttype = {pmid},
  eprint = {19889998}
}

@book{Fisher1995,
  langid = {english},
  location = {{Cambridge [u.a.}},
  title = {Statistical Analysis of Circular Data},
  isbn = {0-521-35018-2 978-0-521-35018-1 0-521-56890-0 978-0-521-56890-6},
  publisher = {{Univ. Press}},
  date = {1995},
  author = {Fisher, Nicholas I},
  file = {/home/fh/lib/books/Fisher1995_Statistical-analysis-of-circular-data.pdf}
}

@article{Gilbert1959,
  langid = {english},
  title = {Random {{Graphs}}},
  volume = {30},
  issn = {0003-4851, 2168-8990},
  url = {http://projecteuclid.org/euclid.aoms/1177706098},
  doi = {10.1214/aoms/1177706098},
  abstract = {Project Euclid - mathematics and statistics online},
  number = {4},
  journaltitle = {The Annals of Mathematical Statistics},
  shortjournal = {Ann. Math. Statist.},
  urldate = {2014-01-23},
  date = {1959-12},
  pages = {1141--1144},
  author = {Gilbert, E. N.},
  file = {/home/fh/lib/articles/Gilbert1959.pdf},
  note = {Mathematical Reviews number (MathSciNet) MR108839, Zentralblatt MATH identifier0168.40801}
}

@article{Costa2013,
  title = {Probabilistic Inference of Short-Term Synaptic Plasticity in Neocortical Microcircuits},
  volume = {7},
  issn = {1662-5188},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3674479/},
  doi = {10.3389/fncom.2013.00075},
  abstract = {Short-term synaptic plasticity is highly diverse across brain area, cortical layer, cell type, and developmental stage. Since short-term plasticity (STP) strongly shapes neural dynamics, this diversity suggests a specific and essential role in neural information processing. Therefore, a correct characterization of short-term synaptic plasticity is an important step towards understanding and modeling neural systems. Phenomenological models have been developed, but they are usually fitted to experimental data using least-mean-square methods. We demonstrate that for typical synaptic dynamics such fitting may give unreliable results. As a solution, we introduce a Bayesian formulation, which yields the posterior distribution over the model parameters given the data. First, we show that common STP protocols yield broad distributions over some model parameters. Using our result we propose a experimental protocol to more accurately determine synaptic dynamics parameters. Next, we infer the model parameters using experimental data from three different neocortical excitatory connection types. This reveals connection-specific distributions, which we use to classify synaptic dynamics. Our approach to demarcate connection-specific synaptic dynamics is an important improvement on the state of the art and reveals novel features from existing data.},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front Comput Neurosci},
  urldate = {2014-11-04},
  date = {2013-06-06},
  author = {Costa, Rui P. and Sjostrom, P. Jesper and van Rossum, Mark C. W.},
  options = {useprefix=true},
  file = {/home/fh/lib/articles/Costa2013.pdf},
  eprinttype = {pmid},
  eprint = {23761760},
  pmcid = {PMC3674479}
}

@article{Clopath2010,
  langid = {english},
  title = {Connectivity Reflects Coding: A Model of Voltage-Based {{STDP}} with Homeostasis},
  volume = {13},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v13/n3/full/nn.2479.html#/supplementary-information},
  doi = {10.1038/nn.2479},
  shorttitle = {Connectivity Reflects Coding},
  abstract = {Electrophysiological connectivity patterns in cortex often have a few strong connections, which are sometimes bidirectional, among a lot of weak connections. To explain these connectivity patterns, we created a model of spike timing\textendash{}dependent plasticity (STDP) in which synaptic changes depend on presynaptic spike arrival and the postsynaptic membrane potential, filtered with two different time constants. Our model describes several nonlinear effects that are observed in STDP experiments, as well as the voltage dependence of plasticity. We found that, in a simulated recurrent network of spiking neurons, our plasticity rule led not only to development of localized receptive fields but also to connectivity patterns that reflect the neural code. For temporal coding procedures with spatio-temporal input correlations, strong connections were predominantly unidirectional, whereas they were bidirectional under rate-coded input with spatial correlations only. Thus, variable connectivity patterns in the brain could reflect different coding principles across brain areas; moreover, our simulations suggested that plasticity is fast.},
  number = {3},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2014-09-30},
  date = {2010-03},
  pages = {344--352},
  keywords = {STDP},
  author = {Clopath, Claudia and B{\"u}sing, Lars and Vasilaki, Eleni and Gerstner, Wulfram},
  file = {/home/fh/lib/articles/Clopath2010.pdf}
}

@book{Bear2006,
  langid = {english},
  location = {{Philadelphia, PA}},
  title = {Neuroscience: {{Exploring}} the {{Brain}}, 3rd {{Edition}}},
  edition = {3rd edition},
  isbn = {978-0-7817-6003-4},
  shorttitle = {Neuroscience},
  abstract = {Widely praised for its student-friendly style and exceptional artwork and pedagogy, Neuroscience: Exploring the Brain is a leading undergraduate textbook on the biology of the brain and the systems that underlie behavior. This edition provides increased coverage of taste and smell, circadian rhythms, brain development, and developmental disorders and includes new information on molecular mechanisms and functional brain imaging. Path of Discovery boxes, written by leading researchers, highlight major current discoveries. In addition, readers will be able to assess their knowledge of neuroanatomy with the Illustrated Guide to Human Neuroanatomy, which includes a perforated self-testing workbook.This edition's robust ancillary package includes a bound-in student CD-ROM, an Instructor's Resource CD-ROM, and resources online.},
  pagetotal = {928},
  publisher = {{Lippincott Williams and Wilkins}},
  date = {2006-02-07},
  author = {Bear, Mark F. and Connors, Barry W. and Paradiso, Michael A.},
  file = {/home/fh/lib/books/Bear2006_Neuroscience-Exploring-the-Brain,-3rd-Edition.pdf}
}

@article{Bi1998,
  langid = {english},
  title = {Synaptic {{Modifications}} in {{Cultured Hippocampal Neurons}}: {{Dependence}} on {{Spike Timing}}, {{Synaptic Strength}}, and {{Postsynaptic Cell Type}}},
  volume = {18},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/18/24/10464},
  shorttitle = {Synaptic {{Modifications}} in {{Cultured Hippocampal Neurons}}},
  abstract = {In cultures of dissociated rat hippocampal neurons, persistent potentiation and depression of glutamatergic synapses were induced by correlated spiking of presynaptic and postsynaptic neurons. The relative timing between the presynaptic and postsynaptic spiking determined the direction and the extent of synaptic changes. Repetitive postsynaptic spiking within a time window of 20 msec after presynaptic activation resulted in long-term potentiation (LTP), whereas postsynaptic spiking within a window of 20 msec before the repetitive presynaptic activation led to long-term depression (LTD). Significant LTP occurred only at synapses with relatively low initial strength, whereas the extent of LTD did not show obvious dependence on the initial synaptic strength. Both LTP and LTD depended on the activation of NMDA receptors and were absent in cases in which the postsynaptic neurons were GABAergic in nature. Blockade of L-type calcium channels with nimodipine abolished the induction of LTD and reduced the extent of LTP. These results underscore the importance of precise spike timing, synaptic strength, and postsynaptic cell type in the activity-induced modification of central synapses and suggest that Hebb's rule may need to incorporate a quantitative consideration of spike timing that reflects the narrow and asymmetric window for the induction of synaptic modification.},
  number = {24},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2014-10-06},
  date = {1998-12-15},
  pages = {10464--10472},
  keywords = {Hebbian,Hebb’s rule,LTD,LTP,cell culture,correlated-activity,hippocampal neurons,plasticity,spike timing,spiking,synaptic modification,target specificity},
  author = {Bi, Guo-qiang and Poo, Mu-ming},
  file = {/home/fh/lib/articles/Bi1998.pdf},
  eprinttype = {pmid},
  eprint = {9852584}
}

@collection{Kandel2012,
  langid = {english},
  location = {{New York}},
  title = {Principles of {{Neural Science}}, {{Fifth Edition}}},
  edition = {5th edition},
  isbn = {978-0-07-139011-8},
  abstract = {Now updated: the definitive neuroscience resource\textemdash{}from Eric R. Kandel, MD (winner of the Nobel Prize in 2000); James H. Schwartz, MD, PhD; Thomas M. Jessell, PhD; Steven A. Siegelbaum, PhD; and A. J. Hudspeth, PhD 900 full-color illustrations Deciphering the link between the human brain and behavior has always been one of the most intriguing\textemdash{}and often challenging\textemdash{}aspects of scientific endeavor. The sequencing of the human genome, and advances in molecular biology, have illuminated the pathogenesis of many neurological diseases and have propelled our knowledge of how the brain controls behavior.  To grasp the wider implications of these developments and gain a fundamental understanding of this dynamic, fast-moving field, Principles of Neuroscience stands alone as the most authoritative and indispensible resource of its kind.  In this classic text, prominent researchers in the field expertly survey the entire spectrum of neural science, giving an up-to-date, unparalleled view of the discipline for anyone who studies brain and mind. Here, in one remarkable volume, is the current state of neural science knowledge\textemdash{}ranging from molecules and cells, to anatomic structures and systems, to the senses and cognitive functions\textemdash{}all supported by more than 900 precise, full-color illustrations. In addition to clarifying complex topics, the book also benefits from a cohesive organization, beginning with an insightful overview of the interrelationships between the brain, nervous system, genes, and behavior. Principles of Neural Science then proceeds with an in-depth examination of the molecular and cellular biology of nerve cells, synaptic transmission, and the neural basis of cognition. The remaining sections illuminate how cells, molecules, and systems give us sight, hearing, touch, movement, thought, learning, memories, and emotions.  The new fifth edition of Principles of Neural Science is thoroughly updated to reflect the tremendous amount of research, and the very latest clinical perspectives, that have significantly transformed the field within the last decade.  Ultimately, Principles of Neural Science affirms that all behavior is an expression of neural activity, and that the future of clinical neurology and psychiatry hinges on the progress of neural science. Far exceeding the scope and scholarship of similar texts, this unmatched guide offers a commanding, scientifically rigorous perspective on the molecular mechanisms of neural function and disease\textemdash{}one that you'll continually rely on to advance your comprehension of brain, mind, and behavior.  FEATURES   The cornerstone reference in the field of neuroscience that explains how the nerves, brain, and mind function  Clear emphasis on how behavior can be examined through the electrical activity of both individual neurons and systems of nerve cells  Current focus on molecular biology as a tool for probing the pathogenesis of many neurological diseases, including muscular dystrophy, Huntington disease, and certain forms of Alzheimer's disease  More than 900 engaging full-color illustrations\textemdash{}including line drawings, radiographs, micrographs, and medical photographs clarify often-complex neuroscience concepts  Outstanding section on the development and emergence of behavior, including important coverage of brain damage repair, the sexual differentiation of the nervous system,  and the aging brain  NEW! More detailed discussions of cognitive and behavioral functions, and an expanded review of cognitive processes  NEW! A focus on the increasing importance of computational neural science, which enhances our ability to record the brain's electrical activity and study cognitive processes more directly  NEW! Chapter-opening Key Concepts provide a convenient, study-enhancing introduction to the material covered in each chapter  Selected Readings and full reference citations at the close of each chapter facilitate further study and research  Helpful appendices highlight basic circuit theory; the neurological examination of the patient; circulation of the brain; the blood-brain barrier, choroid plexus, and cerebrospinal fluid; neural networks; and theoretical approaches to neuroscience/ul$>$},
  pagetotal = {1760},
  publisher = {{McGraw-Hill Professional}},
  date = {2012-10-26},
  editor = {Kandel, Eric R. and Schwartz, James H. and Jessell, Thomas M. and Siegelbaum, Steven A. and Hudspeth, A. J.},
  file = {/home/fh/lib/books/Kandel2012_Principles-of-Neural-Science,-Fifth-Edition.epub;/home/fh/lib/books/Kandel2012_Principles-of-Neural-Science,-Fifth-Edition.pdf}
}

@article{Okun2008,
  langid = {english},
  title = {Instantaneous Correlation of Excitation and Inhibition during Ongoing and Sensory-Evoked Activities},
  volume = {11},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v11/n5/abs/nn.2105.html},
  doi = {10.1038/nn.2105},
  abstract = {Temporal and quantitative relations between excitatory and inhibitory inputs in the cortex are central to its activity, yet they remain poorly understood. In particular, a controversy exists regarding the extent of correlation between cortical excitation and inhibition. Using simultaneous intracellular recordings in pairs of nearby neurons in vivo, we found that excitatory and inhibitory inputs are continuously synchronized and correlated in strength during spontaneous and sensory-evoked activities in the rat somatosensory cortex.},
  number = {5},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2014-10-06},
  date = {2008-05},
  pages = {535--537},
  author = {Okun, Michael and Lampl, Ilan},
  file = {/home/fh/lib/articles/Okun2008.pdf}
}

@article{Sjostrom2001,
  title = {Rate, {{Timing}}, and {{Cooperativity Jointly Determine Cortical Synaptic Plasticity}}},
  volume = {32},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627301005426},
  doi = {10.1016/S0896-6273(01)00542-6},
  abstract = {Cortical long-term plasticity depends on firing rate, spike timing, and cooperativity among inputs, but how these factors interact during realistic patterns of activity is unknown. Here we monitored plasticity while systematically varying the rate, spike timing, and number of coincident afferents. These experiments demonstrate a novel form of cooperativity operating even when postsynaptic firing is evoked by current injection, and reveal a complex dependence of LTP and LTD on rate and timing. Based on these data, we constructed and tested three quantitative models of cortical plasticity. One of these models, in which spike-timing relationships causing LTP ``win'' out over those favoring LTD, closely fits the data and accurately predicts the build-up of plasticity during random firing. This provides a quantitative framework for predicting the impact of in vivo firing patterns on synaptic strength.},
  number = {6},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2014-10-05},
  date = {2001-12-20},
  pages = {1149--1164},
  author = {Sj{\"o}str{\"o}m, Per Jesper and Turrigiano, Gina G and Nelson, Sacha B},
  file = {/home/fh/lib/articles/Sjöström2001.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/BBEEGW2W/S0896627301005426.html}
}

@article{Bi2001,
  title = {Synaptic Modification by Correlated Activity : {{Hebb}}'s {{Postulate}} Revisited},
  volume = {24},
  url = {http://dx.doi.org/10.1146/annurev.neuro.24.1.139},
  doi = {10.1146/annurev.neuro.24.1.139},
  shorttitle = {{{SYNAPTIC MODIFICATION BY CORRELATED ACTIVITY}}},
  abstract = {Correlated spiking of pre- and postsynaptic neurons can result in strengthening or weakening of synapses, depending on the temporal order of spiking. Recent findings indicate that there are narrow and cell type\textendash{}specific temporal windows for such synaptic modification and that the generally accepted input- (or synapse-) specific rule for modification appears not to be strictly adhered to. Spike timing\textendash{}dependent modifications, together with selective spread of synaptic changes, provide a set of cellular mechanisms that are likely to be important for the development and functioning of neural networks. When an axon of cell A is near enough to excite cell B or repeatedly or consistently takes part in firing it, some growth or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased. Donald Hebb (1949)},
  number = {1},
  journaltitle = {Annual Review of Neuroscience},
  urldate = {2014-10-06},
  date = {2001},
  pages = {139--166},
  keywords = {Hebbian synapse,LTD,LTP,input specificity,spike timing},
  author = {Bi, Guo-qiang and Poo, Mu-ming},
  file = {/home/fh/lib/articles/Bi2001.pdf},
  eprinttype = {pmid},
  eprint = {11283308}
}

@article{Kleindienst2011,
  title = {Activity-{{Dependent Clustering}} of {{Functional Synaptic Inputs}} on {{Developing Hippocampal Dendrites}}},
  volume = {72},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627311009263},
  doi = {10.1016/j.neuron.2011.10.015},
  abstract = {Summary
During brain development, before sensory systems become functional, neuronal networks spontaneously generate repetitive bursts of neuronal activity, which are typically synchronized across many neurons. Such activity patterns have been described on the level of networks and cells, but the fine-structure of inputs received by an individual neuron during spontaneous network activity has not been studied. Here, we used calcium imaging to record activity at many synapses of hippocampal pyramidal neurons simultaneously to establish the activity patterns in the majority of synapses of an entire cell. Analysis of the spatiotemporal patterns of synaptic activity revealed a fine-scale connectivity rule: neighboring synapses (\&lt;16~$\mu$m intersynapse distance) are more likely to be coactive than synapses that are farther away from each other. Blocking spiking activity or NMDA receptor activation revealed that the clustering of synaptic inputs required neuronal activity, demonstrating a role of developmentally expressed spontaneous activity for connecting neurons with subcellular precision.},
  number = {6},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2014-10-06},
  date = {2011-12-22},
  pages = {1012--1024},
  author = {Kleindienst, Thomas and Winnubst, Johan and Roth-Alpermann, Claudia and Bonhoeffer, Tobias and Lohmann, Christian},
  file = {/home/fh/lib/articles/Kleindienst2011.pdf}
}

@article{Memmesheimer2014,
  title = {Learning {{Precisely Timed Spikes}}},
  volume = {82},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627314002566},
  doi = {10.1016/j.neuron.2014.03.026},
  abstract = {Summary
To signal the onset of salient sensory features or execute well-timed motor sequences, neuronal circuits must transform streams of incoming spike trains into precisely timed firing. To address the efficiency and fidelity with which neurons can perform such computations, we developed a theory to characterize the capacity of feedforward networks to generate desired spike sequences. We find the maximum number of desired output spikes a neuron can implement to be 0.1\textendash{}0.3 per synapse. We further present a biologically plausible learning rule that allows feedforward and recurrent networks to learn multiple mappings between inputs and desired spike sequences. We apply this framework to reconstruct synaptic weights from spiking activity and study the precision with which the temporal structure of ongoing behavior can be inferred from the spiking of premotor neurons. This work provides a powerful approach for characterizing the computational and learning capacities of single neurons and neuronal circuits.},
  number = {4},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2014-10-06},
  date = {2014-05-21},
  pages = {925--938},
  author = {Memmesheimer, Raoul-Martin and Rubin, Ran and {\"O}lveczky, Bence P. and Sompolinsky, Haim},
  file = {/home/fh/lib/articles/Memmesheimer2014.pdf}
}

@article{Froemke2007,
  langid = {english},
  title = {A Synaptic Memory Trace for Cortical Receptive Field Plasticity},
  volume = {450},
  issn = {0028-0836},
  url = {http://www.nature.com/nature/journal/v450/n7168/abs/nature06289.html},
  doi = {10.1038/nature06289},
  abstract = {Receptive fields of sensory cortical neurons are plastic, changing in response to alterations of neural activity or sensory experience. In this way, cortical representations of the sensory environment can incorporate new information about the world, depending on the relevance or value of particular stimuli. Neuromodulation is required for cortical plasticity, but it is uncertain how subcortical neuromodulatory systems, such as the cholinergic nucleus basalis, interact with and refine cortical circuits. Here we determine the dynamics of synaptic receptive field plasticity in the adult primary auditory cortex (also known as AI) using in vivo whole-cell recording. Pairing sensory stimulation with nucleus basalis activation shifted the preferred stimuli of cortical neurons by inducing a rapid reduction of synaptic inhibition within seconds, which was followed by a large increase in excitation, both specific to the paired stimulus. Although nucleus basalis was stimulated only for a few minutes, reorganization of synaptic tuning curves progressed for hours thereafter: inhibition slowly increased in an activity-dependent manner to rebalance the persistent enhancement of excitation, leading to a retuned receptive field with new preference for the paired stimulus. This restricted period of disinhibition may be a fundamental mechanism for receptive field plasticity, and could serve as a memory trace for stimuli or episodes that have acquired new behavioural significance.},
  number = {7168},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2014-10-06},
  date = {2007-11-15},
  pages = {425--429},
  author = {Froemke, Robert C. and Merzenich, Michael M. and Schreiner, Christoph E.},
  file = {/home/fh/lib/articles/Froemke2007.pdf}
}

@article{Butz2013,
  title = {A {{Simple Rule}} for {{Dendritic Spine}} and {{Axonal Bouton Formation Can Account}} for {{Cortical Reorganization}} after {{Focal Retinal Lesions}}},
  volume = {9},
  url = {http://dx.doi.org/10.1371/journal.pcbi.1003259},
  doi = {10.1371/journal.pcbi.1003259},
  abstract = {Author SummaryThe adult brain is less hard-wired than traditionally thought. About ten percent of synapses in the mature visual cortex is continually replaced by new ones (structural plasticity). This percentage greatly increases after lasting changes in visual input. Due to the topographically organized nerve connections from the retina in the eye to the primary visual cortex in the brain, a small circumscribed lesion in the retina leads to a defined area in the cortex that is deprived of input. Recent experimental studies have revealed that axonal sprouting and dendritic spine turnover are massively increased in and around the cortical area that is deprived of input. However, the driving forces for this structural plasticity remain unclear. Using a novel computational model, we examine whether the need for activity homeostasis of individual neurons may drive cortical reorganization after lasting changes in input activity. We show that homeostatic growth rules indeed give rise to structural and functional reorganization of neuronal networks similar to the cortical reorganization observed experimentally. Understanding the principles of structural plasticity may eventually lead to novel treatment strategies for stimulating functional reorganization after brain damage and neurodegeneration.},
  number = {10},
  journaltitle = {PLoS Comput Biol},
  shortjournal = {PLoS Comput Biol},
  urldate = {2014-10-05},
  date = {2013-10-10},
  pages = {e1003259},
  author = {Butz, Markus and van Ooyen, Arjen},
  options = {useprefix=true},
  file = {/home/fh/lib/articles/Butz2013.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/UIZ7AQAG/infodoi10.1371journal.pcbi.html}
}

@article{Kriener2008,
  title = {Correlations and {{Population Dynamics}} in {{Cortical Networks}}},
  volume = {20},
  issn = {0899-7667},
  url = {http://dx.doi.org/10.1162/neco.2008.02-07-474},
  doi = {10.1162/neco.2008.02-07-474},
  abstract = {The function of cortical networks depends on the collective interplay between neurons and neuronal populations, which is reflected in the correlation of signals that can be recorded at different levels. To correctly interpret these observations it is important to understand the origin of neuronal correlations. Here we study how cells in large recurrent networks of excitatory and inhibitory neurons interact and how the associated correlations affect stationary states of idle network activity. We demonstrate that the structure of the connectivity matrix of such networks induces considerable correlations between synaptic currents as well as between subthreshold membrane potentials, provided Dale's principle is respected. If, in contrast, synaptic weights are randomly distributed, input correlations can vanish, even for densely connected networks. Although correlations are strongly attenuated when proceeding from membrane potentials to action potentials (spikes), the resulting weak correlations in the spike output can cause substantial fluctuations in the population activity, even in highly diluted networks. We show that simple mean-field models that take the structure of the coupling matrix into account can adequately describe the power spectra of the population activity. The consequences of Dale's principle on correlations and rate fluctuations are discussed in the light of recent experimental findings.},
  number = {9},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2014-10-06},
  date = {2008-04-25},
  pages = {2185--2226},
  author = {Kriener, Birgit and Tetzlaff, Tom and Aertsen, Ad and Diesmann, Markus and Rotter, Stefan},
  file = {/home/fh/lib/articles/Kriener2008.pdf}
}

@article{Ko2013,
  langid = {english},
  title = {The Emergence of Functional Microcircuits in Visual Cortex},
  volume = {496},
  issn = {0028-0836},
  url = {http://www.nature.com/nature/journal/v496/n7443/abs/nature12015.html},
  doi = {10.1038/nature12015},
  abstract = {Sensory processing occurs in neocortical microcircuits in which synaptic connectivity is highly structured and excitatory neurons form subnetworks that process related sensory information. However, the developmental mechanisms underlying the formation of functionally organized connectivity in cortical microcircuits remain unknown. Here we directly relate patterns of excitatory synaptic connectivity to visual response properties of neighbouring layer 2/3 pyramidal neurons in mouse visual cortex at different postnatal ages, using two-photon calcium imaging in vivo and multiple whole-cell recordings in vitro. Although neural responses were already highly selective for visual stimuli at eye opening, neurons responding to similar visual features were not yet preferentially connected, indicating that the emergence of feature selectivity does not depend on the precise arrangement of local synaptic connections. After eye opening, local connectivity reorganized extensively: more connections formed selectively between neurons with similar visual responses and connections were eliminated between visually unresponsive neurons, but the overall connectivity rate did not change. We propose a sequential model of cortical microcircuit development based on activity-dependent mechanisms of plasticity whereby neurons first acquire feature preference by selecting feedforward inputs before the onset of sensory experience\textemdash{}a process that may be facilitated by early electrical coupling between neuronal subsets\textemdash{}and then patterned input drives the formation of functional subnetworks through a redistribution of recurrent synaptic connections.},
  number = {7443},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2014-10-06},
  date = {2013-04-04},
  pages = {96--100},
  keywords = {Cellular neuroscience,Striate cortex,Synaptic development},
  author = {Ko, Ho and Cossell, Lee and Baragli, Chiara and Antolik, Jan and Clopath, Claudia and Hofer, Sonja B. and Mrsic-Flogel, Thomas D.},
  file = {/home/fh/lib/articles/Ko2013.pdf}
}

@article{Pouget2013,
  langid = {english},
  title = {Probabilistic Brains: Knowns and Unknowns},
  volume = {16},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v16/n9/full/nn.3495.html},
  doi = {10.1038/nn.3495},
  shorttitle = {Probabilistic Brains},
  abstract = {There is strong behavioral and physiological evidence that the brain both represents probability distributions and performs probabilistic inference. Computational neuroscientists have started to shed light on how these probabilistic representations and computations might be implemented in neural circuits. One particularly appealing aspect of these theories is their generality: they can be used to model a wide range of tasks, from sensory processing to high-level cognition. To date, however, these theories have only been applied to very simple tasks. Here we discuss the challenges that will emerge as researchers start focusing their efforts on real-life computations, with a focus on probabilistic learning, structural learning and approximate inference.},
  number = {9},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2014-10-05},
  date = {2013-09},
  pages = {1170--1178},
  author = {Pouget, Alexandre and Beck, Jeffrey M. and Ma, Wei Ji and Latham, Peter E.},
  file = {/home/fh/lib/articles/Pouget2013.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/ZSZCM73F/nn.3495.html}
}

@article{Grabska-Barwinska2014,
  langid = {english},
  title = {How Well Do Mean Field Theories of Spiking Quadratic-Integrate-and-Fire Networks Work in Realistic Parameter Regimes?},
  volume = {36},
  issn = {0929-5313, 1573-6873},
  url = {http://link.springer.com/article/10.1007/s10827-013-0481-5},
  doi = {10.1007/s10827-013-0481-5},
  abstract = {We use mean field techniques to compute the distribution of excitatory and inhibitory firing rates in large networks of randomly connected spiking quadratic integrate and fire neurons. These techniques are based on the assumption that activity is asynchronous and Poisson. For most parameter settings these assumptions are strongly violated; nevertheless, so long as the networks are not too synchronous, we find good agreement between mean field prediction and network simulations. Thus, much of the intuition developed for randomly connected networks in the asynchronous regime applies to mildly synchronous networks.},
  number = {3},
  journaltitle = {Journal of Computational Neuroscience},
  shortjournal = {J Comput Neurosci},
  urldate = {2014-10-05},
  date = {2014-06-01},
  pages = {469--481},
  keywords = {Human Genetics,Neurology,Neurosciences,Theory of Computation,Mean field theory,Quadratic integrate and fire neuron,Random networks,Recurrent network,Synchronization,Theta neuron},
  author = {Grabska-Barwi{\'n}ska, Agnieszka and Latham, Peter E.},
  file = {/home/fh/lib/articles/Grabska-Barwińska2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/EQ8GSMBH/10.html}
}

@book{Izhikevich2010,
  langid = {english},
  location = {{Cambridge, Mass.; London}},
  title = {Dynamical {{Systems}} in {{Neuroscience}}: {{The Geometry}} of {{Excitability}} and {{Bursting}}},
  isbn = {978-0-262-51420-0},
  shorttitle = {Dynamical {{Systems}} in {{Neuroscience}}},
  abstract = {In order to model neuronal behavior or to interpret the results of                 modeling studies, neuroscientists must call upon methods of nonlinear dynamics. This                 book offers an introduction to nonlinear dynamical systems theory for researchers                 and graduate students in neuroscience. It also provides an overview of neuroscience                 for mathematicians who want to learn the basic facts of                 electrophysiology. Dynamical Systems in Neuroscience                 presents a systematic study of the relationship of electrophysiology,                 nonlinear dynamics, and computational properties of neurons. It emphasizes that                 information processing in the brain depends not only on the electrophysiological                 properties of neurons but also on their dynamical properties. The book introduces                 dynamical systems, starting with one- and two-dimensional Hodgkin-Huxley-type models                 and continuing to a description of bursting systems. Each chapter proceeds from the                 simple to the complex, and provides sample problems at the end. The book explains                 all necessary mathematical concepts using geometrical intuition; it includes many                 figures and few equations, making it especially suitable for non-mathematicians.                 Each concept is presented in terms of both neuroscience and mathematics, providing a                 link between the two disciplines.Nonlinear dynamical systems                 theory is at the core of computational neuroscience research, but it is not a                 standard part of the graduate neuroscience curriculum -- or taught by math or                 physics department in a way that is suitable for students of biology. This book                 offers neuroscience students and researchers a comprehensive account of concepts and                 methods increasingly used in computational neuroscience.An                 additional chapter on synchronization, with more advanced material, can be found at                 the author's website, www.izhikevich.com.},
  pagetotal = {464},
  publisher = {{The MIT Press}},
  date = {2010-01-22},
  author = {Izhikevich, Eugene M.},
  file = {/home/fh/lib/books/Izhikevich2010_Dynamical-Systems-in-Neuroscience-The-Geometry-of-Excitability-and-Bursting.pdf}
}

@article{Hilgetag2004,
  langid = {english},
  title = {Clustered Organization of Cortical Connectivity},
  volume = {2},
  issn = {1539-2791, 1559-0089},
  url = {http://link.springer.com/article/10.1385/NI%3A2%3A3%3A353},
  doi = {10.1385/NI:2:3:353},
  abstract = {Long-range corticocortical connectivity in mammalian brains possesses an intricate, nonrandom organization. Specifically, projections are arranged in `small-world' networks, forming clusters of cortical areas, which are closely linked among each other, but less frequently with areas in other clusters. In order to delineate the structure of cortical clusters and identify their members, we developed a computational approach based on evolutionary optimization. In different compilations of connectivity data for the cat and macaque monkey brain, the algorithm identified a small number of clusters that broadly agreed with functional cortical subdivisions. We propose a simple spatial growth model for evolving clustered connectivity, and discuss structural and functional implications of the clustered, small-world organization of cortical networks.},
  number = {3},
  journaltitle = {Neuroinformatics},
  shortjournal = {Neuroinform},
  urldate = {2015-01-23},
  date = {2004-09-01},
  pages = {353--360},
  keywords = {Neurology,Small-world networks,Biotechnology,Engineering; general,Rhesus macaque monkey,cat,cluster analysis,neural networks,cortical development,robustness,vulnerability,network function,scale-free networks,spatial growth},
  author = {Hilgetag, Claus C. and Kaiser, Marcus},
  file = {/home/fh/lib/articles/Hilgetag2004.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/NNQ3KQ9C/NI23353.html}
}

@article{Stepanyants2005,
  title = {Neurogeometry and Potential Synaptic Connectivity},
  volume = {28},
  issn = {0166-2236},
  url = {http://www.sciencedirect.com/science/article/pii/S0166223605001311},
  doi = {10.1016/j.tins.2005.05.006},
  abstract = {The advent of high-quality 3D reconstructions of neuronal arbors has revived the hope of inferring synaptic connectivity from the geometric shapes of axons and dendrites, or `neurogeometry'. A quantitative description of connectivity must be built on a sound theoretical framework. Here, we review recent developments in neurogeometry that can provide such a framework. We base the geometric description of connectivity on the concept of a `potential synapse' \textendash{} the close apposition between axons and dendrites necessary to form an actual synapse. In addition to describing potential synaptic connectivity in neuronal circuits, neurogeometry provides insight into basic features of functional connectivity, such as specificity and plasticity.},
  number = {7},
  journaltitle = {Trends in Neurosciences},
  shortjournal = {Trends in Neurosciences},
  urldate = {2014-12-09},
  date = {2005-07},
  pages = {387--394},
  author = {Stepanyants, Armen and Chklovskii, Dmitri B.},
  file = {/home/fh/lib/articles/Stepanyants2005.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/HPBWA3MW/S0166223605001311.html}
}

@misc{Hoffmann2014,
  title = {Structural and Dynamical Aspects of Neural Networks with Anisotropic Tissue Geometry},
  abstract = {Non-random connectivity has been repeatedly reported in cortical
networks, yet underlying connection principles of these patterns
remain elusive. Proposing an abstract geometric network model
reflecting stereotypical axonal and dendritic morphology of local
cortical layer 5 networks, we here investigate in how far anisotropy
in connectivity can constitute such an underlying connectivity
rule. Using a combination of analytical considerations and numerical
analysis, we find that while standard network measures and pair
connectivity remain unaffected, higher order connectivity is strongly
influenced by anisotropy, in many cases reflecting patterns found in
local cortical circuits. Presenting an abstract network model
featuring connectivity principles beyond distance-dependency, the
results shown here not only make a strong case for morphology-induced
rules as underlying connection principles of non-random patterns, but
may provide another step towards a network archetype greatly improving
upon the standard random model.},
  date = {2014-06-17},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/documents/Hoffmann2014.pdf}
}

@book{Filo2010,
  location = {{Hoboken, N.J}},
  title = {Information Processing by Biochemical Systems: Neural Network-Type Configurations},
  isbn = {978-0-470-50094-1},
  shorttitle = {Information Processing by Biochemical Systems},
  pagetotal = {148},
  publisher = {{John Wiley \& Sons}},
  date = {2010},
  keywords = {Biocomputers,Neural networks (Computer science),Information technology,Automatic Data Processing,Biochemical Phenomena,Neural Networks (Computer)},
  author = {Filo, Orna and Lotan, Noah},
  file = {/home/fh/lib/books/Filo2010_Information-processing-by-biochemical-systems-neural-network-type-configurations.pdf}
}

@book{Abeles2011,
  langid = {english},
  location = {{S.l.}},
  title = {Local {{Cortical Circuits}}: {{An Electrophysiological Study}}},
  edition = {Softcover reprint of the original 1st ed. 1982 edition},
  isbn = {978-3-642-81710-6},
  shorttitle = {Local {{Cortical Circuits}}},
  abstract = {Neurophysiologists are often accused by colleagues in the physical sci$\-$ ences of designing experiments without any underlying hypothesis. This impression is attributable to the ease of getting lost in the ever-increasing sea of professional publications which do not state explicitly the ultimate goal of the research. On the other hand, many of the explicit models for brain function in the past were so far removed from experimental reality that they had very little impact on further research. It seems that one needs much intimate experience with the real nerv-. ous system before a reasonable model can be suggested. It would have been impossible for Copernicus to suggest his model of the solar system without the detailed observations and tabulations of star and planet motion accu$\-$ mulated by the preceeding generations. This need for intimate experience with the nervous system before daring to put forward some hypothesis about its mechanism of action is especially apparent when theorizing about cerebral cortex function. There is widespread agreement that processing of information in the cor$\-$ tex is associated with complex spatio-temporal patterns of activity. Yet the vast majority of experimental work is based on single neuron recordings or on recordings made with gross electrodes to which tens of thousands of neurons contribute in an unknown fashion. Although these experiments have taught us a great deal about the organization and function of the cor$\-$ tex, they have not enabled us to examine the spatio-temporal organization of neuronal activity in any detail.},
  pagetotal = {116},
  publisher = {{Springer}},
  date = {2011-12-30},
  author = {Abeles, Moshe},
  file = {/home/fh/lib/books/Abeles2011_Local-Cortical-Circuits-An-Electrophysiological-Study.pdf}
}

@book{Ross2009,
  langid = {english},
  location = {{Upper Saddle River, N.J}},
  title = {A {{First Course}} in {{Probability}}},
  edition = {8th edition},
  isbn = {978-0-13-603313-4},
  abstract = {A First Course in Probability, Eighth Edition, features clear and intuitive explanations of the mathematics of probability theory, outstanding problem sets, and a variety of diverse examples and applications. This book is ideal for an upper-level undergraduate or graduate level introduction to probability for math, science, engineering and business students. It assumes a background in elementary calculus.},
  pagetotal = {552},
  publisher = {{Pearson Prentice Hall}},
  date = {2009-01-07},
  author = {Ross, Sheldon},
  file = {/home/fh/lib/books/Ross2009_A-First-Course-in-Probability.pdf}
}

@collection{Feldmeyer2010,
  langid = {english},
  location = {{New York ; London}},
  title = {New {{Aspects}} of {{Axonal Structure}} and {{Function}}},
  edition = {2010 edition},
  isbn = {978-1-4419-1675-4},
  abstract = {Axons are neuronal output elements and are responsible for the transfer and processing of signals from one neuron to another, even over very large distances. For a given neuronal cell type, axons are unique and display very heterogeneous patterns with respect to shape, length and target structure. Axons are the usually long process of a nerve fiber that generally conducts impulses away from the body of the nerve cell. This book is intended to summarize recent findings covering morphological, physiological, developmental, computational and pathophysiological aspects of axons. It attempts to cover new findings concerning axonal structure and functions together with their implications for signal transduction, processes implicated in the formation of axonal arbors and the transport of subcellular elements to their targets, and finally how a dysfunction in one or several of these steps could lead to axonal degeneration and ultimately to neurodegenerative diseases.},
  pagetotal = {237},
  publisher = {{Springer}},
  date = {2010-08-31},
  editor = {Feldmeyer, Dirk and L{\"u}bke, Joachim},
  file = {/home/fh/lib/books/Feldmeyer2010_New-Aspects-of-Axonal-Structure-and-Function.pdf}
}

@article{Lefort2009,
  title = {The {{Excitatory Neuronal Network}} of the {{C2 Barrel Column}} in {{Mouse Primary Somatosensory Cortex}}},
  volume = {61},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627308010921},
  doi = {10.1016/j.neuron.2008.12.020},
  abstract = {Summary
Local microcircuits within neocortical columns form key determinants of sensory processing. Here, we investigate the excitatory synaptic neuronal network of an anatomically defined cortical column, the C2 barrel column of mouse primary somatosensory cortex. This cortical column is known to process tactile information related to the C2 whisker. Through multiple simultaneous whole-cell recordings, we quantify connectivity maps between individual excitatory neurons located across all cortical layers of the C2 barrel column. Synaptic connectivity depended strongly upon somatic laminar location of both presynaptic and postsynaptic neurons, providing definitive evidence for layer-specific signaling pathways. The strongest excitatory influence upon the cortical column was provided by presynaptic layer 4 neurons. In all layers we found rare large-amplitude synaptic connections, which are likely to contribute strongly to reliable information processing. Our data set provides the first functional description of the excitatory synaptic wiring diagram of a physiologically relevant and anatomically well-defined cortical column at single-cell resolution.},
  number = {2},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2014-10-08},
  date = {2009-01-29},
  pages = {301--316},
  keywords = {SYSNEURO},
  author = {Lefort, Sandrine and Tomm, Christian and Floyd Sarria, J. -C. and Petersen, Carl C. H.},
  file = {/home/fh/lib/articles/Lefort2009.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/8ZBRIFJ8/S0896627308010921.html}
}

@book{Dayan2001,
  langid = {english},
  location = {{Cambridge, Mass}},
  title = {Theoretical {{Neuroscience}}: {{Computational}} and {{Mathematical Modeling}} of {{Neural Systems}}},
  edition = {1st edition},
  isbn = {978-0-262-04199-7},
  shorttitle = {Theoretical {{Neuroscience}}},
  abstract = {Theoretical neuroscience provides a quantitative basis for describing what nervous systems do, determining how they function, and uncovering the general principles by which they operate. This text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory.  The book is divided into three parts. Part I discusses the relationship between sensory stimuli and neural responses, focusing on the representation of information by the spiking activity of neurons. Part II discusses the modeling of neurons and neural circuits on the basis of cellular and synaptic biophysics. Part III analyzes the role of plasticity in development and learning. An appendix covers the mathematical methods used, and exercises are available on the book's Web site.},
  pagetotal = {576},
  publisher = {{The MIT Press}},
  date = {2001-12-01},
  author = {Dayan, Peter and Abbott, L. F.},
  file = {/home/fh/lib/books/Dayan2001_Theoretical-Neuroscience-Computational-and-Mathematical-Modeling-of-Neural-Systems.pdf}
}

@collection{Bower2013,
  langid = {english},
  location = {{New York}},
  title = {20 {{Years}} of {{Computational Neuroscience}}},
  edition = {2013 edition},
  isbn = {978-1-4614-1423-0},
  abstract = {When funding agencies and policy organizations consider the role of modeling and simulation in modern biology, the question is often posed, what has been accomplished ? This book will be organized around a symposium on the 20 year history of the CNS meetings, to be held as part of CNS 2010 in San Antonio Texas in July 2010.  The book, like the symposium is intended to summarize progress made in Computational Neuroscience over the last 20 years while also considering current challenges in the field.  As described in the table of contents, the chapter's authors have been selected to provide wide coverage of the applications of computational techniques to a broad range of questions and model systems in neuroscience.    The proposed book will include several features that establish the history of the field.   For each article, its author will select an article originally appearing in a CNS conference proceedings from 15 \textendash{} 20 years ago.  These short (less than 6 page) articles will provide illustrations of the state of the field 20 years ago.  The new articles will describe what has been learned about the subject in the following 20 years, and pose specific challenges for the next 20 years.   The second historical mechanism will be the reproduction of the first 12 years of posters from the CNS meeting.  These posters in and of themselves have become famous in the field (they hang in the halls of the NIH in Bethesda Maryland) and were constructed as allegories  for the state and development of computational neuroscience.  The posters were designed by the book's editor, who will, for the first time, provide a written description of each poster.},
  pagetotal = {283},
  publisher = {{Springer}},
  date = {2013-07-25},
  editor = {Bower, James M.},
  file = {/home/fh/lib/books/Bower2013_20-Years-of-Computational-Neuroscience.pdf}
}

@book{Jianfeng2007,
  langid = {english},
  title = {Computational {{Neuroscience}}: {{A Comprehensive Approach}}},
  edition = {1 edition},
  shorttitle = {Computational {{Neuroscience}}},
  abstract = {No description available},
  pagetotal = {656},
  publisher = {{CRC Press}},
  date = {2007-04-17},
  author = {Jianfeng, Feng},
  editor = {Feng, Jianfeng},
  file = {/home/fh/lib/books/Jianfeng2007_Computational-Neuroscience-A-Comprehensive-Approach.pdf}
}

@article{Kalisman2003,
  langid = {english},
  title = {Deriving Physical Connectivity from Neuronal Morphology},
  volume = {88},
  issn = {0340-1200, 1432-0770},
  url = {http://link.springer.com/article/10.1007/s00422-002-0377-3},
  doi = {10.1007/s00422-002-0377-3},
  abstract = {A model is presented that allows prediction of the probability for the formation of appositions between the axons and dendrites of any two neurons based only on their morphological statistics and relative separation. Statistics of axonal and dendritic morphologies of single neurons are obtained from 3D reconstructions of biocytin-filled cells, and a statistical representation of the same cell type is obtained by averaging across neurons according to the model. A simple mathematical formulation is applied to the axonal and dendritic statistical representations to yield the probability for close appositions. The model is validated by a mathematical proof and by comparison of predicted appositions made by layer 5 pyramidal neurons in the rat somatosensory cortex with real anatomical data. The model could be useful for studying microcircuit connectivity and for designing artificial neural networks.},
  number = {3},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybern.},
  urldate = {2014-12-05},
  date = {2003-03-01},
  pages = {210--218},
  author = {Kalisman, Nir and Silberberg, Gilad and Markram, Henry},
  file = {/home/fh/lib/articles/Kalisman2003.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/5N835XB9/10.html}
}

@article{Rochefort2011,
  title = {Development of {{Direction Selectivity}} in {{Mouse Cortical Neurons}}},
  volume = {71},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627311005186},
  doi = {10.1016/j.neuron.2011.06.013},
  abstract = {Summary
Previous studies of the ferret visual cortex indicate that the development of direction selectivity requires visual experience. Here, we used two-photon calcium imaging to study the development of direction selectivity in layer 2/3 neurons of the mouse visual cortex in~vivo. Surprisingly, just after eye opening nearly all orientation-selective neurons were also direction selective. During later development, the number of neurons responding to drifting gratings increased in parallel with the fraction of neurons that were orientation, but not direction, selective. Our experiments demonstrate that direction selectivity develops normally in dark-reared mice, indicating that the early development of direction selectivity is independent of visual experience. Furthermore, remarkable functional similarities exist between the development of direction selectivity in cortical neurons and the previously reported development of direction selectivity in the mouse retina. Together, these findings provide strong evidence that the development of orientation and direction selectivity in the mouse brain is distinctly different from that in ferrets.},
  number = {3},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2014-12-16},
  date = {2011-08-11},
  pages = {425--432},
  author = {Rochefort, Nathalie L. and Narushima, Madoka and Grienberger, Christine and Marandi, Nima and Hill, Daniel N. and Konnerth, Arthur},
  file = {/home/fh/lib/articles/Rochefort2011.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/FTHZG2PV/S0896627311005186.html}
}

@article{Shepherd2005,
  langid = {english},
  title = {Geometric and Functional Organization of Cortical Circuits},
  volume = {8},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v8/n6/abs/nn1447.html},
  doi = {10.1038/nn1447},
  abstract = {Can neuronal morphology predict functional synaptic circuits? In the rat barrel cortex, 'barrels' and 'septa' delineate an orderly matrix of cortical columns. Using quantitative laser scanning photostimulation we measured the strength of excitatory projections from layer 4 (L4) and L5A to L2/3 pyramidal cells in barrel- and septum-related columns. From morphological reconstructions of excitatory neurons we computed the geometric circuit predicted by axodendritic overlap. Within most individual projections, functional inputs were predicted by geometry and a single scale factor, the synaptic strength per potential synapse. This factor, however, varied between projections and, in one case, even within a projection, up to 20-fold. Relationships between geometric overlap and synaptic strength thus depend on the laminar and columnar locations of both the pre- and postsynaptic neurons, even for neurons of the same type. A large plasticity potential appears to be incorporated into these circuits, allowing for functional 'tuning' with fixed axonal and dendritic arbor geometry.},
  number = {6},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2015-01-23},
  date = {2005-06},
  pages = {782--790},
  author = {Shepherd, Gordon M. G. and Stepanyants, Armen and Bureau, Ingrid and Chklovskii, Dmitri and Svoboda, Karel},
  file = {/home/fh/lib/articles/Shepherd2005.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/TDH5BJKH/nn1447.html}
}

@article{Ganguly2013,
  langid = {english},
  title = {Activity-{{Dependent Neural Plasticity}} from {{Bench}} to {{Bedside}}},
  volume = {80},
  issn = {0896-6273},
  url = {http://www.cell.com/article/S089662731300932X/abstract},
  doi = {10.1016/j.neuron.2013.10.028},
  abstract = {Much progress has been made in understanding how behavioral experience and neural activity can modify the structure and function of neural circuits during development and in the adult brain. Studies of physiological and molecular mechanisms underlying activity-dependent plasticity in animal models have suggested potential therapeutic approaches for a wide range of brain disorders in humans. Physiological and electrical stimulations as well as plasticity-modifying molecular agents may facilitate functional recovery by selectively enhancing existing neural circuits or promoting the formation of new functional circuits. Here, we review the advances in basic studies of neural plasticity mechanisms in developing and adult nervous systems and current clinical treatments that harness neural plasticity, and we offer perspectives on future development of plasticity-based therapy.},
  number = {3},
  journaltitle = {Neuron},
  urldate = {2014-12-16},
  date = {2013-10-30},
  pages = {729--741},
  author = {Ganguly, Karunesh and Poo, Mu-ming},
  file = {/home/fh/lib/articles/Ganguly2013.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/J637Q98M/S0896-6273(13)00932-X.html},
  eprinttype = {pmid},
  eprint = {24183023}
}

@article{Ko2014,
  langid = {english},
  title = {Functional Organization of Synaptic Connections in the Neocortex},
  volume = {346},
  issn = {0036-8075, 1095-9203},
  url = {http://www.sciencemag.org/content/346/6209/555.2},
  doi = {10.1126/science.1260780},
  abstract = {David Marr and Tomaso Poggio proposed that in order to figure out information processing in the brain, we must understand its operation at the computational, algorithmic, and implementational levels (1). Computational tasks of the visual system, for example, include extracting properties of the external world, such as recognizing objects, and estimating their locations and movements. Algorithmically, the visual system adopts a hierarchical organization, whereby visual features of increasing complexity are represented and integrated at successive stages of processing. Some retinal ganglion cells respond best to small, round, visual stimuli of high contrast. This information is relayed by the lateral geniculate nucleus of the thalamus to the primary visual cortex (V1), where neurons become sensitive to the orientation and motion direction of visual features (2). Further up the visual processing hierarchy, neuronal representations become increasingly more complex, as neurons become responsive to contours and objects often invariant of their precise location in visual space. What remains unknown is how, at the implementational level, these computations at different stages of the visual system are carried out by the neuronal networks. Similar to many proteins whose structures determined by crystallography provide mechanistic insights into their functions, knowledge of the connectivity-function relationship of neuronal networks may provide a mechanistic understanding of how the brain generates the representation of increasing levels of abstraction. In view of this, my work with Thomas Mrsic-Flogel and Sonja Hofer at University College London has helped to develop an experimental approach that allowed us to relate the connectivity between cortical neurons to their visual response properties (3).},
  number = {6209},
  journaltitle = {Science},
  shortjournal = {Science},
  urldate = {2014-12-16},
  date = {2014-10-31},
  pages = {555--555},
  author = {Ko, Ho},
  file = {/home/fh/lib/articles/Ko2014.pdf},
  eprinttype = {pmid},
  eprint = {25359956}
}

@book{Cover2006,
  langid = {english},
  location = {{Hoboken, N.J}},
  title = {Elements of {{Information Theory}} 2nd {{Edition}}},
  edition = {2 edition},
  isbn = {978-0-471-24195-9},
  abstract = {The latest edition of this classic is updated with new problem sets and material  The Second Edition of this fundamental textbook maintains the book's tradition of clear, thought-provoking instruction. Readers are provided once again with an instructive mix of mathematics, physics, statistics, and information theory.  All the essential topics in information theory are covered in detail, including entropy, data compression, channel capacity, rate distortion, network information theory, and hypothesis testing. The authors provide readers with a solid understanding of the underlying theory and applications. Problem sets and a telegraphic summary at the end of each chapter further assist readers. The historical notes that follow each chapter recap the main points.  The Second Edition features: * Chapters reorganized to improve teaching * 200 new problems * New material on source coding, portfolio theory, and feedback capacity * Updated references  Now current and enhanced, the Second Edition of Elements of Information Theory remains the ideal textbook for upper-level undergraduate and graduate courses in electrical engineering, statistics, and telecommunications.  An Instructor's Manual presenting detailed solutions to all the problems in the book is available from the Wiley editorial department.},
  pagetotal = {776},
  publisher = {{Wiley-Interscience}},
  date = {2006-07-18},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  file = {/home/fh/lib/books/Cover2006_Elements-of-Information-Theory-2nd-Edition.pdf;/home/fh/lib/books/Cover2006_solutions_to_exercises.pdf}
}

@book{Edelman1987,
  langid = {english},
  location = {{New York}},
  title = {Neural {{Darwinism}}: {{The Theory Of Neuronal Group Selection}}},
  edition = {New edition edition},
  isbn = {978-0-465-04934-9},
  shorttitle = {Neural {{Darwinism}}},
  abstract = {Already the subject of considerable pre-publication discussion, this magisterial work by one of the nation's leading neuroscientists presents a radically new view of the function of the brain and nervous system. Its central idea is that the nervous system in each individual operates as a selective system resembling natural selection in evolution, but operating by different mechanisms. By providing a fundamental neural basis for categorization of the things of this world it unifies perception, action, and learning. The theory also completely revises our view of memory, which it considers to be a dynamic process of recategorization rather than a replicative store of attributes. This has deep implications for the interpretation of various psychological states from attention to dreaming.Neural Darwinism ranges over many disciplines, focusing on key problems in developmental and evolutionary biology, anatomy, physiology, ethology, and psychology. This book should therefore prove indispensable to advanced undergraduate and graduate students in these fields, to students of medicine, and to those in the social sciences concerned with the relation of behavior to biology. Beyond that, this far-ranging theory of brain function is bound to stimulate renewed discussions of such philosophical issues as the mind-body problem, the origins of knowledge, and the perceptual basis of language.},
  pagetotal = {400},
  publisher = {{Basic Books}},
  date = {1987-12-06},
  author = {Edelman, Gerald}
}

@article{Kalisman2005,
  langid = {english},
  title = {The Neocortical Microcircuit as a Tabula Rasa},
  volume = {102},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/102/3/880},
  doi = {10.1073/pnas.0407088102},
  abstract = {The neocortex has a high capacity for plasticity. To understand the full scope of this capacity, it is essential to know how neurons choose particular partners to form synaptic connections. By using multineuron whole-cell recordings and confocal microscopy we found that axons of layer V neocortical pyramidal neurons do not preferentially project toward the dendrites of particular neighboring pyramidal neurons; instead, axons promiscuously touch all neighboring dendrites without any bias. Functional synaptic coupling of a small fraction of these neurons is, however, correlated with the existence of synaptic boutons at existing touch sites. These data provide the first direct experimental evidence for a tabula rasa-like structural matrix between neocortical pyramidal neurons and suggests that pre- and postsynaptic interactions shape the conversion between touches and synapses to form specific functional microcircuits. These data also indicate that the local neocortical microcircuit has the potential to be differently rewired without the need for remodeling axonal or dendritic arbors.},
  number = {3},
  journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
  shortjournal = {PNAS},
  urldate = {2015-01-23},
  date = {2005-01-18},
  pages = {880--885},
  keywords = {connectivity,neocortex,spines},
  author = {Kalisman, Nir and Silberberg, Gilad and Markram, Henry},
  file = {/home/fh/lib/articles/Kalisman2005.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/QGFN4MDX/880.html},
  eprinttype = {pmid},
  eprint = {15630093}
}

@article{Brown2009,
  langid = {english},
  title = {Intracortical Circuits of Pyramidal Neurons Reflect Their Long-Range Axonal Targets},
  volume = {457},
  issn = {0028-0836},
  url = {http://www.nature.com/nature/journal/v457/n7233/abs/nature07658.html},
  doi = {10.1038/nature07658},
  abstract = {Cortical columns generate separate streams of information that are distributed to numerous cortical and subcortical brain regions. We asked whether local intracortical circuits reflect these different processing streams by testing whether the intracortical connectivity among pyramidal neurons reflects their long-range axonal targets. We recorded simultaneously from up to four retrogradely labelled pyramidal neurons that projected to the superior colliculus, the contralateral striatum or the contralateral cortex to assess their synaptic connectivity. Here we show that the probability of synaptic connection depends on the functional identities of both the presynaptic and postsynaptic neurons. We first found that the frequency of monosynaptic connections among corticostriatal pyramidal neurons is significantly higher than among corticocortical or corticotectal pyramidal neurons. We then show that the probability of feed-forward connections from corticocortical neurons to corticotectal neurons is approximately three- to fourfold higher than the probability of monosynaptic connections among corticocortical or corticotectal cells. Moreover, we found that the average axodendritic overlap of the presynaptic and postsynaptic pyramidal neurons could not fully explain the differences in connection probability that we observed. The selective synaptic interactions we describe demonstrate that the organization of local networks of pyramidal cells reflects the long-range targets of both the presynaptic and postsynaptic neurons.},
  number = {7233},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2015-01-23},
  date = {2009-02-26},
  pages = {1133--1136},
  author = {Brown, Solange P. and Hestrin, Shaul},
  file = {/home/fh/lib/articles/Brown2009.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/ZMNVUE9S/nature07658.html}
}

@book{Godfrey-Smith2003,
  langid = {english},
  location = {{Chicago}},
  title = {Theory and {{Reality}}: {{An Introduction}} to the {{Philosophy}} of {{Science}}},
  edition = {1 edition},
  isbn = {978-0-226-30063-4},
  shorttitle = {Theory and {{Reality}}},
  abstract = {How does science work?  Does it tell us what the world is "really" like?  What makes it different from other ways of understanding the universe?  In Theory and Reality, Peter Godfrey-Smith addresses these questions by taking the reader on a grand tour of one hundred years of debate about science. The result is a completely accessible introduction to the main themes of the philosophy of science.  Intended for undergraduates and general readers with no prior background in philosophy, Theory and Reality covers logical positivism; the problems of induction and confirmation; Karl Popper's theory of science; Thomas Kuhn and "scientific revolutions"; the views of Imre Lakatos, Larry Laudan, and Paul Feyerabend; and challenges to the field from sociology of science, feminism, and science studies. The book then looks in more detail at some specific problems and theories, including scientific realism, the theory-ladeness of observation, scientific explanation, and Bayesianism. Finally, Godfrey-Smith defends a form of philosophical naturalism as the best way to solve the main problems in the field. Throughout the text he points out connections between philosophical debates and wider discussions about science in recent decades, such as the infamous "science wars." Examples and asides engage the beginning student; a glossary of terms explains key concepts; and suggestions for further reading are included at the end of each chapter. However, this is a textbook that doesn't feel like a textbook because it captures the historical drama of changes in how science has been conceived over the last one hundred years. Like no other text in this field, Theory and Reality combines a survey of recent history of the philosophy of science with current key debates in language that any beginning scholar or critical reader can follow.},
  pagetotal = {272},
  publisher = {{University of Chicago Press}},
  date = {2003-08-01},
  author = {Godfrey-Smith, Peter},
  file = {/home/fh/lib/books/Godfrey-Smith2003_Theory-and-Reality-An-Introduction-to-the-Philosophy-of-Science.pdf}
}

@article{Snider2010,
  langid = {english},
  title = {A {{Universal Property}} of {{Axonal}} and {{Dendritic Arbors}}},
  volume = {66},
  issn = {0896-6273},
  url = {http://www.cell.com/article/S0896627310001042/abstract},
  doi = {10.1016/j.neuron.2010.02.013},
  abstract = {Axonal and dendritic arbors can be characterized statistically by their spatial density function, a function that specifies the probability of finding a branch of a particular arbor at each point in a neural circuit. Based on an analysis of over a thousand arbors from many neuron types in various species, we have discovered an unexpected simplicity in arbor structure: all of the arbors we have examined, both axonal and dendritic, can be described by a Gaussian density function truncated at about two standard deviations. Because all arbors are characterized by density functions with this single functional form, only four parameters are required to specify an arbor's size and shape: the total length of its branches and the standard deviations of the Gaussian in three orthogonal directions. This simplicity in arbor structure can have implications for the developmental wiring of neural circuits.},
  number = {1},
  journaltitle = {Neuron},
  urldate = {2015-01-23},
  date = {2010-04-15},
  pages = {45--56},
  keywords = {SYSNEURO,DEVBIO},
  author = {Snider, Joseph and Pillai, Andrea and Stevens, Charles F.},
  file = {/home/fh/lib/articles/Snider2010.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/RZFMAAC9/S0896-6273(10)00104-2.html},
  eprinttype = {pmid},
  eprint = {20399728}
}

@article{vanPelt2013,
  title = {Estimating Neuronal Connectivity from Axonal and Dendritic Density Fields},
  volume = {7},
  issn = {1662-5188},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3839411/},
  doi = {10.3389/fncom.2013.00160},
  abstract = {Neurons innervate space by extending axonal and dendritic arborizations. When axons and dendrites come in close proximity of each other, synapses between neurons can be formed. Neurons vary greatly in their morphologies and synaptic connections with other neurons. The size and shape of the arborizations determine the way neurons innervate space. A neuron may therefore be characterized by the spatial distribution of its axonal and dendritic ``mass.'' A population mean ``mass'' density field of a particular neuron type can be obtained by averaging over the individual variations in neuron geometries. Connectivity in terms of candidate synaptic contacts between neurons can be determined directly on the basis of their arborizations but also indirectly on the basis of their density fields. To decide when a candidate synapse can be formed, we previously developed a criterion defining that axonal and dendritic line pieces should cross in 3D and have an orthogonal distance less than a threshold value. In this paper, we developed new methodology for applying this criterion to density fields. We show that estimates of the number of contacts between neuron pairs calculated from their density fields are fully consistent with the number of contacts calculated from the actual arborizations. However, the estimation of the connection probability and the expected number of contacts per connection cannot be calculated directly from density fields, because density fields do not carry anymore the correlative structure in the spatial distribution of synaptic contacts. Alternatively, these two connectivity measures can be estimated from the expected number of contacts by using empirical mapping functions. The neurons used for the validation studies were generated by our neuron simulator NETMORPH. An example is given of the estimation of average connectivity and Euclidean pre- and postsynaptic distance distributions in a network of neurons represented by their population mean density fields.},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front Comput Neurosci},
  urldate = {2015-01-23},
  date = {2013-11-25},
  author = {van Pelt, Jaap and van Ooyen, Arjen},
  options = {useprefix=true},
  file = {/home/fh/lib/articles/van Pelt2013.pdf},
  eprinttype = {pmid},
  eprint = {24324430},
  pmcid = {PMC3839411}
}

@book{Jaynes2003,
  langid = {english},
  location = {{Cambridge, UK ; New York, NY}},
  title = {Probability {{Theory}}: {{The Logic}} of {{Science}}},
  isbn = {978-0-521-59271-0},
  shorttitle = {Probability {{Theory}}},
  abstract = {Going beyond the conventional mathematics of probability theory, this study views the subject in a wider context. It discusses new results, along with applications of probability theory to a variety of problems. The book contains many exercises and is suitable for use as a textbook on graduate-level courses involving data analysis. Aimed at readers already familiar with applied mathematics at an advanced undergraduate level or higher, it is of interest to scientists concerned with inference from incomplete information.},
  pagetotal = {753},
  publisher = {{Cambridge University Press}},
  date = {2003-06-09},
  author = {Jaynes, E. T.},
  editor = {Bretthorst, G. Larry},
  file = {/home/fh/lib/books/Jaynes2003_Probability-Theory-The-Logic-of-Science.pdf}
}

@thesis{Perin2010,
  title = {Emergent {{Dynamics}} in {{Neocortical Microcircuits}}},
  url = {http://infoscience.epfl.ch/record/147975},
  date = {2010},
  author = {Perin, Rodrigo},
  file = {/home/fh/lib/thesis/Perin2010.pdf}
}

@article{Hellwig2000,
  langid = {english},
  title = {A Quantitative Analysis of the Local Connectivity between Pyramidal Neurons in Layers 2/3 of the Rat Visual Cortex},
  volume = {82},
  issn = {0340-1200, 1432-0770},
  url = {http://link.springer.com/article/10.1007/PL00007964},
  doi = {10.1007/PL00007964},
  abstract = {This study provides a detailed quantitative estimate for local synaptic connectivity between neocortical pyramidal neurons. A new way of obtaining such an estimate is presented. In acute slices of the rat visual cortex, four layer 2 and four layer 3 pyramidal neurons were intracellularly injected with biocytin. Axonal and dendritic arborizations were three-dimensionally reconstructed with the aid of a computer-based camera lucida system. In a computer experiment, pairs of pre- and postsynaptic neurons were formed and potential synaptic contacts were calculated. For each pair, the calculations were carried out for a whole range of distances (0 to 500 $\mu$m) between the presynaptic and the postsynaptic neuron, in order to estimate cortical connectivity as a function of the spatial separation of neurons. It was also differentiated whether neurons were situated in the same or in different cortical layers. The data thus obtained was used to compute connection probabilities, the average number of contacts between neurons, the frequency of specific numbers of contacts and the total number of contacts a dendritic tree receives from the surrounding cortical volume. Connection probabilities ranged from 50\% to 80\% for directly adjacent neurons and from 0\% to 15\% for neurons 500 $\mu$m apart. In many cases, connections were mediated by one contact only. However, close neighbors made on average up to 3 contacts with each other. The question as to whether the method employed in this study yields a realistic estimate of synaptic connectivity is discussed. It is argued that the results can be used as a detailed blueprint for building artificial neural networks with a cortex-like architecture.},
  number = {2},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol Cybern},
  urldate = {2015-01-23},
  date = {2000-01-01},
  pages = {111--121},
  author = {Hellwig, Bernhard},
  file = {/home/fh/lib/articles/Hellwig2000.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/6JMIN8KP/PL00007964.html}
}

@article{Liley1994,
  title = {Intracortical Connectivity of Pyramidal and Stellate Cells: Estimates of Synaptic Densities and Coupling Symmetry},
  volume = {5},
  issn = {0954-898X},
  url = {http://informahealthcare.com/doi/abs/10.1088/0954-898X_5_2_004},
  doi = {10.1088/0954-898X_5_2_004},
  shorttitle = {Intracortical Connectivity of Pyramidal and Stellate Cells},
  abstract = {A method is outlined for estimating the the average number of synapses forming between cortical neurons as a function of their intercellular separation and the geometry of their dendritic and axonal arborization. Consideration is confined to the formation of local intracortical connections and to the case where the distribution of axonal and dendritic fibres has spherical symmetry. Parameters are deduced from quantitative anatomical studies in murine cortex. It is demonstrated that the majority of local connections forming within a given volume of isotropic cortex can be accounted for on the assumption that local connections between neurons form randomly. From these computations the symmetry of connection between neurons, the likely position for synapse formation on the dendritic tree and the relative synaptic densities attributable to long-and short-range interaction between excitatory and inhibitory neural subsets is determined. Local intracortical couplings appear to be highly asymmetric, and account for about 3200 synapses forming on pyramidal and stellate cells.},
  number = {2},
  journaltitle = {Network: Computation in Neural Systems},
  shortjournal = {Network},
  urldate = {2015-01-23},
  date = {1994-01-01},
  pages = {175--189},
  author = {Liley, D T J and Wright, J J},
  file = {/home/fh/lib/articles/Liley1994.pdf}
}

@book{Kuhn1996,
  langid = {english},
  location = {{Chicago, IL}},
  title = {The {{Structure}} of {{Scientific Revolutions}}},
  edition = {3rd edition},
  isbn = {978-0-226-45808-3},
  abstract = {Thomas S. Kuhn's classic book is now available with a new index."A landmark in intellectual history which has attracted attention far beyond its own immediate field. . . . It is written with a combination of depth and clarity that make it an almost unbroken series of aphorisms. . . . Kuhn does not permit truth to be a criterion of scientific theories, he would presumably not claim his own theory to be true. But if causing a revolution is the hallmark of a superior paradigm, [this book] has been a resounding success." --Nicholas Wade, Science"Perhaps the best explanation of [the] process of discovery." --William Erwin Thompson, New York Times Book Review"Occasionally there emerges a book which has an influence far beyond its originally intended audience. . . . Thomas Kuhn's The Structure of Scientific Revolutions . . . has clearly emerged as just such a work." --Ron Johnston, Times Higher Education Supplement"Among the most influential academic books in this century." --Choice--One of "The Hundred Most Influential Books Since the Second World War," Times Literary SupplementThomas S. Kuhn was the Laurence Rockefeller Professor Emeritus of linguistics and philosophy at the Massachusetts Institute of Technology. His books include The Essential Tension; Black-Body Theory and the Quantum Discontinuity, 1894-1912; and The Copernican Revolution.},
  pagetotal = {212},
  publisher = {{University of Chicago Press}},
  date = {1996-12-15},
  author = {Kuhn, Thomas S.},
  file = {/home/fh/lib/books/Kuhn1996_The-Structure-of-Scientific-Revolutions.pdf}
}

@book{Hofstadter1999,
  langid = {english},
  location = {{New York}},
  title = {G{\"o}del, {{Escher}}, {{Bach}}: {{An Eternal Golden Braid}}},
  edition = {20 Anv edition},
  isbn = {978-0-465-02656-2},
  shorttitle = {G{\"o}del, {{Escher}}, {{Bach}}},
  abstract = {Douglas Hofstadter's book is concerned directly with the nature of ``maps'' or links between formal systems. However, according to Hofstadter, the formal system that underlies all mental activity transcends the system that supports it. If life can grow out of the formal chemical substrate of the cell, if consciousness can emerge out of a formal system of firing neurons, then so too will computers attain human intelligence. G{\"o}del Escher and Bach is a wonderful exploration of fascinating ideas at the heart of cognitive science: meaning, reduction, recursion, and much more.},
  pagetotal = {824},
  publisher = {{Basic Books}},
  date = {1999-02-05},
  author = {Hofstadter, Douglas R.},
  file = {/home/fh/lib/books/Hofstadter1999_Gödel,-Escher,-Bach-An-Eternal-Golden-Braid.epub}
}

@report{zotero-null-198,
  title = {Biblatex {{Package}} 1.7},
  file = {/home/fh/lib/manuals/latex/biblatex_package_1.7.pdf},
  note = {manuals/latex}
}

@article{Klinshov2014,
  title = {Dense {{Neuron Clustering Explains Connectivity Statistics}} in {{Cortical Microcircuits}}},
  volume = {9},
  url = {http://dx.doi.org/10.1371/journal.pone.0094292},
  doi = {10.1371/journal.pone.0094292},
  abstract = {Local cortical circuits appear highly non-random, but the underlying connectivity rule remains elusive. Here, we analyze experimental data observed in layer 5 of rat neocortex and suggest a model for connectivity from which emerge essential observed non-random features of both wiring and weighting. These features include lognormal distributions of synaptic connection strength, anatomical clustering, and strong correlations between clustering and connection strength. Our model predicts that cortical microcircuits contain large groups of densely connected neurons which we call clusters. We show that such a cluster contains about one fifth of all excitatory neurons of a circuit which are very densely connected with stronger than average synapses. We demonstrate that such clustering plays an important role in the network dynamics, namely, it creates bistable neural spiking in small cortical circuits. Furthermore, introducing local clustering in large-scale networks leads to the emergence of various patterns of persistent local activity in an ongoing network activity. Thus, our results may bridge a gap between anatomical structure and persistent activity observed during working memory and other cognitive processes.},
  number = {4},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS ONE},
  urldate = {2015-01-26},
  date = {2014-04-14},
  pages = {e94292},
  author = {Klinshov, Vladimir V. and Teramae, Jun-nosuke and Nekorkin, Vladimir I. and Fukai, Tomoki},
  file = {/home/fh/lib/articles/Klinshov2014.pdf}
}

@article{Markram1997,
  title = {Physiology and Anatomy of Synaptic Connections between Thick Tufted Pyramidal Neurones in the Developing Rat Neocortex.},
  volume = {500},
  issn = {0022-3751},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1159394/},
  abstract = {1. Dual voltage recordings were made from pairs of adjacent, synaptically connected thick tufted layer 5 pyramidal neurones in brain slices of young rat (14-16 days) somatosensory cortex to examine the physiological properties of unitary EPSPs. Pre- and postsynaptic neurones were filled with biocytin and examined in the light and electron microscope to quantify the morphology of axonal and dendritic arbors and the number and location of synaptic contacts on the target neurone. 2. In 138 synaptic connections between pairs of pyramidal neurones 96 (70\%) were unidirectional and 42 (30\%) were bidirectional. The probability of finding a synaptic connection in dual recordings was 0.1. Unitary EPSPs evoked by a single presynaptic action potential (AP) had a mean peak amplitude ranging from 0.15 to 5.5 mV in different connections with a mean of 1.3 +/- 1.1 mV, a latency of 1.7 +/- 0.9 ms, a 20-80\% rise time of 2.9 +/- 2.3 ms and a decay time constant of 40 +/- 18 ms at 32-24 degrees C and -60 +/- 2 mV membrane potential. 3. Peak amplitudes of unitary EPSPs fluctuated randomly from trial to trial. The coefficient of variation (c.v.) of the unitary EPSP amplitudes ranged from 0.13 to 2.8 in different synaptic connections (mean, 0.52; median, 0.41). The percentage of failures of single APs to evoke a unitary EPSP ranged from 0 to 73\% (mean, 14\%; median, 7\%). Both c.v. and percentage of failures decreased with increasing mean EPSP amplitude. 4. Postsynaptic glutamate receptors which mediate unitary EPSPs at -60 mV were predominantly of the L-alpha-amino-3-hydroxy-5-methyl-4-isoxazolepropionate (AMPA) receptor type. Receptors of the N-methyl-D-aspartate (NMDA) type contributed only a small fraction ($<$ 20\%) to the voltage-time integral of the unitary EPSP at -60 mV, but their contribution increased at more positive membrane potentials. 5. Branching patterns of dendrites and axon collaterals of forty-five synaptically connected neurones, when examined in the light microscope, indicated that the axonal and dendritic anatomy of both projecting and target neurones and of uni- and bidirectionally connected neurones was uniform. 6. The number of potential synaptic contacts formed by a presynaptic neurone on a target neurone varied between four and eight (mean, 5.5 +/- 1.1 contacts; n = 19 connections). Synaptic contacts were preferentially located on basal dendrites (63\%, 82 +/- 35 microns from the soma, n = 67) and apical oblique dendrites (27\%, 145 +/- 59 microns, n = 29), and 35\% of all contacts were located on tertiary basal dendritic branches. The mean geometric distances (from the soma) of the contacts of a connection varied between 80 and 585 microns (mean, 147 microns; median, 105 microns). The correlation between EPSP amplitude and the number of morphologically determined synaptic contacts or the mean geometric distances from the soma was only weak (correlation coefficients were 0.2 and 0.26, respectively). 7. Compartmental models constructed from camera lucida drawings of eight target neurones showed that synaptic contacts were located at mean electrotonic distances between 0.07 and 0.33 from the soma (mean, 0.13). Simulations of unitary EPSPs, assuming quantal conductance changes with fast rise time and short duration, indicated that amplitudes of quantal EPSPs at the soma were attenuated, on average, to $<$ 10\% of dendritic EPSPs and varied in amplitude up to 10-fold depending on the dendritic location of synaptic contacts. The inferred quantal peak conductance increase varied between 1.5 and 5.5 nS (mean, 3 nS). 8. The combined physiological and morphological measurements in conjunction with EPSP simulations indicated that the 20-fold range in efficacy of the synaptic connections between thick tufted pyramidal neurones, which have their synaptic contacts preferentially located on basal and apical oblique dendrites, was due to differences in transmitter release probability of the projecting neurones and, to a lesser extent, to differenc},
  issue = {Pt 2},
  journaltitle = {The Journal of Physiology},
  shortjournal = {J Physiol},
  urldate = {2015-03-05},
  date = {1997-04-15},
  pages = {409--440},
  keywords = {bidirs},
  author = {Markram, Henry and L{\"u}bke, Joachim and Frotscher, Michael and Roth, Arnd and Sakmann, Bert},
  file = {/home/fh/lib/articles/Markram1997.pdf},
  eprinttype = {pmid},
  eprint = {9147328},
  pmcid = {PMC1159394}
}

@article{Lubke2003,
  langid = {english},
  title = {Morphometric {{Analysis}} of the {{Columnar Innervation Domain}} of {{Neurons Connecting Layer}} 4 and {{Layer}} 2/3 of {{Juvenile Rat Barrel Cortex}}},
  volume = {13},
  issn = {1047-3211, 1460-2199},
  url = {http://cercor.oxfordjournals.org/content/13/10/1051},
  doi = {10.1093/cercor/13.10.1051},
  abstract = {We have investigated the dendritic and axonal morphology of connected pairs of L4 spiny neurons and L2/3 pyramidal cells in rat barrel cortex. The `projection' field of the axons of L4 spiny neurons in layers 2/3, 4 and 5 has a width of 400\textendash{}500 $\mu$m thereby defining an anatomical barrel-column. In layer 2/3, the averaged axonal `projection' field of L4 spiny neurons together with the dendritic `receptive' field of the connected L2/3 pyramidal cells form a mostly column-restricted anatomical L4-to-L2/3 `innervation domain' that extends 300\textendash{}400 $\mu$m and includes mostly basal dendrites. In the L4-to-L2/3 innervation domain a single L4 spiny neuron contacts \textasciitilde{}300\textendash{}400 pyramidal cells while in the L4-to-L4 innervation domain it contacts \textasciitilde{}200 other L4 spiny neurons. Similarly \textasciitilde{}300\textendash{}400 L4 spiny neurons converge onto a single pyramidal cell and \textasciitilde{}200 L4 spiny neurons innervate another L4 spiny neuron. The L2/3 pyramidal cell axon has a vertical projection field spanning all cortical layers, and a long-range horizontal field in layers 2/3 (width 1100\textendash{}1200 $\mu$m) and 5 (700\textendash{}800 $\mu$m) projecting across column borders. The results suggest that the flow of excitation within a barrel-column is determined by the largely columnar confinement of the L4-to-L4 and L4-to-L2/3 innervation domains. A whisker deflection activates \textasciitilde{}140 L4 spiny neurons that will generate EPSPs in most barrel-related L2/3 pyramidal cells of a principal whisker column. The translaminar synaptic transmission to layer 2/3 and the axonal projection fields of L2/3 pyramidal cells are the major determinants of the dynamic, multi-columnar map in which a single whisker deflection is represented in the cortex.},
  number = {10},
  journaltitle = {Cerebral Cortex},
  shortjournal = {Cereb. Cortex},
  urldate = {2015-01-26},
  date = {2003-01-10},
  pages = {1051--1063},
  keywords = {_tablet},
  author = {L{\"u}bke, Joachim and Roth, Arnd and Feldmeyer, Dirk and Sakmann, Bert},
  file = {/home/fh/lib/articles/Lübke2003.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/EU8E4VUJ/1051.html},
  eprinttype = {pmid},
  eprint = {12967922}
}

@article{Seth2005,
  title = {Neural {{Darwinism}} and Consciousness},
  volume = {14},
  issn = {1053-8100},
  url = {http://www.sciencedirect.com/science/article/pii/S1053810004000923},
  doi = {10.1016/j.concog.2004.08.008},
  abstract = {Neural Darwinism (ND) is a large scale selectionist theory of brain development and function that has been hypothesized to relate to consciousness. According to ND, consciousness is entailed by reentrant interactions among neuronal populations in the thalamocortical system (the `dynamic core'). These interactions, which permit high-order discriminations among possible core states, confer selective advantages on organisms possessing them by linking current perceptual events to a past history of value-dependent learning. Here, we assess the consistency of ND with 16 widely recognized properties of consciousness, both physiological (for example, consciousness is associated with widespread, relatively fast, low amplitude interactions in the thalamocortical system), and phenomenal (for example, consciousness involves the existence of a private flow of events available only to the experiencing subject). While no theory accounts fully for all of these properties at present, we find that ND and its recent extensions fare well.},
  number = {1},
  journaltitle = {Consciousness and Cognition},
  shortjournal = {Consciousness and Cognition},
  series = {Neurobiology of Animal Consciousness},
  urldate = {2015-01-28},
  date = {2005-03},
  pages = {140--168},
  keywords = {Edelman,Dynamic core,Neural Darwinism,Thalamocortical system,Complexity,Degeneracy,Reentry},
  author = {Seth, Anil K. and Baars, Bernard J.},
  file = {/home/fh/lib/articles/Seth2005.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/R5HMSQ66/S1053810004000923.html}
}

@article{Schubert2001,
  langid = {english},
  title = {Layer-{{Specific Intracolumnar}} and {{Transcolumnar Functional Connectivity}} of {{Layer V Pyramidal Cells}} in {{Rat Barrel Cortex}}},
  volume = {21},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/21/10/3580},
  abstract = {Layer V pyramidal cells in rat barrel cortex are considered to play an important role in intracolumnar and transcolumnar signal processing. However, the precise circuitry mediating this processing is still incompletely understood. Here we obtained detailed maps of excitatory and inhibitory synaptic inputs onto the two major layer V pyramidal cell subtypes, intrinsically burst spiking (IB) and regular spiking (RS) cells, using a combination of caged glutamate photolysis, whole-cell patch-clamp recording, and three-dimensional reconstruction of biocytin-labeled cells. To excite presynaptic neurons with laminar specificity, the release of caged glutamate was calibrated and restricted to small areas of 50 \texttimes{} 50 $\mu$m in all cortical layers and in at least two neighboring barrel-related columns. IB cells received intracolumnar excitatory input from all layers, with the largest EPSP amplitudes originating from neurons in layers IV and VI. Prominent transcolumnar excitatory inputs were provided by presynaptic neurons also located in layers IV, V, and VI of neighboring columns. Inhibitory inputs were rare. In contrast, RS cells received distinct intracolumnar inhibitory inputs, especially from layers II/III and V. Intracolumnar excitatory inputs to RS cells were prominent from layers II\textendash{}V, but relatively weak from layer VI. Conspicuous transcolumnar excitatory inputs could be evoked solely in layers IV and V. Our results show that layer V pyramidal cells are synaptically driven by presynaptic neurons located in every layer of the barrel cortex. RS cells seem to be preferentially involved in intracolumnar signal processing, whereas IB cells effectively integrate excitatory inputs across several columns.},
  number = {10},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2015-01-26},
  date = {2001-05-15},
  pages = {3580--3592},
  keywords = {barrel cortex,layer V,pyramidal cell,burst spiking,regular spiking,functional connectivity,excitatory inputs,inhibitory inputs,morphology,electrophysiology,biocytin,caged glutamate,somatosensory,slices,_tablet},
  author = {Schubert, Dirk and Staiger, Jochen F. and Cho, Nichole and K{\"o}tter, Rolf and Zilles, Karl and Luhmann, Heiko J.},
  file = {/home/fh/lib/articles/Schubert2001.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/XHDH5W3F/3580.html},
  eprinttype = {pmid},
  eprint = {11331387}
}

@article{Feldmeyer2012,
  title = {Excitatory Neuronal Connectivity in the Barrel Cortex},
  volume = {6},
  issn = {1662-5129},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3394394/},
  doi = {10.3389/fnana.2012.00024},
  abstract = {Neocortical areas are believed to be organized into vertical modules, the cortical columns, and the horizontal layers 1\textendash{}6. In the somatosensory barrel cortex these columns are defined by the readily discernible barrel structure in layer 4. Information processing in the neocortex occurs along vertical and horizontal axes, thereby linking individual barrel-related columns via axons running through the different cortical layers of the barrel cortex. Long-range signaling occurs within the neocortical layers but also through axons projecting through the white matter to other neocortical areas and subcortical brain regions. Because of the ease of identification of barrel-related columns, the rodent barrel cortex has become a prototypical system to study the interactions between different neuronal connections within a sensory cortical area and between this area and other cortical as well subcortical regions. Such interactions will be discussed specifically for the feed-forward and feedback loops between the somatosensory and the somatomotor cortices as well as the different thalamic nuclei. In addition, recent advances concerning the morphological characteristics of excitatory neurons and their impact on the synaptic connectivity patterns and signaling properties of neuronal microcircuits in the whisker-related somatosensory cortex will be reviewed. In this context, their relationship between the structural properties of barrel-related columns and their function as a module in vertical synaptic signaling in the whisker-related cortical areas will be discussed.},
  journaltitle = {Frontiers in Neuroanatomy},
  shortjournal = {Front Neuroanat},
  urldate = {2015-01-26},
  date = {2012-07-11},
  keywords = {_tablet},
  author = {Feldmeyer, Dirk},
  file = {/home/fh/lib/articles/Feldmeyer2012.pdf},
  eprinttype = {pmid},
  eprint = {22798946},
  pmcid = {PMC3394394}
}

@article{Davison2012,
  title = {Automated {{Capture}} of {{Experiment Context}} for {{Easier Reproducibility}} in {{Computational Research}}},
  volume = {14},
  issn = {1521-9615},
  url = {http://scitation.aip.org/content/aip/journal/cise/14/4/10.1109/MCSE.2012.41},
  doi = {10.1109/MCSE.2012.41},
  abstract = {Published scientific research that relies on numerical computations is too often not reproducible. For computational research to become consistently and reliably reproducible, the process must become easier to achieve, as part of day-to-day research. A combination of best practices and automated tools can make it easier to create reproducible research.},
  number = {4},
  journaltitle = {Computing in Science \& Engineering},
  urldate = {2015-03-06},
  date = {2012-07-01},
  pages = {48--56},
  author = {Davison, Andrew},
  file = {/home/fh/lib/articles/Davison2012.pdf}
}

@article{Edelman1993,
  title = {Neural {{Darwinism}}: {{Selection}} and Reentrant Signaling in Higher Brain Function},
  volume = {10},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/089662739390304A},
  doi = {10.1016/0896-6273(93)90304-A},
  shorttitle = {Neural {{Darwinism}}},
  abstract = {Variation and selection within neural populations play key roles in the development and function of the brain. In this article, I review a population theory of the nervous system aimed at understanding the significance of these processes. Since its original formulation in 1978, considerable evidence has accumulated to support this theory of neuronal group selection. Extensive neural modeling based on the theory has provided useful insights into several outstanding neurobiological problems including those concerned with integration of cortical function, sensorimotor control, and perceptually based behavior.},
  number = {2},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2015-01-28},
  date = {1993-02},
  pages = {115--125},
  author = {Edelman, Gerald M.},
  file = {/home/fh/lib/articles/Edelman1993.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/2D94R7FF/089662739390304A.html}
}

@article{Sjostrom2008,
  langid = {english},
  title = {Dendritic {{Excitability}} and {{Synaptic Plasticity}}},
  volume = {88},
  issn = {0031-9333, 1522-1210},
  url = {http://physrev.physiology.org/content/88/2/769},
  doi = {10.1152/physrev.00016.2007},
  abstract = {Most synaptic inputs are made onto the dendritic tree. Recent work has shown that dendrites play an active role in transforming synaptic input into neuronal output and in defining the relationships between active synapses. In this review, we discuss how these dendritic properties influence the rules governing the induction of synaptic plasticity. We argue that the location of synapses in the dendritic tree, and the type of dendritic excitability associated with each synapse, play decisive roles in determining the plastic properties of that synapse. Furthermore, since the electrical properties of the dendritic tree are not static, but can be altered by neuromodulators and by synaptic activity itself, we discuss how learning rules may be dynamically shaped by tuning dendritic function. We conclude by describing how this reciprocal relationship between plasticity of dendritic excitability and synaptic plasticity has changed our view of information processing and memory storage in neuronal networks.},
  number = {2},
  journaltitle = {Physiological Reviews},
  urldate = {2015-01-28},
  date = {2008-04-01},
  pages = {769--840},
  author = {Sj{\"o}str{\"o}m, P. Jesper and Rancz, Ede A. and Roth, Arnd and H{\"a}usser, Michael},
  file = {/home/fh/lib/articles/Sjöström2008.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/V2SGFE8S/769.html},
  eprinttype = {pmid},
  eprint = {18391179},
  note = {Most synaptic inputs are made onto the dendritic tree. Recent work has shown that dendrites play an active role in transforming synaptic input into neuronal output and in defining the relationships between active synapses. In this review, we discuss how these dendritic properties influence the rules governing the induction of synaptic plasticity. We argue that the location of synapses in the dendritic tree, and the type of dendritic excitability associated with each synapse, play decisive roles in determining the plastic properties of that synapse. Furthermore, since the electrical properties of the dendritic tree are not static, but can be altered by neuromodulators and by synaptic activity itself, we discuss how learning rules may be dynamically shaped by tuning dendritic function. We conclude by describing how this reciprocal relationship between plasticity of dendritic excitability and synaptic plasticity has changed our view of information processing and memory storage in neuronal networks.}
}

@article{Ramaswamy2012,
  title = {Intrinsic Morphological Diversity of Thick-Tufted Layer 5 Pyramidal Neurons Ensures Robust and Invariant Properties of in Silico Synaptic Connections},
  volume = {590},
  issn = {0022-3751},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3381307/},
  doi = {10.1113/jphysiol.2011.219576},
  abstract = {Non-technical summary
Pyramidal neurons are output neurons of the neocortex. The thick-tufted layer 5 (TTL5) pyramidal neurons are one of the most extensively studied neocortical cell types and are characterized by an exquisite morphological structure. Despite their characteristic morphology, TTL5 neurons in the neocortical microcircuit display an intrinsic diversity that renders each neuron morphologically unique. In order to investigate the functional significance of this intrinsic morphological diversity, we reconstructed networks of TTL5 neurons through a detailed computer model and compared the properties of modelled synaptic connections against experimental data. We found that the average synaptic properties of modelled connections between TTL5 neurons closely matched experimental observations and remained unaltered by changes to several parameters at the local network level. These results show that the intrinsic morphological diversity of TTL5 neurons is a mechanism to ensure that the average synaptic properties are robust to changes at the local network level.

Abstract
The morphology of neocortical pyramidal neurons is not only highly characteristic but also displays an intrinsic diversity that renders each neuron morphologically unique. We investigated the significance of this intrinsic morphological diversity in in silico networks composed of thick-tufted layer 5 (TTL5) pyramidal neurons, by comparing the in silico and in vitro properties of TTL5 synaptic connections. The synaptic locations of in silico connections were determined by placing 3D reconstructed TTL5 neurons randomly in a volume equivalent to that of layer 5 in the juvenile rat somatosensory cortex and using a `collision-detection' algorithm to identify the incidental loci of axo-dendritic overlap. The activation time of the modelled synapses and their biophysical properties were characterized based on experimental measurements. We found that the anatomical loci of synapses and the physiological properties of the somatically recorded EPSPs closely matched those recorded experimentally without the need for any fine-tuning. Furthermore, perturbations to both the physiological or anatomical parameters of the model did not alter the average physiological properties of the population of modelled synaptic connections. This microcircuit-level robust behaviour was due to the intrinsic diversity of the morphology of pyramidal neurons in the microcircuit. We conclude that synaptic transmission in a network of TTL5 neurons is highly invariant across microcircuits suggesting that intrinsic diversity is a mechanism to ensure the same average synaptic properties in different animals of the same species. Finally, we show that the average physiological properties of the TTL5 microcircuit are surprisingly robust to anatomical and physiological perturbations also partly due to the intrinsic diversity of pyramidal neuron morphology.},
  issue = {Pt 4},
  journaltitle = {The Journal of Physiology},
  shortjournal = {J Physiol},
  urldate = {2015-01-31},
  date = {2012-02-15},
  pages = {737--752},
  author = {Ramaswamy, Srikanth and Hill, Sean L and King, James G and Sch{\"u}rmann, Felix and Wang, Yun and Markram, Henry},
  file = {/home/fh/lib/articles/Ramaswamy2012.pdf},
  eprinttype = {pmid},
  eprint = {22083599},
  pmcid = {PMC3381307}
}

@article{Fino2011a,
  title = {Dense {{Inhibitory Connectivity}} in {{Neocortex}}},
  volume = {69},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627311001231},
  doi = {10.1016/j.neuron.2011.02.025},
  abstract = {Summary
The connectivity diagram of neocortical circuits is still unknown, and there are conflicting data as to whether cortical neurons are wired specifically or not. To investigate the basic structure of cortical microcircuits, we use a two-photon photostimulation technique that enables the systematic mapping of synaptic connections with single-cell resolution. We map the inhibitory connectivity between upper layers somatostatin-positive GABAergic interneurons and pyramidal cells in mouse frontal cortex. Most, and sometimes all, inhibitory neurons are locally connected to every sampled pyramidal cell. This dense inhibitory connectivity is found at both young and mature developmental ages. Inhibitory innervation of neighboring pyramidal cells is similar, regardless of whether they are connected among themselves or not. We conclude that local inhibitory connectivity is promiscuous, does not form subnetworks, and can approach the theoretical limit of a completely connected synaptic matrix.},
  number = {6},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2015-01-28},
  date = {2011-03-24},
  pages = {1188--1203},
  author = {Fino, Elodie and Yuste, Rafael},
  file = {/home/fh/lib/articles/Fino2011.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/4EXKPEAU/S0896627311001231.html}
}

@article{Stepanyants2008,
  langid = {english},
  title = {Local {{Potential Connectivity}} in {{Cat Primary Visual Cortex}}},
  volume = {18},
  issn = {1047-3211, 1460-2199},
  url = {http://cercor.oxfordjournals.org/content/18/1/13},
  doi = {10.1093/cercor/bhm027},
  abstract = {Time invariant description of synaptic connectivity in cortical circuits may be precluded by the ongoing growth and retraction of dendritic spines accompanied by the formation and elimination of synapses. On the other hand, the spatial arrangement of axonal and dendritic branches appears stable. This suggests that an invariant description of connectivity can be cast in terms of potential synapses, which are locations in the neuropil where an axon branch of one neuron is proximal to a dendritic branch of another neuron. In this paper, we attempt to reconstruct the potential connectivity in local cortical circuits of the cat primary visual cortex (V1). Based on multiple single-neuron reconstructions of axonal and dendritic arbors in 3 dimensions, we evaluate the expected number of potential synapses and the probability of potential connectivity among excitatory (pyramidal and spiny stellate) neurons and inhibitory basket cells. The results provide a quantitative description of structural organization of local cortical circuits. For excitatory neurons from different cortical layers, we compute local domains, which contain their potentially pre- and postsynaptic excitatory partners. These domains have columnar shapes with laminar specific radii and are roughly of the size of the ocular dominance column. Therefore, connections between most excitatory neurons in the ocular dominance column can be implemented by local synaptogenesis. Structural connectivity involving inhibitory basket cells is generally weaker than excitatory connectivity. Here, only nearby neurons are capable of establishing more than one potential synapse, implying that within the ocular dominance column these connections have more limited potential for circuit remodeling.},
  number = {1},
  journaltitle = {Cerebral Cortex},
  shortjournal = {Cereb. Cortex},
  urldate = {2015-01-28},
  date = {2008-01-01},
  pages = {13--28},
  keywords = {morphology,excitatory,inhibitory,interlaminar connectivity,neurogeometry},
  author = {Stepanyants, Armen and Hirsch, Judith A. and Martinez, Luis M. and Kisv{\'a}rday, Zolt{\'a}n F. and Ferecsk{\'o}, Alex S. and Chklovskii, Dmitri B.},
  file = {/home/fh/lib/articles/Stepanyants2008.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/87JUCGC3/13.html},
  eprinttype = {pmid},
  eprint = {17420172}
}

@article{Romand2011,
  title = {Morphological {{Development}} of {{Thick}}-{{Tufted Layer V Pyramidal Cells}} in the {{Rat Somatosensory Cortex}}},
  volume = {5},
  issn = {1662-5129},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3043270/},
  doi = {10.3389/fnana.2011.00005},
  abstract = {The thick-tufted layer V pyramidal (TTL5) neuron is a key neuron providing output from the neocortex. Although it has been extensively studied, principles governing its dendritic and axonal arborization during development are still not fully quantified. Using 3-D model neurons reconstructed from biocytin-labeled cells in the rat somatosensory cortex, this study provides a detailed morphological analysis of TTL5 cells at postnatal day (P) 7, 14, 21, 36, and 60. Three developmental periods were revealed, which were characterized by distinct growing rates and properties of alterations in different compartments. From P7 to P14, almost all compartments grew fast, and filopodia-like segments along apical dendrite disappeared; From P14 to P21, the growth was localized on specified segments of each compartment, and the densities of spines and boutons were significantly increased; From P21 to P60, the number of basal dendritic segments was significantly increased at specified branch orders, and some basal and oblique dendritic segments were lengthened or thickened. Development changes were therefore seen in two modes: the fast overall growth during the first period and the slow localized growth (thickening mainly on intermediates or lengthening mainly on terminals) at the subsequent stages. The lengthening may be accompanied by the retraction on different segments. These results reveal a differential regulation in the arborization of neuronal compartments during development, supporting the notion of functional compartmental development. This quantification provides new insight into the potential value of the TTL5 morphology for information processing, and for other purposes as well.},
  journaltitle = {Frontiers in Neuroanatomy},
  shortjournal = {Front Neuroanat},
  urldate = {2015-03-05},
  date = {2011-02-17},
  author = {Romand, Sandrine and Wang, Yun and Toledo-Rodriguez, Maria and Markram, Henry},
  file = {/home/fh/lib/articles/Romand2011.pdf},
  eprinttype = {pmid},
  eprint = {21369363},
  pmcid = {PMC3043270}
}

@article{Ohki2007,
  title = {Specificity and Randomness in the Visual Cortex},
  volume = {17},
  issn = {0959-4388},
  url = {http://www.sciencedirect.com/science/article/pii/S0959438807000906},
  doi = {10.1016/j.conb.2007.07.007},
  abstract = {Research on the functional anatomy of visual cortical circuits has recently zoomed in from the macroscopic level to the microscopic. High-resolution functional imaging has revealed that the functional architecture of orientation maps in higher mammals is built with single-cell precision. By contrast, orientation selectivity in rodents is dispersed on visual cortex in a salt-and-pepper fashion, despite highly tuned visual responses. Recent studies of synaptic physiology indicate that there are disjoint subnetworks of interconnected cells in the rodent visual cortex. These intermingled subnetworks, described in vitro, may relate to the intermingled ensembles of cells tuned to different orientations, described in vivo. This hypothesis may soon be tested with new anatomic techniques that promise to reveal the detailed wiring diagram of cortical circuits.},
  number = {4},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  series = {Sensory systems},
  urldate = {2015-01-28},
  date = {2007-08},
  pages = {401--407},
  author = {Ohki, Kenichi and Reid, R Clay},
  file = {/home/fh/lib/articles/Ohki2007.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/CDZVKC83/S0959438807000906.html}
}

@article{Markram2008,
  title = {Fixing the Location and Dimensions of Functional Neocortical Columns},
  volume = {2},
  issn = {1955-2068},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2645561/},
  doi = {10.2976/1.2919545},
  abstract = {The quest to understand the way in which neurons interconnect to form circuits that function as a unit began when Ramon y Cajal concluded that axo-dendritic apposition were too conspicuous to be incidental and proposed that two neurons must be communicating through these points of contact (see Shepherd and Erulkar, 1997, Trends Neurosci., 20, 385\textendash{}392). Lorente de N{\'o} was probably the first to predict that a defined group of vertically displaced neurons in the neocortex could form functional units (Lorente de N{\'o}, 1938, Physiology of the Nervous System, 20, OUP: 291\textendash{}330) for which Mountcastle found experimental evidence (see Mountcastle, 1997, Brain, 120, 701\textendash{}722) and which was ultimately demonstrated by Hubel and Wiesel in their elegant discovery of the orientation selective columns (Hubel and Wiesel, 1959, J. Physiol., 148, 574\textendash{}591). Until today, however, it is still not clear what shapes functional columns. Anatomical units, as in the barrel cortex, would make it easier to explain, but the neocortex is largely a continuous slab of closely packed neurons from which multiple modules emerge that can overlap partially or even completely on the same anatomical space. Are the columns in fixed anatomical locations or are they dynamically assigned and what anatomical and physiological properties are operating to shape their dimensions? A recent study explores how the geometry of single neurons places structural constraints on the dimensions of columns in the visual cortex (Stepanyants et al., 2008, Cereb Cortex, 18, 13\textendash{}24).},
  number = {3},
  journaltitle = {HFSP Journal},
  shortjournal = {HFSP J},
  urldate = {2015-01-29},
  date = {2008-06},
  pages = {132--135},
  author = {Markram, Henry},
  file = {/home/fh/lib/articles/Markram2008.pdf},
  eprinttype = {pmid},
  eprint = {19404466},
  pmcid = {PMC2645561}
}

@article{Megias2001,
  title = {Total Number and Distribution of Inhibitory and Excitatory Synapses on Hippocampal {{CA1}} Pyramidal Cells},
  volume = {102},
  issn = {0306-4522},
  url = {http://www.sciencedirect.com/science/article/pii/S0306452200004966},
  doi = {10.1016/S0306-4522(00)00496-6},
  abstract = {The integrative properties of neurons depend strongly on the number, proportions and distribution of excitatory and inhibitory synaptic inputs they receive. In this study the three-dimensional geometry of dendritic trees and the density of symmetrical and asymmetrical synapses on different cellular compartments of rat hippocampal CA1 area pyramidal cells was measured to calculate the total number and distribution of excitatory and inhibitory inputs on a single cell.

A single pyramidal cell has $\sim$12,000 $\mu$m dendrites and receives around 30,000 excitatory and 1700 inhibitory inputs, of which 40\% are concentrated in the perisomatic region and 20\% on dendrites in the stratum lacunosum-moleculare. The pre- and post-synaptic features suggest that CA1 pyramidal cell dendrites are heterogeneous. Strata radiatum and oriens dendrites are similar and differ from stratum lacunosum-moleculare dendrites. Proximal apical and basal strata radiatum and oriens dendrites are spine-free or sparsely spiny. Distal strata radiatum and oriens dendrites (forming 68.5\% of the pyramidal cells' dendritic tree) are densely spiny; their excitatory inputs terminate exclusively on dendritic spines, while inhibitory inputs target only dendritic shafts. The proportion of inhibitory inputs on distal spiny strata radiatum and oriens dendrites is low ($\sim$3\%). In contrast, proximal dendritic segments receive mostly (70\textendash{}100\%) inhibitory inputs. Only inhibitory inputs innervate the somata (77\textendash{}103 per cell) and axon initial segments. Dendrites in the stratum lacunosum-moleculare possess moderate to small amounts of spines. Excitatory synapses on stratum lacunosum-moleculare dendrites are larger than the synapses in other layers, are frequently perforated ($\sim$40\%) and can be located on dendritic shafts. Inhibitory inputs, whose percentage is relatively high ($\sim$14\textendash{}17\%), also terminate on dendritic spines.

Our results indicate that: (i) the highly convergent excitation arriving onto the distal dendrites of pyramidal cells is primarily controlled by proximally located inhibition; (ii) the organization of excitatory and inhibitory inputs in layers receiving Schaffer collateral input (radiatum/oriens) versus perforant path input (lacunosum-moleculare) is significantly different.},
  number = {3},
  journaltitle = {Neuroscience},
  shortjournal = {Neuroscience},
  urldate = {2015-02-05},
  date = {2001-02-05},
  pages = {527--540},
  keywords = {synaptic convergence,dendrite geometry,serial reconstruction,3D,electron microscopy,database for modeling},
  author = {Meg{\i}\'as, M and Emri, Zs and Freund, T. F and Guly{\'a}s, A. I},
  file = {/home/fh/lib/articles/Megı́as2001.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/SUJWNAI4/S0306452200004966.html}
}

@article{Feldmeyer2002,
  langid = {english},
  title = {Synaptic Connections between Layer 4 Spiny Neurone- Layer 2/3 Pyramidal Cell Pairs in Juvenile Rat Barrel Cortex: Physiology and Anatomy of Interlaminar Signalling within a Cortical Column},
  volume = {538},
  issn = {1469-7793},
  url = {http://onlinelibrary.wiley.com/doi/10.1113/jphysiol.2001.012959/abstract},
  doi = {10.1113/jphysiol.2001.012959},
  shorttitle = {Synaptic Connections between Layer 4 Spiny Neurone- Layer 2/3 Pyramidal Cell Pairs in Juvenile Rat Barrel Cortex},
  abstract = {Whole-cell voltage recordings were obtained from 64 synaptically coupled excitatory layer 4 (L4) spiny neurones and L2/3 pyramidal cells in acute slices of the somatosensory cortex (`barrel' cortex) of 17- to 23-days-old rats. Single action potentials (APs) in the L4 spiny neurone evoked single unitary EPSPs in the L2/3 pyramidal cell with a peak amplitude of 0.7 $\pm$ 0.6 mV. The average latency was 2.1 $\pm$ 0.6 ms, the rise time was 0.8 $\pm$ 0.3 ms and the decay time constant was 12.7 $\pm$ 3.5 ms. The percentage of failures of an AP in a L4 spiny neurone to evoke a unitary EPSP in the L2/3 pyramidal cell was 4.9 $\pm$ 8.8 \% and the coefficient of variation (c.v.) of the unitary EPSP amplitude was 0.27 $\pm$ 0.13. Both c.v. and percentage of failures decreased with increased average EPSP amplitude. Postsynaptic glutamate receptors (GluRs) in L2/3 pyramidal cells were of the N-methyl-d-aspartate (NMDA) receptor (NMDAR) and the non-NMDAR type. At -60 mV in the presence of extracellular Mg2+ (1 mm), 29 $\pm$ 15 \% of the EPSP voltage-time integral was blocked by NMDAR antagonists. In 0 Mg2+, the NMDAR/AMPAR ratio of the EPSC was 0.50 $\pm$ 0.29, about half the value obtained for L4 spiny neurone connections. Burst stimulation of L4 spiny neurones showed that EPSPs in L2/3 pyramidal cells depressed over a wide range of frequencies (1\textendash{}100 s-1). However, at higher frequencies (30 s-1) EPSP summation overcame synaptic depression so that the summed EPSP was larger than the first EPSP amplitude in the train. The number of putative synaptic contacts established by the axonal collaterals of the L4 projection neurone with the target neurone in layer 2/3 varied between 4 and 5, with an average of 4.5 $\pm$ 0.5 (n= 13 pairs). Synapses were established on basal dendrites of the pyramidal cell. Their mean geometric distance from the pyramidal cell soma was 67 $\pm$ 34 $\mu$m (range, 16\textendash{}196 $\mu$m). The results suggest that each connected L4 spiny neurone produces a weak but reliable EPSP in the pyramidal cell. Therefore transmission of signals to layer 2/3 is likely to have a high threshold requiring simultaneous activation of many L4 neurons, implying that L4 spiny neurone to L2/3 pyramidal cell synapses act as a gate for the lateral spread of excitation in layer 2/3.},
  number = {3},
  journaltitle = {The Journal of Physiology},
  urldate = {2015-01-29},
  date = {2002},
  pages = {803--822},
  author = {Feldmeyer, Dirk and L{\"u}bke, Joachim and Silver, R. Angus and Sakmann, Bert},
  file = {/home/fh/lib/articles/Feldmeyer2002.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/GMJHB7GE/abstract.html}
}

@book{Klenke2006,
  langid = {Deutsch},
  location = {{Berlin}},
  title = {Wahrscheinlichkeitstheorie},
  edition = {Auflage: 1},
  isbn = {978-3-540-25545-1},
  abstract = {Dieses Lehrbuch bietet eine umfassende moderne Einf{\"u}hrung in die wichtigsten Gebiete der Wahrscheinlichkeitstheorie und ihre ma{\ss}theoretischen Grundlagen. Themenschwerpunkte sind u.a.: Ma{\ss}- und Integrationstheorie, Grenzwerts{\"a}tze f{\"u}r Summen von Zufallsvariablen, Martingale oder Perkolation. {\"U}ber 200 {\"U}bungsaufgaben und zahlreiche Abbildungen runden die Darstellung ab. Breite und Auswahl der Themen sind einmalig in der deutschsprachigen Literatur.},
  pagetotal = {602},
  publisher = {{Springer}},
  date = {2006},
  author = {Klenke, Achim},
  file = {/home/fh/lib/books/Klenke2006_Wahrscheinlichkeitstheorie.pdf}
}

@book{Rudin1991,
  langid = {english},
  location = {{New York}},
  title = {Functional {{Analysis}}},
  edition = {2Rev Ed edition},
  isbn = {978-0-07-100944-7},
  pagetotal = {424},
  publisher = {{McGraw-Hill Science/Engineerin}},
  date = {1991},
  author = {Rudin, Walter},
  file = {/home/fh/lib/books/Rudin1991_Functional-Analysis.pdf}
}

@article{Karube2004,
  langid = {english},
  title = {Axon {{Branching}} and {{Synaptic Bouton Phenotypes}} in {{GABAergic Nonpyramidal Cell Subtypes}}},
  volume = {24},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/24/12/2853},
  doi = {10.1523/JNEUROSCI.4814-03.2004},
  abstract = {GABAergic nonpyramidal cells, cortical interneurons, consist of heterogeneous subtypes differing in their axonal field and target selectivity. It remains to be investigated how the diverse innervation patterns are generated and how these spatially complicated, but synaptically specific wirings are achieved. Here, we asked whether a particular cell type obeys a specific branching and bouton arrangement principle or differs from others only in average morphometric values of the morphological template common to nonpyramidal cells. For this purpose, we subclassified nonpyramidal cells within each physiological class by quantitative parameters of somata, dendrites, and axons and characterized axon branching and bouton distribution patterns quantitatively. Each subtype showed a characteristic set of vertical and horizontal bouton spreads around the somata. Each parameter, such as branching angles, internode or interbouton intervals, followed its own characteristic distribution pattern irrespective of subtypes, suggesting that nonpyramidal cells have the common mechanism for formation of the axon branching pattern and bouton arrangement. Fitting of internode and interbouton interval distributions to the exponential indicated their apparent random occurrence. Decay constants of the fitted exponentials varied among nonpyramidal cells, but each subtype expressed a particular set of interbouton and internode interval averages. The distinctive combination of innervation field shape and local axon phenotypes suggests a marked functional difference in the laminar and columnar integration properties of different GABAergic subtypes, as well as the subtype-specific density of inhibited targets.},
  number = {12},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2015-01-29},
  date = {2004-03-24},
  pages = {2853--2865},
  keywords = {frontal cortex,interneuron,GABA,synaptic bouton,fast-spiking cell,late-spiking cell},
  author = {Karube, Fuyuki and Kubota, Yoshiyuki and Kawaguchi, Yasuo},
  file = {/home/fh/lib/articles/Karube2004.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/HEX8BIAZ/2853.html},
  eprinttype = {pmid},
  eprint = {15044524}
}

@article{Martin2014,
  langid = {english},
  title = {Superficial Layer Pyramidal Cells Communicate Heterogeneously between Multiple Functional Domains of Cat Primary Visual Cortex},
  volume = {5},
  url = {http://www.nature.com/ncomms/2014/141024/ncomms6252/full/ncomms6252.html},
  doi = {10.1038/ncomms6252},
  abstract = {The axons of pyramidal neurons in the superficial layers of the neocortex of higher mammals form lateral networks of discrete clusters of synaptic boutons. In primary visual cortex the clusters are reported to link domains that share the same orientation preferences, but how individual neurons contribute to this network is unknown. Here we performed optical imaging to record the intrinsic signal, which is an indirect measure of neuronal firing, and determined the global map of orientation preferences in the cat primary visual system. In the same experiment, single cells were recorded and labelled intracellularly. We found that individual axons arborise within the retinotopic representation of the classical receptive field, but their bouton clusters were not aligned along their preferred axis of orientation along the retinotopic map. Axon clusters formed in a variety of different orientation domains, not just the like-orientation domains. This topography and heterogeneity of single-cell connectivity provides circuits for normalization and context-dependent feature processing of visual scenes.},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  urldate = {2015-01-29},
  date = {2014-10-24},
  keywords = {Neuroscience,Biological sciences},
  author = {Martin, Kevan A. C. and Roth, Stephan and Rusch, Elisha S.},
  file = {/home/fh/lib/articles/Martin2014.pdf}
}

@article{Spruston2008,
  langid = {english},
  title = {Pyramidal Neurons: Dendritic Structure and Synaptic Integration},
  volume = {9},
  issn = {1471-003X},
  url = {http://www.nature.com/nrn/journal/v9/n3/full/nrn2286.html},
  doi = {10.1038/nrn2286},
  shorttitle = {Pyramidal Neurons},
  abstract = {Pyramidal neurons are characterized by their distinct apical and basal dendritic trees and the pyramidal shape of their soma. They are found in several regions of the CNS and, although the reasons for their abundance remain unclear, functional studies \textemdash{} especially of CA1 hippocampal and layer V neocortical pyramidal neurons \textemdash{} have offered insights into the functions of their unique cellular architecture. Pyramidal neurons are not all identical, but some shared functional principles can be identified. In particular, the existence of dendritic domains with distinct synaptic inputs, excitability, modulation and plasticity appears to be a common feature that allows synapses throughout the dendritic tree to contribute to action-potential generation. These properties support a variety of coincidence-detection mechanisms, which are likely to be crucial for synaptic integration and plasticity.},
  number = {3},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  urldate = {2015-01-29},
  date = {2008-03},
  pages = {206--221},
  author = {Spruston, Nelson},
  file = {/home/fh/lib/articles/Spruston2008.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/JHPUPAXD/nrn2286.html}
}

@article{Kleindienst2011a,
  title = {Activity-{{Dependent Clustering}} of {{Functional Synaptic Inputs}} on {{Developing Hippocampal Dendrites}}},
  volume = {72},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627311009263},
  doi = {10.1016/j.neuron.2011.10.015},
  abstract = {Summary
During brain development, before sensory systems become functional, neuronal networks spontaneously generate repetitive bursts of neuronal activity, which are typically synchronized across many neurons. Such activity patterns have been described on the level of networks and cells, but the fine-structure of inputs received by an individual neuron during spontaneous network activity has not been studied. Here, we used calcium imaging to record activity at many synapses of hippocampal pyramidal neurons simultaneously to establish the activity patterns in the majority of synapses of an entire cell. Analysis of the spatiotemporal patterns of synaptic activity revealed a fine-scale connectivity rule: neighboring synapses (\&lt;16~$\mu$m intersynapse distance) are more likely to be coactive than synapses that are farther away from each other. Blocking spiking activity or NMDA receptor activation revealed that the clustering of synaptic inputs required neuronal activity, demonstrating a role of developmentally expressed spontaneous activity for connecting neurons with subcellular precision.},
  number = {6},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2015-02-05},
  date = {2011-12-22},
  pages = {1012--1024},
  author = {Kleindienst, Thomas and Winnubst, Johan and Roth-Alpermann, Claudia and Bonhoeffer, Tobias and Lohmann, Christian},
  file = {/home/fh/lib/articles/Kleindienst20112.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/DTS7BTCK/S0896627311009263.html}
}

@book{Sporns2010,
  langid = {english},
  location = {{Cambridge, Mass}},
  title = {Networks of the {{Brain}}},
  edition = {1 edition},
  isbn = {978-0-262-01469-4},
  abstract = {Over the last decade, the study of complex networks has expanded across                 diverse scientific fields. Increasingly, science is concerned with the structure,                 behavior, and evolution of complex systems ranging from cells to ecosystems. Modern                 network approaches are beginning to reveal fundamental principles of brain                 architecture and function, and in Networks of the Brain, Olaf                 Sporns describes how the integrative nature of brain function can be illuminated                 from a complex network perspective. Highlighting the many emerging points of contact                 between neuroscience and network science, the book serves to introduce network                 theory to neuroscientists and neuroscience to those working on theoretical network                 models.Brain networks span the microscale of individual cells and                 synapses and the macroscale of cognitive systems and embodied cognition. Sporns                 emphasizes how networks connect levels of organization in the brain and how they                 link structure to function. In order to keep the book accessible and focused on the                 relevance to neuroscience of network approaches, he offers an informal and                 nonmathematical treatment of the subject. After describing the basic concepts of                 network theory and the fundamentals of brain connectivity, Sporns discusses how                 network approaches can reveal principles of brain architecture. He describes new                 links between network anatomy and function and investigates how networks shape                 complex brain dynamics and enable adaptive neural computation. The book documents                 the rapid pace of discovery and innovation while tracing the historical roots of the                 field. The study of brain connectivity has already opened new                 avenues of study in neuroscience. Networks of the Brain offers a                 synthesis of the sciences of complex networks and the brain that will be an                 essential foundation for future research.},
  pagetotal = {424},
  publisher = {{The MIT Press}},
  date = {2010-10-01},
  author = {Sporns, Olaf},
  file = {/home/fh/lib/books/Sporns2010_Networks-of-the-Brain.pdf}
}

@book{Daley2003,
  langid = {english},
  location = {{New York}},
  title = {An {{Introduction}} to the {{Theory}} of {{Point Processes}}, {{Volume}} 1},
  edition = {2nd edition},
  isbn = {978-0-387-95541-4},
  abstract = {Point processes and random measures find wide applicability in telecommunications, earthquakes, image analysis, spatial point patterns, and stereology, to name but a few areas. The authors have made a major reshaping of their work in their first edition of 1988 and now present their Introduction to the Theory of Point Processes in two volumes with sub-titles Elementary Theory and Models and General Theory and Structure.Volume One contains the introductory chapters from the first edition, together with an informal treatment of some of the later material intended to make it more accessible to readers primarily interested in models and applications. The main new material in this volume relates to marked point processes and to processes evolving in time, where the conditional intensity methodology provides a basis for model building, inference, and prediction. There are abundant examples whose purpose is both didactic and to illustrate further applications of the ideas and models that are the main substance of the text.},
  pagetotal = {471},
  publisher = {{Springer}},
  date = {2003-11-14},
  author = {Daley, D. J. and Vere-Jones, D.},
  file = {/home/fh/lib/books/Daley2003_An-Introduction-to-the-Theory-of-Point-Processes,-Volume-1.pdf}
}

@book{Gerstner2002,
  langid = {english},
  location = {{Cambridge, U.K. ; New York}},
  title = {Spiking {{Neuron Models}}: {{Single Neurons}}, {{Populations}}, {{Plasticity}}},
  edition = {1 edition},
  isbn = {978-0-521-89079-3},
  shorttitle = {Spiking {{Neuron Models}}},
  abstract = {This introduction to spiking neurons can be used in advanced-level courses in computational neuroscience, theoretical biology, neural modeling, biophysics, or neural networks. It focuses on phenomenological approaches rather than detailed models in order to provide the reader with a conceptual framework. The authors formulate the theoretical concepts clearly without many mathematical details. While the book contains standard material for courses in computational neuroscience, neural modeling, or neural networks, it also provides an entry to current research. No prior knowledge beyond undergraduate mathematics is required.},
  pagetotal = {496},
  publisher = {{Cambridge University Press}},
  date = {2002-08-26},
  author = {Gerstner, Wulfram and Kistler, Werner M.},
  file = {/home/fh/lib/books/Gerstner2002_Spiking-Neuron-Models-Single-Neurons,-Populations,-Plasticity.pdf}
}

@collection{Chaovalitwongse2010,
  langid = {english},
  location = {{New York}},
  title = {Computational {{Neuroscience}}},
  edition = {2010 edition},
  isbn = {978-0-387-88629-9},
  abstract = {This volume includes contributions from diverse disciplines including electrical engineering, biomedical engineering, industrial engineering, and medicine, bridging a vital gap between the mathematical sciences and neuroscience research. Covering a wide range of research topics, this volume demonstrates how various methods from data mining, signal processing, optimization and cutting-edge medical techniques can be used to tackle the most challenging problems in~modern neuroscience.},
  pagetotal = {396},
  publisher = {{Springer}},
  date = {2010-05-12},
  editor = {Chaovalitwongse, Wanpracha and Pardalos, Panos M. and Xanthopoulos, Petros},
  file = {/home/fh/lib/books/Chaovalitwongse2010_Computational-Neuroscience.pdf}
}

@book{Feller1968,
  langid = {english},
  location = {{New York}},
  title = {An {{Introduction}} to {{Probability Theory}} and {{Its Applications}}, {{Vol}}. 1, 3rd {{Edition}}},
  edition = {3rd edition},
  isbn = {978-0-471-25708-0},
  abstract = {Major changes in this edition include the substitution of probabilistic arguments for combinatorial artifices, and the addition of new sections on branching processes, Markov chains, and the De Moivre-Laplace theorem.},
  pagetotal = {509},
  publisher = {{Wiley}},
  date = {1968},
  author = {Feller, William},
  file = {/home/fh/lib/books/Feller1968_An-Introduction-to-Probability-Theory-and-Its-Applications,-Vol.-1,-3rd-Edition.pdf}
}

@article{Hellwig1994,
  langid = {english},
  title = {Synapses on Axon Collaterals of Pyramidal Cells Are Spaced at Random Intervals: A {{Golgi}} Study in the Mouse Cerebral Cortex},
  volume = {71},
  issn = {0340-1200, 1432-0770},
  url = {http://link.springer.com/article/10.1007/BF00198906},
  doi = {10.1007/BF00198906},
  shorttitle = {Synapses on Axon Collaterals of Pyramidal Cells Are Spaced at Random Intervals},
  abstract = {In this study we investigated the arrangement of synapses on local axon collaterals of Golgi-stained pyramidal neurons in the mouse cerebral cortex. As synaptic markers we considered axonal swellings visible at high magnification under the light microscope. Such axonal swellings coincide with synaptic boutons, as has been demonstrated in a number of combined light and electron microscopic studies. These studies also indicated that, in most cases, one bouton corresponds precisely to one synapse. Golgi-impregnated axonal trees of 20 neocortical pyramidal neurons were drawn with a camera lucida. Axonal swellings were marked on the drawings. Most swellings were `en passant'; occasionally, they were situated at the tip of short, spine-like processes. On axon collaterals, the average interval between swellings was 4.5 $\mu$m. On the axonal main stem, the swellings were always less densely packed than on the collaterals. Statistical analysis of the spatial distribution of the swellings did not reveal any special patterns. Instead, the arrangement of swellings on individual collaterals follows a Poisson distribution. Moreover, the same holds to a large extent for the entire collection of pyramidal cell collaterals. This suggests that a single Poisson process, characterized by only one rate parameter (number of synapses per unit length), describes most of the spatial distribution of synapses along pyramidal cell collaterals. These findings do not speak in favour of a pronounced target specificity of pyramidal neurons at the synaptic level. Instead, our results support a probabilistic model of cortical connectivity.},
  number = {1},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybern.},
  urldate = {2015-01-29},
  date = {1994-05-01},
  pages = {1--12},
  keywords = {Bioinformatics,Computer Appl. in Life Sciences,Neurobiology},
  author = {Hellwig, Bernhard and Sch{\"u}z, Almut and Aertsen, Ad},
  file = {/home/fh/lib/articles/Hellwig1994.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/75HB8WU3/BF00198906.html}
}

@article{Branco2011,
  title = {Synaptic {{Integration Gradients}} in {{Single Cortical Pyramidal Cell Dendrites}}},
  volume = {69},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627311001036},
  doi = {10.1016/j.neuron.2011.02.006},
  abstract = {Summary
Cortical pyramidal neurons receive thousands of synaptic inputs arriving at different dendritic locations with varying degrees of temporal synchrony. It is not known if different locations along single cortical dendrites integrate excitatory inputs in different ways. Here we have used two-photon glutamate uncaging and compartmental modeling to reveal a gradient of nonlinear synaptic integration in basal and apical oblique dendrites of cortical pyramidal neurons. Excitatory inputs to the proximal dendrite sum linearly and require precise temporal coincidence for effective summation, whereas distal inputs are amplified with high gain and integrated over broader time windows. This allows distal inputs to overcome their electrotonic disadvantage, and become surprisingly more effective than proximal inputs at influencing action potential output. Thus, single dendritic branches can already exhibit nonuniform synaptic integration, with the computational strategy shifting from temporal coding to rate coding along the dendrite.},
  number = {5},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2015-02-05},
  date = {2011-03-10},
  pages = {885--892},
  author = {Branco, Tiago and H{\"a}usser, Michael},
  file = {/home/fh/lib/articles/Branco2011.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/6EC8Z2GX/S0896627311001036.html}
}

@article{Katz2009,
  title = {Synapse {{Distribution Suggests}} a {{Two}}-{{Stage Model}} of {{Dendritic Integration}} in {{CA1 Pyramidal Neurons}}},
  volume = {63},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627309005108},
  doi = {10.1016/j.neuron.2009.06.023},
  abstract = {Summary
Competing models have been proposed to explain how neurons integrate the thousands of inputs distributed throughout their dendritic trees. In~a simple global integration model, inputs from all locations sum in the axon. In a two-stage integration model, inputs contribute directly to dendritic spikes, and outputs from multiple branches sum in the axon. These two models yield opposite predictions of how synapses at different dendritic locations should be scaled if they are to contribute equally to neuronal output. We used serial-section electron microscopy to reconstruct individual apical oblique dendritic branches of CA1 pyramidal neurons and observe a synapse distribution consistent with the two-stage integration model. Computational modeling suggests that the observed synapse distribution enhances the~contribution of each dendritic branch to neuronal output.},
  number = {2},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2015-02-05},
  date = {2009-07-30},
  pages = {171--177},
  keywords = {EVO_ECOL,SIGNALING,CELLBIO},
  author = {Katz, Yael and Menon, Vilas and Nicholson, Daniel A. and Geinisman, Yuri and Kath, William L. and Spruston, Nelson},
  file = {/home/fh/lib/articles/Katz2009.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/K2CWJTWS/S0896627309005108.html}
}

@book{Neuroscience2002,
  langid = {english},
  location = {{Washington, D.C}},
  title = {Brain {{Facts}}: {{A Primer}} on the {{Brain}} and {{Nervous System}}},
  edition = {Fourth Edition edition},
  isbn = {978-0-916110-00-0},
  shorttitle = {Brain {{Facts}}},
  abstract = {Book by Society for Neuroscience},
  pagetotal = {53},
  publisher = {{The Society For Neuroscience}},
  date = {2002},
  author = {for Neuroscience, The Society},
  file = {/home/fh/lib/books/Neuroscience2002_Brain-Facts-A-Primer-on-the-Brain-and-Nervous-System.pdf}
}

@article{Dahlhaus1997,
  title = {Identification of Synaptic Connections in Neural Ensembles by Graphical Models},
  volume = {77},
  issn = {0165-0270},
  url = {http://www.sciencedirect.com/science/article/pii/S0165027097001003},
  doi = {10.1016/S0165-0270(97)00100-3},
  abstract = {A method for the identification of direct synaptic connections in a larger neural net is presented. It is based on a conditional correlation graph for multivariate point processes. The connections are identified via the partial spectral coherence of two neurons, given all others. It is shown how these coherences can be calculated by inversion of the spectral density matrix. In simulations with GENESIS, we discuss the relevance of the method for identifying different neural ensembles including an excitatory feedback loop and networks with lateral inhibitions.},
  number = {1},
  journaltitle = {Journal of Neuroscience Methods},
  shortjournal = {Journal of Neuroscience Methods},
  urldate = {2015-03-25},
  date = {1997-11-07},
  pages = {93--107},
  keywords = {Neuronal net,Synaptic connectivity,Partial spectral coherence,Multivariate point processes,Graphical models},
  author = {Dahlhaus, Rainer and Eichler, Michael and Sandk{\"u}hler, J{\"u}rgen},
  file = {/home/fh/lib/articles/Dahlhaus1997.pdf}
}

@article{Tartaglia2009,
  title = {Perceptual Learning and Roving: {{Stimulus}} Types and Overlapping Neural Populations},
  volume = {49},
  issn = {0042-6989},
  url = {http://www.sciencedirect.com/science/article/pii/S0042698909000583},
  doi = {10.1016/j.visres.2009.02.013},
  shorttitle = {Perceptual Learning and Roving},
  abstract = {In perceptual learning, performance usually improves when observers train with one type of stimulus, for example, a bisection stimulus. Roving denotes the situation when, instead of one, two or more types of stimuli are presented randomly interleaved, for example, a bisection stimulus and a vernier. For some combinations of stimulus types, performance improves in roving situations whereas for others it does not. To investigate when roving impedes perceptual learning, we conducted four experiments. Performance improved, for example, when we roved a bisection stimulus and a vernier but not when we roved certain types of bisection stimuli. We propose that roving hinders perceptual learning when the stimulus types are clearly distinct from each other but still excite overlapping but not identical neural populations.},
  number = {11},
  journaltitle = {Vision Research},
  shortjournal = {Vision Research},
  urldate = {2015-05-23},
  date = {2009-06},
  pages = {1420--1427},
  keywords = {Bisection task,Hyperacuity,Spatial vision},
  author = {Tartaglia, Elisa M. and Aberg, Kristoffer C. and Herzog, Michael H.},
  file = {/home/fh/lib/articles/Tartaglia2009.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/UPETUW22/S0042698909000583.html}
}

@article{Heeger2000,
  title = {Poisson Model of Spike Generation},
  volume = {5},
  journaltitle = {Handout, University of Standford},
  date = {2000},
  author = {Heeger, David},
  file = {/home/fh/lib/articles/Heeger2000.pdf}
}

@article{Herzog2011,
  title = {Perceptual Learning, Roving and the Unsupervised Bias},
  volume = {61},
  issn = {0042-6989},
  url = {http://www.sciencedirect.com/science/article/pii/S0042698911003816},
  doi = {10.1016/j.visres.2011.11.001},
  abstract = {Perceptual learning improves perception through training. Perceptual learning improves with most stimulus types but fails when certain stimulus types are mixed during training (roving). This result is surprising because classical supervised and unsupervised neural network models can cope easily with roving conditions. What makes humans so inferior compared to these models? As experimental and conceptual work has shown, human perceptual learning is neither supervised nor unsupervised but reward-based learning. Reward-based learning suffers from the so-called unsupervised bias, i.e., to prevent synaptic ``drift'', the average reward has to be exactly estimated. However, this is impossible when two or more stimulus types with different rewards are presented during training (and the reward is estimated by a running average). For this reason, we propose no learning occurs in roving conditions. However, roving hinders perceptual learning only for combinations of similar stimulus types but not for dissimilar ones. In this latter case, we propose that a critic can estimate the reward for each stimulus type separately. One implication of our analysis is that the critic cannot be located in the visual system.},
  journaltitle = {Vision Research},
  shortjournal = {Vision Research},
  series = {Perceptual Learning - mechanisms and manifestations},
  urldate = {2015-05-23},
  date = {2011-11-19},
  pages = {95--99},
  keywords = {neural networks,Bisection stimuli,Perceptual learning,Roving},
  author = {Herzog, Michael H. and Aberg, Kristoffer C. and Fr{\'e}maux, Nicolas and Gerstner, Wulfram and Sprekeler, Henning},
  file = {/home/fh/lib/articles/Herzog2011.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/VAQMV5FV/S0042698911003816.html}
}

@unpublished{Pfaffelhuber2014,
  title = {Stochastische {{Prozesse}}},
  date = {2014},
  author = {Pfaffelhuber, Peter},
  file = {/home/fh/lib/manuscripts/Pfaffelhuber2014_Stochastische-Prozesse.pdf}
}

@article{Krone1986,
  langid = {english},
  title = {Spatiotemporal {{Receptive Fields}}: {{A Dynamical Model Derived}} from {{Cortical Architectonics}}},
  volume = {226},
  issn = {0080-4649},
  url = {http://rspb.royalsocietypublishing.org/content/226/1245/421},
  doi = {10.1098/rspb.1986.0002},
  shorttitle = {Spatiotemporal {{Receptive Fields}}},
  abstract = {We assume that the mammalian neocortex is built up out of some six layers which differ in their morphology and their external connections. Intrinsic connectivity is largely excitatory, leading to a considerable amount of positive feedback. The majority of cortical neurons can be divided into two main classes: the pyramidal cells, which are said to be excitatory, and local cells (most notably the non-spiny stellate cells), which are said to be inhibitory. The form of the dendritic and axonal arborizations of both groups is discussed in detail. This results in a simplified model of the cortex as a stack of six layers with mutual connections determined by the principles of fibre anatomy. This stack can be treated as a multi-input-multi-output system by means of the linear systems theory of homogeneous layers. The detailed equations for the simulation are derived in the Appendix. The results of the simulations show that the temporal and spatial behaviour of an excitation distribution cannot be treated separately. Further, they indicate specific processing in the different layers and some independence from details of wiring. Finally, the simulation results are applied to the theory of visual receptive fields. This yields some insight into the mechanisms possibly underlying hypercomplexity, putative nonlinearities, lateral inhibition, oscillating cell responses, and velocity-dependent tuning curves.},
  number = {1245},
  journaltitle = {Proceedings of the Royal Society of London B: Biological Sciences},
  urldate = {2015-04-19},
  date = {1986-01-22},
  pages = {421--444},
  author = {Krone, G. and Mallot, H. and Palm, G. and Schuz, A.},
  file = {/home/fh/lib/articles/Krone1986.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/SZWFJZB6/421.html},
  eprinttype = {pmid},
  eprint = {2869496}
}

@article{Hopfield1982,
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities.},
  volume = {79},
  issn = {0027-8424},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238/},
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
  number = {8},
  journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
  shortjournal = {Proc Natl Acad Sci U S A},
  urldate = {2015-04-20},
  date = {1982-04},
  pages = {2554--2558},
  author = {Hopfield, J J},
  file = {/home/fh/lib/articles/Hopfield1982.pdf},
  eprinttype = {pmid},
  eprint = {6953413},
  pmcid = {PMC346238}
}

@article{Miller1999,
  langid = {english},
  title = {Is the Development of Orientation Selectivity Instructed by Activity?},
  volume = {41},
  issn = {1097-4695},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1097-4695(199910)41:1<44::AID-NEU7>3.0.CO;2-V/abstract},
  doi = {10.1002/(SICI)1097-4695(199910)41:1<44::AID-NEU7>3.0.CO;2-V},
  abstract = {Is the development of orientation selectivity in visual cortex instructed by the patterns of neural activity of input neurons? We review evidence as to the role of activity, review models of activity-instructed development, and discuss how these models can be tested. The models can explain the normal development of simple cells with binocularly matched orientation preferences, the effects of monocular deprivation and reverse suture on the orientation map, and the development of a full intracortical circuit sufficient to explain mature response properties including the contrast-invariance of orientation tuning. Existing experiments are consistent with the models, in that (a) selective blockade of ON-center ganglion cells, which will degrade or eliminate the information predicted to drive development of orientation selectivity, in fact prevents development of orientation selectivity; and (b) the spontaneous activities of inputs serving the two eyes are correlated in the lateral geniculate nucleus at appropriate developmental times, as was predicted to be required to achieve binocular matching of preferred orientations. However, definitive tests remain to be done to firmly establish the instructive rather than simply permissive role of activity and determine whether the retinotopically and center type\textendash{}specific patterns of activity predicted by the models actually exist. We conclude by critically examining alternative scenarios for the development of orientation selectivity and maps, including the idea that maps are genetically prespecified. \textcopyright{} 1999 John Wiley \& Sons, Inc. J Neurobiol 41: 44\textendash{}57, 1999},
  number = {1},
  journaltitle = {Journal of Neurobiology},
  shortjournal = {J. Neurobiol.},
  urldate = {2015-06-06},
  date = {1999-10-01},
  pages = {44--57},
  keywords = {Hebbian synaptic plasticity,LGN spontaneous activity,orientation maps,simple cells,visual cortex},
  author = {Miller, Kenneth D. and Erwin, Ed and Kayser, Andrew},
  file = {/home/fh/lib/articles/Miller1999.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/ABFTHQ33/abstract\;jsessionid=B828DEC5AD129ED6573D3C3F5DF13D82.html}
}

@unpublished{Hoffmann2015h,
  title = {Stochastische {{Prozesse}}},
  date = {2015},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2015_Stochastische-Prozesse.pdf}
}

@article{Erdi2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.5909},
  title = {Teaching {{Computational Neuroscience}}},
  issn = {1871-4080, 1871-4099},
  url = {http://arxiv.org/abs/1412.5909},
  doi = {10.1007/s11571-015-9340-6.},
  abstract = {The problems and beauty of teaching computational neuroscience are discussed by reviewing three new textbooks.},
  journaltitle = {Cognitive Neurodynamics},
  urldate = {2015-06-06},
  date = {2015-03-21},
  keywords = {Quantitative Biology - Neurons and Cognition},
  author = {{\'E}rdi, P{\'e}ter},
  file = {/home/fh/lib/articles/Érdi2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/UI68KA8S/1412.html}
}

@article{Heju2015,
  title = {Helmholtz {{Juniors PhD}} Survey 2014},
  date = {2015},
  author = {Heju},
  file = {/home/fh/lib/articles/Heju2015.pdf}
}

@article{Cooper1979,
  langid = {english},
  title = {A Theory for the Acquisition and Loss of Neuron Specificity in Visual Cortex},
  volume = {33},
  issn = {0340-1200, 1432-0770},
  url = {http://link.springer.com/article/10.1007/BF00337414},
  doi = {10.1007/BF00337414},
  abstract = {We assume that between lateral geniculate and visual cortical cells there exist labile synapses that modify themselves in a new fashion called threshold passive modification and in addition, non-labile synapses that contain permanent information. In the theory which results there is an increase in the specificity of response of a cortical cell when it is exposed to stimuli due to normal patterned visual experience. Non-patterned input, such as might be expected when an animal is dark-reared or raised with eyelids sutured, results in a loss of specificity, with details depending on whether noise to labile and non-labile junctions is correlated. Specificity can sometimes be regained, however, with a return of input due to patterned vision. We propose that this provides a possible explanation of experimental results obtained by Imbert and Buisseret (1975); Blakemore and Van Sluyters (1975); Buisseret and Imbert (1976); and Fr{\'e}gnac and Imbert (1977, 1978).},
  number = {1},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybernetics},
  urldate = {2015-04-20},
  date = {1979-03-01},
  pages = {9--28},
  keywords = {Neurosciences,Zoology},
  author = {Cooper, Leon N. and Liberman, Fishel and Oja, Erkki},
  file = {/home/fh/lib/articles/Cooper1979.pdf;/home/fh/lib/articles/Cooper19792.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/W78EN59B/BF00337414.html}
}

@article{London2010,
  title = {Sensitivity to Perturbations in Vivo Implies High Noise and Suggests Rate Coding in Cortex},
  volume = {466},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/nature09086},
  doi = {10.1038/nature09086},
  number = {7302},
  journaltitle = {Nature},
  urldate = {2015-10-25},
  date = {2010-07-01},
  pages = {123--127},
  author = {London, Michael and Roth, Arnd and Beeren, Lisa and H{\"a}usser, Michael and Latham, Peter E.},
  file = {/home/fh/lib/articles/London2010.pdf}
}

@unpublished{Depperschmidt2011,
  title = {Markovketten},
  pagetotal = {60},
  date = {2011},
  author = {Depperschmidt, Andrej},
  file = {/home/fh/lib/manuscripts/Depperschmidt2011_Markovketten.pdf}
}

@article{Egger2014,
  title = {Generation of Dense Statistical Connectomes from Sparse Morphological Data},
  volume = {8},
  issn = {1662-5129},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4226167/},
  doi = {10.3389/fnana.2014.00129},
  abstract = {Sensory-evoked signal flow, at cellular and network levels, is primarily determined by the synaptic wiring of the underlying neuronal circuitry. Measurements of synaptic innervation, connection probabilities and subcellular organization of synaptic inputs are thus among the most active fields of research in contemporary neuroscience. Methods to measure these quantities range from electrophysiological recordings over reconstructions of dendrite-axon overlap at light-microscopic levels to dense circuit reconstructions of small volumes at electron-microscopic resolution. However, quantitative and complete measurements at subcellular resolution and mesoscopic scales to obtain all local and long-range synaptic in/outputs for any neuron within an entire brain region are beyond present methodological limits. Here, we present a novel concept, implemented within an interactive software environment called NeuroNet, which allows (i) integration of sparsely sampled (sub)cellular morphological data into an accurate anatomical reference frame of the brain region(s) of interest, (ii) up-scaling to generate an average dense model of the neuronal circuitry within the respective brain region(s) and (iii) statistical measurements of synaptic innervation between all neurons within the model. We illustrate our approach by generating a dense average model of the entire rat vibrissal cortex, providing the required anatomical data, and illustrate how to measure synaptic innervation statistically. Comparing our results with data from paired recordings in vitro and in vivo, as well as with reconstructions of synaptic contact sites at light- and electron-microscopic levels, we find that our in silico measurements are in line with previous results.},
  journaltitle = {Frontiers in Neuroanatomy},
  shortjournal = {Front Neuroanat},
  urldate = {2015-12-06},
  date = {2014-11-10},
  author = {Egger, Robert and Dercksen, Vincent J. and Udvary, Daniel and Hege, Hans-Christian and Oberlaender, Marcel},
  file = {/home/fh/lib/articles/Egger2014.pdf},
  eprinttype = {pmid},
  eprint = {25426033},
  pmcid = {PMC4226167}
}

@article{Hodgkin1952,
  langid = {english},
  title = {A Quantitative Description of Membrane Current and Its Application to Conduction and Excitation in Nerve},
  volume = {117},
  issn = {1469-7793},
  url = {http://onlinelibrary.wiley.com/doi/10.1113/jphysiol.1952.sp004764/abstract},
  doi = {10.1113/jphysiol.1952.sp004764},
  number = {4},
  journaltitle = {The Journal of Physiology},
  urldate = {2016-01-04},
  date = {1952-08-28},
  pages = {500--544},
  author = {Hodgkin, A. L. and Huxley, A. F.},
  file = {/home/fh/lib/articles/Hodgkin1952.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/4MZ6B758/abstract.html}
}

@unpublished{Hoffmann2009,
  title = {Wahrscheinlichkeitstheorie},
  date = {2009},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2009_Wahrscheinlichkeitstheorie.pdf}
}

@article{Hagihara2015,
  title = {Neuronal Activity Is Not Required for the Initial Formation and Maturation of Visual Selectivity},
  volume = {18},
  issn = {1097-6256, 1546-1726},
  url = {http://www.nature.com/doifinder/10.1038/nn.4155},
  doi = {10.1038/nn.4155},
  number = {12},
  journaltitle = {Nature Neuroscience},
  urldate = {2016-02-05},
  date = {2015-11-02},
  pages = {1780--1788},
  author = {Hagihara, Kenta M and Murakami, Tomonari and Yoshida, Takashi and Tagawa, Yoshiaki and Ohki, Kenichi},
  file = {/home/fh/lib/articles/Hagihara20152.pdf}
}

@unpublished{Hoffmann2015f,
  title = {Markovketten},
  date = {2015},
  author = {Hoffmann},
  file = {/home/fh/lib/manuscripts/Hoffmann2015_Markovketten.pdf}
}

@article{Muller2015,
  title = {Python in {{Neuroscience}}},
  volume = {9},
  issn = {1662-5196},
  url = {http://www.frontiersin.org/Journal/FullText.aspx?s=752&name=neuroinformatics&ART_DOI=10.3389/fninf.2015.00011},
  doi = {10.3389/fninf.2015.00011},
  journaltitle = {Frontiers in Neuroinformatics},
  shortjournal = {Frontiers in Neuroinformatics},
  date = {2015},
  author = {Muller, Eilif and Bednar, James A and Diesmann, Markus and Gewaltig, Marc-Oliver and Hines, Michael and Davison, Andrew P},
  file = {/home/fh/lib/articles/Muller2015.EPUB;/home/fh/lib/articles/Muller2015.PDF}
}

@report{zotero-null-343,
  title = {Standalone {{Package}} 1.1b},
  file = {/home/fh/lib/manuals/latex/standalone_package_1.1b.pdf},
  note = {manuals/latex}
}

@article{Jahn2012,
  langid = {english},
  title = {Molecular Machines Governing Exocytosis of Synaptic Vesicles},
  volume = {490},
  issn = {0028-0836},
  url = {http://www.nature.com/nature/journal/v490/n7419/full/nature11320.html},
  doi = {10.1038/nature11320},
  abstract = {Calcium-dependent exocytosis of synaptic vesicles mediates the release of neurotransmitters. Important proteins in this process have been identified such as the SNAREs, synaptotagmins, complexins, Munc18 and Munc13. Structural and functional studies have yielded a wealth of information about the physiological role of these proteins. However, it has been surprisingly difficult to arrive at a unified picture of the molecular sequence of events from vesicle docking to calcium-triggered membrane fusion. Using mainly a biochemical and biophysical perspective, we briefly survey the molecular mechanisms in an attempt to functionally integrate the key proteins into the emerging picture of the neuronal fusion machine.},
  number = {7419},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2015-11-17},
  date = {2012-10-11},
  pages = {201--207},
  keywords = {Neuroscience,Biochemistry,Cell biology},
  author = {Jahn, Reinhard and Fasshauer, Dirk},
  file = {/home/fh/lib/articles/Jahn2012.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/4VMKXNSM/nature11320.html}
}

@book{Werner2011,
  langid = {german},
  location = {{Berlin}},
  title = {Funktionalanalysis},
  edition = {7., korrigierte und erw. Aufl},
  isbn = {978-3-642-21016-7 978-3-642-21017-4},
  pagetotal = {551},
  series = {Springer-Lehrbuch},
  publisher = {{Springer}},
  date = {2011},
  keywords = {Funktionalanalysis,Lehrbuch,Global analysis,Mathematics},
  author = {Werner, Dirk},
  file = {/home/fh/lib/books/Werner2011_Funktionalanalysis.pdf}
}

@unpublished{Kuwert2014,
  title = {Funktionalanalysis 2014},
  date = {2014},
  author = {Kuwert, Ernst},
  file = {/home/fh/lib/manuscripts/Kuwert2014_Funktionalanalysis-2014.pdf}
}

@book{Alt2012,
  langid = {german},
  location = {{Berlin}},
  title = {Lineare Funktionalanalysis: eine anwendungsorientierte Einf{\"u}hrung},
  edition = {6., {\"u}berarb. Aufl},
  isbn = {978-3-642-22260-3 978-3-642-22261-0},
  shorttitle = {Lineare Funktionalanalysis},
  pagetotal = {449},
  publisher = {{Springer}},
  date = {2012},
  keywords = {Lehrbuch,Lineare Funktionalanalysis,Funktionenraum,Lebesgue-Integral,Lineares Funktional,Kompakter Operator,Dualraum,Spektraltheorie,Functional analysis},
  author = {Alt, Hans Wilhelm},
  file = {/home/fh/lib/articles/Book/Alt20122.pdf;/home/fh/lib/books/Alt2012_Lineare-Funktionalanalysis-eine-anwendungsorientierte-Einführung.pdf}
}

@book{Geiger2002,
  langid = {german},
  location = {{Berlin}},
  title = {Theorie und Numerik restringierter Optimierungsaufgaben: [mit 140 {\"U}bungsaufgaben]},
  isbn = {978-3-540-42790-2},
  shorttitle = {Theorie und Numerik restringierter Optimierungsaufgaben},
  pagetotal = {487},
  publisher = {{Springer}},
  date = {2002},
  keywords = {Optimierung,Nebenbedingung,29,Mathematical optimization},
  author = {Geiger, Carl and Kanzow, Christian},
  file = {/home/fh/lib/books/Geiger2002_Theorie-und-Numerik-restringierter-Optimierungsaufgaben-[mit-140-Übungsaufgaben].pdf}
}

@book{Meister2015,
  langid = {german},
  location = {{Wiesbaden}},
  title = {Numerik linearer Gleichungssysteme: eine Einf{\"u}hrung in moderne Verfahren ; mit MATLAB-Implementierungen von C. V{\"o}mel},
  edition = {5., {\"u}berarb. Aufl},
  isbn = {978-3-658-07200-1 978-3-658-07199-8},
  shorttitle = {Numerik linearer Gleichungssysteme},
  pagetotal = {276},
  series = {Lehrbuch},
  publisher = {{Springer Spektrum}},
  date = {2015},
  keywords = {Lineares Gleichungssystem,Numerisches Verfahren},
  author = {Meister, Andreas},
  file = {/home/fh/lib/articles/Book/Meister20152.pdf;/home/fh/lib/books/Meister2015_Numerik-linearer-Gleichungssysteme-eine-Einführung-in-moderne-Verfahren-\;-mit-MATLAB-Implementierungen-von-C.-Vömel.pdf}
}

@unpublished{Goette2011,
  title = {Topologie \& {{Algebraische Topologie}}},
  date = {2011},
  author = {Goette, Sebastian},
  file = {/home/fh/lib/manuscripts/Goette2011_Topologie-&-Algebraische-Topologie.pdf}
}

@book{Schuppar2015,
  langid = {german},
  location = {{Berlin}},
  title = {Elementare Numerik f{\"u}r die Sekundarstufe},
  isbn = {978-3-662-43479-6 978-3-662-43478-9},
  pagetotal = {391},
  series = {Mathematik Primar- und Sekundarstufe I + II},
  publisher = {{Springer Spektrum}},
  date = {2015},
  keywords = {Numerische Mathematik,Einführung},
  author = {Schuppar, Berthold and Humenberger, Hans},
  file = {/home/fh/lib/books/Schuppar2015_Elementare-Numerik-für-die-Sekundarstufe.pdf}
}

@book{Hanke-Bourgeois2009,
  langid = {german},
  location = {{Wiesbaden}},
  title = {Grundlagen der numerischen Mathematik und des wissenschaftlichen Rechnens},
  edition = {3., aktualisierte Aufl},
  isbn = {978-3-8348-0708-3},
  pagetotal = {840},
  series = {Studium},
  publisher = {{Vieweg + Teubner}},
  date = {2009},
  keywords = {Lehrbuch,Numerisches Verfahren,Numerische Mathematik,Mathematisches Modell,Differentialgleichung,Numerical analysis},
  author = {Hanke-Bourgeois, Martin},
  file = {/home/fh/lib/articles/Book/Hanke-Bourgeois2009.pdf;/home/fh/lib/articles/Book/Hanke-Bourgeois20092.pdf;/home/fh/lib/articles/Book/Hanke-Bourgeois20093.pdf;/home/fh/lib/books/Hanke-Bourgeois2009_Grundlagen-der-numerischen-Mathematik-und-des-wissenschaftlichen-Rechnens.pdf}
}

@book{Rannacher2006,
  title = {Einf{\"u}hrung in Die {{Numerische Mathematik}}},
  date = {2006},
  author = {Rannacher, Rolf},
  file = {/home/fh/lib/books/Rannacher2006_Einführung-in-die-Numerische-Mathematik.pdf}
}

@book{Plato2010,
  langid = {german},
  location = {{Wiesbaden}},
  title = {Numerische Mathematik kompakt: Grundlagenwissen f{\"u}r Studium und Praxis ; [mit Online-Service]},
  edition = {4. aktualisierte Aufl},
  isbn = {978-3-8348-1018-2},
  shorttitle = {Numerische Mathematik kompakt},
  pagetotal = {426},
  series = {Numerische Mathematik},
  publisher = {{Vieweg + Teubner}},
  date = {2010},
  keywords = {Lehrbuch,Numerische Mathematik,Numerical analysis},
  author = {Plato, Robert},
  file = {/home/fh/lib/articles/Book/Plato20102.pdf;/home/fh/lib/books/Plato2010_Numerische-Mathematik-kompakt-Grundlagenwissen-für-Studium-und-Praxis-\;-[mit-Online-Service].pdf}
}

@article{Sadeh2015,
  title = {Emergence of {{Functional Specificity}} in {{Balanced Networks}} with {{Synaptic Plasticity}}},
  volume = {11},
  url = {http://dx.doi.org/10.1371/journal.pcbi.1004307},
  doi = {10.1371/journal.pcbi.1004307},
  abstract = {Author Summary In primary visual cortex of mammals, neurons are selective to the orientation of contrast edges. In some species, as cats and monkeys, neurons preferring similar orientations are adjacent on the cortical surface, leading to smooth orientation maps. In rodents, in contrast, such spatial orientation maps do not exist, and neurons of different specificities are mixed in a salt-and-pepper fashion. During development, however, a ``functional'' map of orientation selectivity emerges, where connections between neurons of similar preferred orientations are selectively enhanced. Here we show how such feature-specific connectivity can arise in realistic neocortical networks of excitatory and inhibitory neurons. Our results demonstrate how recurrent dynamics can work in cooperation with synaptic plasticity to form networks where neurons preferring similar stimulus features connect more strongly together. Such networks, in turn, are known to enhance the specificity of neuronal responses to a stimulus. Our study thus reveals how self-organizing connectivity in neuronal networks enable them to achieve new or enhanced functions, and it underlines the essential role of recurrent inhibition and plasticity in this process.},
  number = {6},
  journaltitle = {PLoS Comput Biol},
  shortjournal = {PLoS Comput Biol},
  urldate = {2016-01-08},
  date = {2015-06-19},
  pages = {e1004307},
  author = {Sadeh, Sadra and Clopath, Claudia and Rotter, Stefan},
  file = {/home/fh/lib/articles/Sadeh2015.pdf}
}

@article{Borst2014,
  title = {Fly Visual Course Control: Behaviour, Algorithms and Circuits},
  volume = {15},
  issn = {1471-003X, 1471-0048},
  url = {http://www.nature.com/doifinder/10.1038/nrn3799},
  doi = {10.1038/nrn3799},
  shorttitle = {Fly Visual Course Control},
  number = {9},
  journaltitle = {Nature Reviews Neuroscience},
  urldate = {2015-12-04},
  date = {2014-08-13},
  pages = {590--599},
  author = {Borst, Alexander},
  file = {/home/fh/lib/articles/Borst2014.pdf}
}

@unpublished{Lebiedz2011,
  title = {Optimierung},
  date = {2011},
  author = {Lebiedz},
  file = {/home/fh/lib/manuscripts/Lebiedz2011_Optimierung.pdf}
}

@unpublished{Hoffmann2015,
  title = {Optimierung},
  date = {2015},
  author = {Hoffmann},
  file = {/home/fh/lib/manuscripts/Hoffmann2015_Optimierung.pdf}
}

@article{Vreeswijk1996,
  langid = {english},
  title = {Chaos in {{Neuronal Networks}} with {{Balanced Excitatory}} and {{Inhibitory Activity}}},
  volume = {274},
  issn = {0036-8075, 1095-9203},
  url = {http://www.sciencemag.org/content/274/5293/1724},
  doi = {10.1126/science.274.5293.1724},
  abstract = {Neurons in the cortex of behaving animals show temporally irregular spiking patterns. The origin of this irregularity and its implications for neural processing are unknown. The hypothesis that the temporal variability in the firing of a neuron results from an approximate balance between its excitatory and inhibitory inputs was investigated theoretically. Such a balance emerges naturally in large networks of excitatory and inhibitory neuronal populations that are sparsely connected by relatively strong synapses. The resulting state is characterized by strongly chaotic dynamics, even when the external inputs to the network are constant in time. Such a network exhibits a linear response, despite the highly nonlinear dynamics of single neurons, and reacts to changing external stimuli on time scales much smaller than the integration time constant of a single neuron.},
  number = {5293},
  journaltitle = {Science},
  shortjournal = {Science},
  urldate = {2015-11-24},
  date = {1996-06-12},
  pages = {1724--1726},
  author = {van Vreeswijk, C. and Sompolinsky, H.},
  file = {/home/fh/lib/articles/Vreeswijk1996.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/D7S6AC8N/1724.html},
  eprinttype = {pmid},
  eprint = {8939866}
}

@unpublished{Hoffmann2015i,
  title = {Topologie},
  date = {2015},
  author = {Hoffmann},
  file = {/home/fh/lib/manuscripts/Hoffmann2015_Topologie.pdf}
}

@report{zotero-null-387,
  title = {{{TikZ}} and {{PGF}} 1.18},
  file = {/home/fh/lib/manuals/latex/tikz_and_pgf_1.18.pdf},
  note = {manuals/latex}
}

@incollection{Braitenberg1992,
  langid = {english},
  title = {Manifesto of {{Brain Science}}},
  isbn = {978-3-642-49969-2 978-3-642-49967-8},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-49967-8_29},
  booktitle = {Information {{Processing}} in the {{Cortex}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-10-18},
  date = {1992},
  pages = {473--477},
  keywords = {Neurosciences,Artificial Intelligence (incl. Robotics),Simulation and Modeling,Mathematical and Computational Biology,Statistics for Life Sciences; Medicine; Health Sciences,Biophysics and Biological Physics},
  author = {Braitenberg, Valentino},
  editor = {Aertsen, Dr Ad and Braitenberg, Professor Dr Valentino},
  file = {/home/fh/lib/book_sections/Braitenberg1992.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/8PNUXR8A/10.html},
  doi = {10.1007/978-3-642-49967-8_29}
}

@unpublished{Goette2009,
  title = {Analysis {{I}}-{{III}}},
  date = {2009},
  author = {Goette, Sebastian},
  file = {/home/fh/lib/manuscripts/Goette2009_Analysis-I-III.pdf}
}

@unpublished{Hoffmann2015e,
  title = {Operatornorm},
  date = {2015},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2015_Operatornorm.pdf}
}

@report{zotero-null-399,
  title = {Adjustbox {{Package}}},
  file = {/home/fh/lib/manuals/latex/adjustbox_package.pdf},
  note = {manuals/latex}
}

@report{zotero-null-400,
  title = {{{TikZ}} - {{Commutative Diagrams}}},
  file = {/home/fh/lib/manuals/latex/tikz_-_commutative_diagrams.pdf},
  note = {manuals/latex}
}

@report{zotero-null-402,
  title = {Algorithms {{Bundle}}},
  file = {/home/fh/lib/manuals/latex/algorithms_bundle.pdf},
  note = {manuals/latex}
}

@unpublished{Hoffmann2015b,
  title = {Funktional {{Analysis}} ({{Kuwert}})},
  date = {2015},
  author = {Hoffmann},
  file = {/home/fh/lib/manuscripts/Hoffmann2015_Funktional-Analysis-(Kuwert).pdf}
}

@report{zotero-null-406,
  title = {Minted {{Source Code Highlighting}}},
  file = {/home/fh/lib/manuals/latex/minted_source_code_highlighting.pdf},
  note = {manuals/latex}
}

@report{zotero-null-409,
  title = {Overpic},
  file = {/home/fh/lib/manuals/latex/overpic.pdf},
  note = {manuals/latex}
}

@report{zotero-null-410,
  title = {{{TikZ}}-{{Graph}} ({{French}})},
  file = {/home/fh/lib/manuals/latex/tikz-graph_(french).pdf},
  note = {manuals/latex}
}

@report{zotero-null-412,
  title = {Beamer {{Class}} 3.33},
  file = {/home/fh/lib/manuals/latex/beamer_class_3.33.pdf},
  note = {manuals/latex}
}

@article{Vlachos2012,
  langid = {english},
  title = {Repetitive {{Magnetic Stimulation Induces Functional}} and {{Structural Plasticity}} of {{Excitatory Postsynapses}} in {{Mouse Organotypic Hippocampal Slice Cultures}}},
  volume = {32},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.0409-12.2012},
  doi = {10.1523/JNEUROSCI.0409-12.2012},
  number = {48},
  journaltitle = {Journal of Neuroscience},
  urldate = {2015-11-04},
  date = {2012-11-28},
  pages = {17514--17523},
  author = {Vlachos, A. and Muller-Dahlhaus, F. and Rosskopp, J. and Lenz, M. and Ziemann, U. and Deller, T.},
  file = {/home/fh/lib/articles/Vlachos2012.pdf}
}

@unpublished{Hoffmann2010,
  title = {Adjunct {{Functors}}},
  date = {2010},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2010_Adjunct-Functors.pdf}
}

@report{zotero-null-420,
  title = {{{TikZ}} - {{Minimal Introduction}}},
  file = {/home/fh/lib/manuals/latex/tikz_-_minimal_introduction.pdf},
  note = {manuals/latex}
}

@article{Mertz2007,
  title = {Graphics with {{TikZ}}.},
  volume = {1},
  url = {http://works.bepress.com/andrew_mertz/3/},
  journaltitle = {The PracTEX Journal},
  urldate = {2015-10-25},
  date = {2007},
  author = {Mertz, Andrew and Slough, William},
  file = {/home/fh/lib/articles/Mertz2007.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/VM4AAGRS/3.html}
}

@unpublished{Hoffmann2015d,
  title = {Topologie \& {{Algebraische Topologie}}},
  date = {2015},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2015_Topologie-&-Algebraische-Topologie.pdf}
}

@report{zotero-null-429,
  title = {{{TikZ}} - {{A}} Brief {{Introduction}}},
  file = {/home/fh/lib/manuals/latex/tikz_-_a_brief_introduction.pdf},
  note = {manuals/latex}
}

@unpublished{Hoffmann2012,
  title = {Appendix {{A}} - {{Topologie}}},
  date = {2012},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2012_Appendix-A---Topologie.pdf}
}

@article{Hagihara2015a,
  title = {Neuronal Activity Is Not Required for the Initial Formation and Maturation of Visual Selectivity},
  volume = {18},
  issn = {1097-6256, 1546-1726},
  url = {http://www.nature.com/doifinder/10.1038/nn.4155},
  doi = {10.1038/nn.4155},
  number = {12},
  journaltitle = {Nature Neuroscience},
  urldate = {2015-11-29},
  date = {2015-11-02},
  pages = {1780--1788},
  author = {Hagihara, Kenta M and Murakami, Tomonari and Yoshida, Takashi and Tagawa, Yoshiaki and Ohki, Kenichi},
  file = {/home/fh/lib/articles/Hagihara2015.pdf}
}

@book{Bollobas2001,
  langid = {english},
  location = {{Cambridge ; New York}},
  title = {Random {{Graphs}}},
  edition = {2nd edition},
  isbn = {978-0-521-80920-7},
  pagetotal = {520},
  publisher = {{Cambridge University Press}},
  date = {2001-10-08},
  author = {Bollob{\'a}s, B{\'e}la},
  file = {/home/fh/lib/books/Bollobás2001_Random-Graphs.pdf}
}

@article{Egger2015a,
  langid = {english},
  title = {Robustness of Sensory-Evoked Excitation Is Increased by Inhibitory Inputs to Distal Apical Tuft Dendrites},
  volume = {112},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1518773112},
  doi = {10.1073/pnas.1518773112},
  number = {45},
  journaltitle = {Proceedings of the National Academy of Sciences},
  urldate = {2015-11-30},
  date = {2015-11-10},
  pages = {14072--14077},
  keywords = {_tablet},
  author = {Egger, Robert and Schmitt, Arno C. and Wallace, Damian J. and Sakmann, Bert and Oberlaender, Marcel and Kerr, Jason N. D.},
  file = {/home/fh/lib/articles/Egger2015.pdf}
}

@book{Diestel2000,
  langid = {english},
  location = {{New York}},
  title = {Graph {{Theory}}},
  edition = {2nd edition},
  isbn = {978-0-387-95014-3},
  pagetotal = {322},
  publisher = {{Springer}},
  date = {2000-02-18},
  author = {Diestel, Reinhard},
  file = {/home/fh/lib/books/Diestel2000_Graph-Theory.pdf}
}

@article{Mainen1995,
  langid = {english},
  title = {Reliability of Spike Timing in Neocortical Neurons},
  volume = {268},
  issn = {0036-8075},
  abstract = {It is not known whether the variability of neural activity in the cerebral cortex carries information or reflects noisy underlying mechanisms. In an examination of the reliability of spike generation using recordings from neurons in rat neocortical slices, the precision of spike timing was found to depend on stimulus transients. Constant stimuli led to imprecise spike trains, whereas stimuli with fluctuations resembling synaptic activity produced spike trains with timing reproducible to less than 1 millisecond. These data suggest a low intrinsic noise level in spike generation, which could allow cortical neurons to accurately transform synaptic input into spike sequences, supporting a possible role for spike timing in the processing of cortical information by the neocortex.},
  number = {5216},
  journaltitle = {Science (New York, N.Y.)},
  shortjournal = {Science},
  date = {1995-06-09},
  pages = {1503--1506},
  keywords = {Animals,Electric Stimulation,Evoked Potentials,In Vitro Techniques,Neurons,Occipital Lobe,Rats,Rats; Sprague-Dawley,Synaptic Transmission},
  author = {Mainen, Z. F. and Sejnowski, T. J.},
  file = {/home/fh/lib/articles/Mainen1995.pdf},
  eprinttype = {pmid},
  eprint = {7770778}
}

@book{West2000,
  langid = {english},
  location = {{Upper Saddle River, N.J}},
  title = {Introduction to {{Graph Theory}}},
  edition = {2 edition},
  isbn = {978-0-13-014400-3},
  pagetotal = {470},
  publisher = {{Pearson}},
  date = {2000-09-01},
  author = {West, Douglas},
  file = {/home/fh/lib/books/West2000_Introduction-to-Graph-Theory.pdf}
}

@article{Baudot2013,
  title = {Animation of Natural Scene by Virtual Eye-Movements Evokes High Precision and Low Noise in {{V1}} Neurons},
  volume = {7},
  url = {http://journal.frontiersin.org/article/10.3389/fncir.2013.00206/abstract},
  doi = {10.3389/fncir.2013.00206},
  abstract = {Synaptic noise is thought to be a limiting factor for computational efficiency in the brain. In visual cortex (V1), ongoing activity is present in vivo, and spiking responses to simple stimuli are highly unreliable across trials. Stimulus statistics used to plot receptive fields, however, are quite different from those experienced during natural visuomotor exploration. We recorded V1 neurons intracellularly in the anaesthetized and paralyzed cat and compared their spiking and synaptic responses to full field natural images animated by simulated eye-movements to those evoked by simpler (grating) or higher dimensionality statistics (dense noise). In most cells, natural scene animation was the only condition where high temporal precision (in the 10\textendash{}20 ms range) was maintained during sparse and reliable activity. At the subthreshold level, irregular but highly reproducible membrane potential dynamics were observed, even during long (several 100 ms) ``spike-less'' periods. We showed that both the spatial structure of natural scenes and the temporal dynamics of eye-movements increase the signal-to-noise ratio by a non-linear amplification of the signal combined with a reduction of the subthreshold contextual noise. These data support the view that the sparsening and the time precision of the neural code in V1 may depend primarily on three factors: (1) broadband input spectrum: the bandwidth must be rich enough for recruiting optimally the diversity of spatial and time constants during recurrent processing; (2) tight temporal interplay of excitation and inhibition: conductance measurements demonstrate that natural scene statistics narrow selectively the duration of the spiking opportunity window during which the balance between excitation and inhibition changes transiently and reversibly; (3) signal energy in the lower frequency band: a minimal level of power is needed below 10 Hz to reach consistently the spiking threshold, a situation rarely reached with visual dense noise.},
  journaltitle = {Frontiers in Neural Circuits},
  shortjournal = {Front. Neural Circuits},
  urldate = {2015-10-25},
  date = {2013},
  pages = {206},
  keywords = {visual cortex,natural visual statistics,sensory coding,intracellular membrane potential dynamics,eye movements,reliability},
  author = {Baudot, Pierre and Levy, Manuel and Marre, Olivier and Monier, Cyril and Pananceau, Marc and Fr{\'e}gnac, Yves},
  file = {/home/fh/lib/articles/Baudot2013.pdf}
}

@book{Snustad2012,
  location = {{Hoboken, NJ}},
  title = {Principles of Genetics},
  edition = {6th ed},
  isbn = {978-0-470-90359-9},
  pagetotal = {766},
  publisher = {{Wiley}},
  date = {2012},
  keywords = {Genetics},
  author = {Snustad, D. Peter and Simmons, Michael J.},
  file = {/home/fh/lib/books/Snustad2012_Principles-of-genetics.pdf}
}

@unpublished{Waldmann2011,
  title = {Operator-{{Algebraic Methods}} in {{Quantum Mechanics}}},
  date = {2011},
  author = {Waldmann, Stefan},
  file = {/home/fh/lib/manuscripts/Waldmann2011_Operator-Algebraic-Methods-in-Quantum-Mechanics_2.pdf}
}

@article{Hires2015,
  langid = {english},
  title = {Low-Noise Encoding of Active Touch by Layer 4 in the Somatosensory Cortex},
  volume = {4},
  issn = {2050-084X},
  url = {http://elifesciences.org/lookup/doi/10.7554/eLife.06619},
  doi = {10.7554/eLife.06619},
  journaltitle = {eLife},
  urldate = {2015-10-25},
  date = {2015-08-05},
  author = {Hires, Andrew Samuel and Gutnisky, Diego A and Yu, Jianing and O'Connor, Daniel H and Svoboda, Karel},
  file = {/home/fh/lib/articles/Hires2015.pdf}
}

@book{Mesbahi2010,
  location = {{Princeton}},
  title = {Graph Theoretic Methods in Multiagent Networks},
  isbn = {978-0-691-14061-2},
  pagetotal = {403},
  series = {Princeton series in applied mathematics},
  publisher = {{Princeton University Press}},
  date = {2010},
  keywords = {Network analysis (Planning),Graphic methods,Multiagent systems,Mathematical models},
  author = {Mesbahi, Mehran and Egerstedt, Magnus},
  file = {/home/fh/lib/books/Mesbahi2010_Graph-theoretic-methods-in-multiagent-networks.pdf}
}

@book{Janson2000,
  location = {{New York}},
  title = {Random Graphs},
  isbn = {978-0-471-17541-4},
  pagetotal = {333},
  series = {Wiley-Interscience series in discrete mathematics and optimization},
  publisher = {{John Wiley}},
  date = {2000},
  keywords = {Random graphs},
  author = {Janson, Svante and {\L}uczak, Tomasz and Ruci{\'n}ski, Andrzej},
  file = {/home/fh/lib/books/Janson2000_Random-graphs.djvu}
}

@book{Solomon1978,
  langid = {english},
  location = {{Philadelphia, Pa}},
  title = {Geometric Probability},
  isbn = {978-0-89871-025-0},
  pagetotal = {174},
  number = {28},
  series = {CBMS-NSF regional conference series in applied mathematics},
  publisher = {{Society for Industrial and Applied Mathematics}},
  date = {1978},
  author = {Solomon, Herbert},
  editora = {{Society for Industrial and Applied Mathematics}},
  editoratype = {collaborator},
  file = {/home/fh/lib/books/Solomon1978_Geometric-probability.pdf}
}

@unpublished{Hoffmann2015c,
  title = {Optimierung - {{Zusammenfassung}}},
  date = {2015},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2015_Optimierung---Zusammenfassung.pdf}
}

@book{Fisher1987,
  location = {{Cambridge [Cambridgeshire] ; New York}},
  title = {Statistical Analysis of Spherical Data},
  isbn = {978-0-521-24273-8},
  pagetotal = {329},
  publisher = {{Cambridge University Press}},
  date = {1987},
  keywords = {Mathematical statistics,Spherical data},
  author = {Fisher, N. I. and Lewis, Toby and Embleton, B. J. J.},
  file = {/home/fh/lib/books/Fisher1987_Statistical-analysis-of-spherical-data.pdf}
}

@article{Muller-Dahlhaus2013,
  title = {Unraveling the Cellular and Molecular Mechanisms of Repetitive Magnetic Stimulation},
  volume = {6},
  issn = {1662-5099},
  url = {http://journal.frontiersin.org/article/10.3389/fnmol.2013.00050/abstract},
  doi = {10.3389/fnmol.2013.00050},
  journaltitle = {Frontiers in Molecular Neuroscience},
  urldate = {2015-11-04},
  date = {2013},
  author = {M{\"u}ller-Dahlhaus, Florian and Vlachos, Andreas},
  file = {/home/fh/lib/articles/Müller-Dahlhaus2013.pdf}
}

@book{Mardia2000,
  location = {{Chichester ; New York}},
  title = {Directional Statistics},
  isbn = {978-0-471-95333-3},
  pagetotal = {429},
  series = {Wiley series in probability and statistics},
  publisher = {{J. Wiley}},
  date = {2000},
  keywords = {Mathematical statistics,Distribution (Probability theory),Sampling (Statistics)},
  author = {Mardia, K. V. and Jupp, Peter E.},
  file = {/home/fh/lib/books/Mardia2000_Directional-statistics2.pdf}
}

@article{Fishell2013,
  langid = {english},
  title = {The {{Neuron Identity Problem}}: {{Form Meets Function}}},
  volume = {80},
  issn = {08966273},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627313009872},
  doi = {10.1016/j.neuron.2013.10.035},
  shorttitle = {The {{Neuron Identity Problem}}},
  number = {3},
  journaltitle = {Neuron},
  urldate = {2015-10-25},
  date = {2013-10},
  pages = {602--612},
  author = {Fishell, Gord and Heintz, Nathaniel},
  file = {/home/fh/lib/articles/Fishell2013.pdf}
}

@unpublished{Hoffmann2015g,
  title = {Stochastische {{Prozesse}} - {{Zusammenfassung}}},
  date = {2015},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2015_Stochastische-Prozesse---Zusammenfassung.pdf}
}

@article{Deisseroth2013,
  langid = {english},
  title = {Engineering {{Approaches}} to {{Illuminating Brain Structure}} and {{Dynamics}}},
  volume = {80},
  issn = {08966273},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627313009367},
  doi = {10.1016/j.neuron.2013.10.032},
  number = {3},
  journaltitle = {Neuron},
  urldate = {2015-10-25},
  date = {2013-10},
  pages = {568--577},
  author = {Deisseroth, Karl and Schnitzer, Mark J.},
  file = {/home/fh/lib/articles/Deisseroth2013.pdf}
}

@article{Shadlen2013,
  langid = {english},
  title = {Decision {{Making}} as a {{Window}} on {{Cognition}}},
  volume = {80},
  issn = {08966273},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627313009999},
  doi = {10.1016/j.neuron.2013.10.047},
  number = {3},
  journaltitle = {Neuron},
  urldate = {2015-10-25},
  date = {2013-10},
  pages = {791--806},
  author = {Shadlen, Michael N. and Kiani, Roozbeh},
  file = {/home/fh/lib/articles/Shadlen2013.pdf}
}

@collection{VanOoyen2003,
  location = {{Cambridge, Mass}},
  title = {Modeling Neural Development},
  isbn = {978-0-262-22066-8},
  pagetotal = {322},
  series = {Developmental cognitive neuroscience},
  publisher = {{MIT Press}},
  date = {2003},
  keywords = {Developmental neurobiology,Neural networks (Neurobiology),Methodology},
  editor = {Van Ooyen, Arjen},
  file = {/home/fh/lib/books/Van Ooyen2003_Modeling-neural-development.pdf}
}

@article{Cuntz2012,
  langid = {english},
  title = {A Scaling Law Derived from Optimal Dendritic Wiring},
  volume = {109},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/109/27/11014},
  doi = {10.1073/pnas.1200430109},
  abstract = {The wide diversity of dendritic trees is one of the most striking features of neural circuits. Here we develop a general quantitative theory relating the total length of dendritic wiring to the number of branch points and synapses. We show that optimal wiring predicts a 2/3 power law between these measures. We demonstrate that the theory is consistent with data from a wide variety of neurons across many different species and helps define the computational compartments in dendritic trees. Our results imply fundamentally distinct design principles for dendritic arbors compared with vascular, bronchial, and botanical trees.},
  number = {27},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  urldate = {2015-11-29},
  date = {2012-03-07},
  pages = {11014--11018},
  keywords = {morphology,_tablet,computational neuroscience,branching,dendrite,minimum spanning tree},
  author = {Cuntz, Hermann and Mathy, Alexandre and H{\"a}usser, Michael},
  file = {/home/fh/lib/articles/Cuntz2012.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/GI5A83GX/11014.html},
  eprinttype = {pmid},
  eprint = {22715290}
}

@article{Nudo2013,
  title = {Recovery after Brain Injury: Mechanisms and Principles},
  volume = {7},
  issn = {1662-5161},
  url = {http://journal.frontiersin.org/article/10.3389/fnhum.2013.00887/abstract},
  doi = {10.3389/fnhum.2013.00887},
  shorttitle = {Recovery after Brain Injury},
  journaltitle = {Frontiers in Human Neuroscience},
  urldate = {2015-10-25},
  date = {2013},
  author = {Nudo, Randolph J.},
  file = {/home/fh/lib/articles/Nudo2013.pdf}
}

@article{Lenz2015,
  langid = {english},
  title = {Repetitive Magnetic Stimulation Induces Plasticity of Excitatory Postsynapses on Proximal Dendrites of Cultured Mouse {{CA1}} Pyramidal Neurons},
  volume = {220},
  issn = {1863-2653, 1863-2661},
  url = {http://link.springer.com/10.1007/s00429-014-0859-9},
  doi = {10.1007/s00429-014-0859-9},
  number = {6},
  journaltitle = {Brain Structure and Function},
  urldate = {2015-11-04},
  date = {2015-11},
  pages = {3323--3337},
  author = {Lenz, Maximilian and Platschek, Steffen and Priesemann, Viola and Becker, Denise and Willems, Laurent M. and Ziemann, Ulf and Deller, Thomas and M{\"u}ller-Dahlhaus, Florian and Jedlicka, Peter and Vlachos, Andreas},
  file = {/home/fh/lib/articles/Lenz2015.pdf}
}

@article{Krieg2014,
  title = {A Unifying Theory of Synaptic Long-Term Plasticity Based on a Sparse Distribution of Synaptic Strength},
  volume = {6},
  url = {http://journal.frontiersin.org/article/10.3389/fnsyn.2014.00003/full},
  doi = {10.3389/fnsyn.2014.00003},
  abstract = {Long-term synaptic plasticity is fundamental to learning and network function. It has been studied under various induction protocols and depends on firing rates, membrane voltage, and precise timing of action potentials. These protocols show different facets of a common underlying mechanism but they are mostly modeled as distinct phenomena. Here, we show that all of these different dependencies can be explained from a single computational principle. The objective is a sparse distribution of excitatory synaptic strength, which may help to reduce metabolic costs associated with synaptic transmission. Based on this objective we derive a stochastic gradient ascent learning rule which is of differential-Hebbian type. It is formulated in biophysical quantities and can be related to current mechanistic theories of synaptic plasticity. The learning rule accounts for experimental findings from all major induction protocols and explains a classic phenomenon of metaplasticity. Furthermore, our model predicts the existence of metaplasticity for spike-timing-dependent plasticity Thus, we provide a theory of long-term synaptic plasticity that unifies different induction protocols and provides a connection between functional and mechanistic levels of description.},
  journaltitle = {Frontiers in Synaptic Neuroscience},
  shortjournal = {Front. Synaptic Neurosci},
  urldate = {2016-01-13},
  date = {2014},
  pages = {3},
  keywords = {STDP,sparseness,metaplasticity,synaptic plasticity,computational},
  author = {Krieg, Daniel and Triesch, Jochen},
  file = {/home/fh/lib/articles/Krieg2014.pdf}
}

@article{Maggio2014,
  langid = {english},
  title = {Synaptic Plasticity at the Interface of Health and Disease: {{New}} Insights on the Role of Endoplasmic Reticulum Intracellular Calcium Stores},
  volume = {281},
  issn = {03064522},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0306452214007921},
  doi = {10.1016/j.neuroscience.2014.09.041},
  shorttitle = {Synaptic Plasticity at the Interface of Health and Disease},
  journaltitle = {Neuroscience},
  urldate = {2015-11-04},
  date = {2014-12},
  pages = {135--146},
  author = {Maggio, N. and Vlachos, A.},
  file = {/home/fh/lib/articles/Maggio2014.pdf}
}

@unpublished{Hoffmann2015j,
  title = {Funktionalanalysis - {{Zusammenfassung}}},
  date = {2015},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2015_Funktionalanalysis---Zusammenfassung.pdf}
}

@article{Steinberg2013,
  langid = {english},
  title = {A Causal Link between Prediction Errors, Dopamine Neurons and Learning},
  volume = {16},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v16/n7/full/nn.3413.html},
  doi = {10.1038/nn.3413},
  abstract = {Situations in which rewards are unexpectedly obtained or withheld represent opportunities for new learning. Often, this learning includes identifying cues that predict reward availability. Unexpected rewards strongly activate midbrain dopamine neurons. This phasic signal is proposed to support learning about antecedent cues by signaling discrepancies between actual and expected outcomes, termed a reward prediction error. However, it is unknown whether dopamine neuron prediction error signaling and cue-reward learning are causally linked. To test this hypothesis, we manipulated dopamine neuron activity in rats in two behavioral procedures, associative blocking and extinction, that illustrate the essential function of prediction errors in learning. We observed that optogenetic activation of dopamine neurons concurrent with reward delivery, mimicking a prediction error, was sufficient to cause long-lasting increases in cue-elicited reward-seeking behavior. Our findings establish a causal role for temporally precise dopamine neuron signaling in cue-reward learning, bridging a critical gap between experimental evidence and influential theoretical frameworks.},
  number = {7},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2015-11-10},
  date = {2013-07},
  pages = {966--973},
  keywords = {Classical conditioning},
  author = {Steinberg, Elizabeth E. and Keiflin, Ronald and Boivin, Josiah R. and Witten, Ilana B. and Deisseroth, Karl and Janak, Patricia H.},
  file = {/home/fh/lib/articles/Steinberg2013.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/JTTIPGEF/nn.3413.html}
}

@unpublished{Hoffmann2015a,
  title = {Markovketten - {{Zusammenfassung}}},
  date = {2015},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2015_Markovketten---Zusammenfassung.pdf}
}

@article{Takemura2013,
  title = {A Visual Motion Detection Circuit Suggested by {{Drosophila}} Connectomics},
  volume = {500},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/nature12450},
  doi = {10.1038/nature12450},
  number = {7461},
  journaltitle = {Nature},
  urldate = {2015-12-01},
  date = {2013-08-07},
  pages = {175--181},
  keywords = {_tablet},
  author = {Takemura, Shin-ya and Bharioke, Arjun and Lu, Zhiyuan and Nern, Aljoscha and Vitaladevuni, Shiv and Rivlin, Patricia K. and Katz, William T. and Olbris, Donald J. and Plaza, Stephen M. and Winston, Philip and Zhao, Ting and Horne, Jane Anne and Fetter, Richard D. and Takemura, Satoko and Blazek, Katerina and Chang, Lei-Ann and Ogundeyi, Omotara and Saunders, Mathew A. and Shapiro, Victor and Sigmund, Christopher and Rubin, Gerald M. and Scheffer, Louis K. and Meinertzhagen, Ian A. and Chklovskii, Dmitri B.},
  file = {/home/fh/lib/articles/Takemura2013.pdf}
}

@article{Waters2006,
  langid = {english},
  title = {Background {{Synaptic Activity Is Sparse}} in {{Neocortex}}},
  volume = {26},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.2152-06.2006},
  doi = {10.1523/JNEUROSCI.2152-06.2006},
  number = {32},
  journaltitle = {Journal of Neuroscience},
  urldate = {2015-12-08},
  date = {2006-08-09},
  pages = {8267--8277},
  author = {Waters, J. and Helmchen, F.},
  file = {/home/fh/lib/articles/Waters2006.pdf}
}

@article{Cohen2012,
  langid = {english},
  title = {Neuron-Type-Specific Signals for Reward and Punishment in the Ventral Tegmental Area},
  volume = {482},
  issn = {0028-0836},
  url = {http://www.nature.com/nature/journal/v482/n7383/full/nature10754.html},
  doi = {10.1038/nature10754},
  abstract = {Dopamine has a central role in motivation and reward. Dopaminergic neurons in the ventral tegmental area (VTA) signal the discrepancy between expected and actual rewards (that is, reward prediction error), but how they compute such signals is unknown. We recorded the activity of VTA neurons while mice associated different odour cues with appetitive and aversive outcomes. We found three types of neuron based on responses to odours and outcomes: approximately half of the neurons (type I, 52\%) showed phasic excitation after reward-predicting odours and rewards in a manner consistent with reward prediction error coding; the other half of neurons showed persistent activity during the delay between odour and outcome that was modulated positively (type II, 31\%) or negatively (type III, 18\%) by the value of outcomes. Whereas the activity of type I neurons was sensitive to actual outcomes (that is, when the reward was delivered as expected compared to when it was unexpectedly omitted), the activity of type II and type III neurons was determined predominantly by reward-predicting odours. We /`tagged/' dopaminergic and GABAergic neurons with the light-sensitive protein channelrhodopsin-2 and identified them based on their responses to optical stimulation while recording. All identified dopaminergic neurons were of type I and all GABAergic neurons were of type II. These results show that VTA GABAergic neurons signal expected reward, a key variable for dopaminergic neurons to calculate reward prediction error.},
  number = {7383},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2015-11-10},
  date = {2012-02-02},
  pages = {85--88},
  keywords = {Neuroscience,Animal behaviour},
  author = {Cohen, Jeremiah Y. and Haesler, Sebastian and Vong, Linh and Lowell, Bradford B. and Uchida, Naoshige},
  file = {/home/fh/lib/articles/Cohen2012.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/IIF3W3DZ/nature10754.html}
}

@book{Drongelen2007,
  location = {{Amsterdam ; Burlington, MA}},
  title = {Signal Processing for Neuroscientists: Introduction to the Analysis of Physiological Signals},
  isbn = {978-0-12-370867-0},
  shorttitle = {Signal Processing for Neuroscientists},
  pagetotal = {308},
  publisher = {{Elsevier/Academic Press}},
  date = {2007},
  keywords = {Neurology,Neurosciences,Mathematical models,Signal processing,Digital techniques,Data processing,Physiology},
  author = {van Drongelen, Wim},
  file = {/home/fh/lib/books/Drongelen2007_Signal-processing-for-neuroscientists-introduction-to-the-analysis-of-physiological-signals.pdf}
}

@book{Oppenheim1997,
  location = {{Upper Saddle River, N.J}},
  title = {Signals \& Systems},
  edition = {2nd ed},
  isbn = {978-0-13-814757-0},
  pagetotal = {957},
  series = {Prentice-Hall signal processing series},
  publisher = {{Prentice Hall}},
  date = {1997},
  keywords = {System analysis,Signal theory (Telecommunication)},
  author = {Oppenheim, Alan V. and Willsky, Alan S. and Nawab, Syed Hamid},
  file = {/home/fh/lib/books/Oppenheim1997_Signals-&-systems.djvu}
}

@book{Sarachik1997,
  location = {{Cambridge}},
  title = {Principles of Linear Systems},
  isbn = {978-0-521-57057-2 978-0-521-57606-2},
  pagetotal = {283},
  publisher = {{Cambridge Univ. Press}},
  date = {1997},
  keywords = {System analysis,State-space methods,Linear systems},
  author = {Sarachik, Philip E.},
  file = {/home/fh/lib/books/Sarachik1997_Principles-of-linear-systems.pdf;/home/fh/lib/books/Sarachik1997_Principles-of-linear-systems2.pdf}
}

@article{Eshel2015,
  langid = {english},
  title = {Arithmetic and Local Circuitry Underlying Dopamine Prediction Errors},
  volume = {525},
  issn = {0028-0836},
  url = {http://www.nature.com/nature/journal/v525/n7568/full/nature14855.html},
  doi = {10.1038/nature14855},
  abstract = {Dopamine neurons are thought to facilitate learning by comparing actual and expected reward. Despite two decades of investigation, little is known about how this comparison is made. To determine how dopamine neurons calculate prediction error, we combined optogenetic manipulations with extracellular recordings in the ventral tegmental area while mice engaged in classical conditioning. Here we demonstrate, by manipulating the temporal expectation of reward, that dopamine neurons perform subtraction, a computation that is ideal for reinforcement learning but rarely observed in the brain. Furthermore, selectively exciting and inhibiting neighbouring GABA ($\gamma$-aminobutyric acid) neurons in the ventral tegmental area reveals that these neurons are a source of subtraction: they inhibit dopamine neurons when reward is expected, causally contributing to prediction-error calculations. Finally, bilaterally stimulating ventral tegmental area GABA neurons dramatically reduces anticipatory licking to conditioned odours, consistent with an important role for these neurons in reinforcement learning. Together, our results uncover the arithmetic and local circuitry underlying dopamine prediction errors.},
  number = {7568},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2015-11-10},
  date = {2015-09-10},
  pages = {243--246},
  keywords = {Reward,Motivation,Learning and memory},
  author = {Eshel, Neir and Bukwich, Michael and Rao, Vinod and Hemmelder, Vivian and Tian, Ju and Uchida, Naoshige},
  file = {/home/fh/lib/articles/Eshel2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/RT5HCSGP/nature14855.html}
}

@article{Okabe2014,
  langid = {english},
  title = {Neurons {{Limit Angiogenesis}} by {{Titrating VEGF}} in {{Retina}}},
  volume = {159},
  issn = {00928674},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S009286741401174X},
  doi = {10.1016/j.cell.2014.09.025},
  number = {3},
  journaltitle = {Cell},
  urldate = {2015-12-08},
  date = {2014-10},
  pages = {584--596},
  author = {Okabe, Keisuke and Kobayashi, Sakiko and Yamada, Toru and Kurihara, Toshihide and Tai-Nagara, Ikue and Miyamoto, Takeshi and Mukouyama, Yoh-suke and Sato, Thomas N. and Suda, Toshio and Ema, Masatsugu and Kubota, Yoshiaki},
  file = {/home/fh/lib/articles/Okabe2014.pdf}
}

@article{Naumann2010,
  title = {Monitoring Neural Activity with Bioluminescence during Natural Behavior},
  volume = {13},
  issn = {1097-6256, 1546-1726},
  url = {http://www.nature.com/doifinder/10.1038/nn.2518},
  doi = {10.1038/nn.2518},
  number = {4},
  journaltitle = {Nature Neuroscience},
  urldate = {2015-12-08},
  date = {2010-04},
  pages = {513--520},
  author = {Naumann, Eva A and Kampff, Adam R and Prober, David A and Schier, Alexander F and Engert, Florian},
  file = {/home/fh/lib/articles/Naumann2010.pdf}
}

@article{Deger2012,
  langid = {english},
  title = {Spike-{{Timing Dependence}} of {{Structural Plasticity Explains Cooperative Synapse Formation}} in the {{Neocortex}}},
  volume = {8},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1002689},
  doi = {10.1371/journal.pcbi.1002689},
  number = {9},
  journaltitle = {PLoS Computational Biology},
  urldate = {2015-12-11},
  date = {2012-09-20},
  pages = {e1002689},
  author = {Deger, Moritz and Helias, Moritz and Rotter, Stefan and Diesmann, Markus},
  editor = {Sporns, Olaf},
  file = {/home/fh/lib/articles/Deger2012.pdf}
}

@article{Chung2015,
  langid = {english},
  title = {The Effect of Sensory Feedback on Crayfish Posture and Locomotion: {{I}}. {{Experimental}} Analysis of Closing the Loop},
  volume = {113},
  issn = {0022-3077, 1522-1598},
  url = {http://jn.physiology.org/lookup/doi/10.1152/jn.00248.2014},
  doi = {10.1152/jn.00248.2014},
  shorttitle = {The Effect of Sensory Feedback on Crayfish Posture and Locomotion},
  number = {6},
  journaltitle = {Journal of Neurophysiology},
  urldate = {2015-12-08},
  date = {2015-03-15},
  pages = {1763--1771},
  author = {Chung, Bryce and Bacqu{\'e}-Cazenave, Julien and Cofer, David W. and Cattaert, Daniel and Edwards, Donald H.},
  file = {/home/fh/lib/articles/Chung2015.pdf}
}

@book{Proakis1996,
  location = {{Upper Saddle River, N.J}},
  title = {Digital Signal Processing: Principles, Algorithms, and Applications},
  edition = {3rd ed},
  isbn = {978-0-13-373762-2},
  shorttitle = {Digital Signal Processing},
  pagetotal = {1},
  publisher = {{Prentice Hall}},
  date = {1996},
  keywords = {Signal processing,Digital techniques},
  author = {Proakis, John G. and Manolakis, Dimitris G.},
  file = {/home/fh/lib/books/Proakis1996_Digital-signal-processing-principles,-algorithms,-and-applications.pdf}
}

@article{Waelti2001,
  langid = {english},
  title = {Dopamine Responses Comply with Basic Assumptions of Formal Learning Theory},
  volume = {412},
  issn = {0028-0836},
  url = {http://www.nature.com/nature/journal/v412/n6842/full/412043a0.html},
  doi = {10.1038/35083500},
  abstract = {According to contemporary learning theories, the discrepancy, or error, between the actual and predicted reward determines whether learning occurs when a stimulus is paired with a reward. The role of prediction errors is directly demonstrated by the observation that learning is blocked when the stimulus is paired with a fully predicted reward. By using this blocking procedure, we show that the responses of dopamine neurons to conditioned stimuli was governed differentially by the occurrence of reward prediction errors rather than stimulus\textendash{}reward associations alone, as was the learning of behavioural reactions. Both behavioural and neuronal learning occurred predominantly when dopamine neurons registered a reward prediction error at the time of the reward. Our data indicate that the use of analytical tests derived from formal behavioural learning theory provides a powerful approach for studying the role of single neurons in learning.},
  number = {6842},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2015-11-10},
  date = {2001-07-05},
  pages = {43--48},
  author = {Waelti, Pascale and Dickinson, Anthony and Schultz, Wolfram},
  file = {/home/fh/lib/articles/Waelti2001.pdf;/home/fh/lib/articles/Waelti20012.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/B82UH3IC/412043a0.html}
}

@book{Lathi1998,
  location = {{Carmichael, Calif}},
  title = {Signal Processing and Linear Systems},
  isbn = {978-0-941413-35-0},
  pagetotal = {850},
  publisher = {{Berkeley Cambridge Press}},
  date = {1998},
  keywords = {Signal processing,System analysis,Linear systems},
  author = {Lathi, B. P.},
  file = {/home/fh/lib/books/Lathi1998_Signal-processing-and-linear-systems.djvu}
}

@book{Haykin2002,
  location = {{New York}},
  title = {Signals and Systems},
  edition = {2nd ed},
  isbn = {978-0-471-16474-6},
  pagetotal = {802},
  publisher = {{Wiley}},
  date = {2002},
  keywords = {Signal processing,System analysis,Linear time invariant systems,Telecommunication systems},
  author = {Haykin, Simon S. and Van Veen, Barry},
  file = {/home/fh/lib/books/Haykin2002_Signals-and-systems.djvu}
}

@book{Kailath1980,
  location = {{Englewood Cliffs, N.J}},
  title = {Linear Systems},
  isbn = {978-0-13-536961-6},
  pagetotal = {682},
  series = {Prentice-Hall information and system science series},
  publisher = {{Prentice-Hall}},
  date = {1980},
  keywords = {System analysis,Linear systems},
  author = {Kailath, Thomas},
  file = {/home/fh/lib/books/Kailath1980_Linear-systems.pdf}
}

@book{Gerstner2014,
  location = {{Cambridge, United Kingdom}},
  title = {Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition},
  isbn = {978-1-107-06083-8 978-1-107-63519-7},
  shorttitle = {Neuronal Dynamics},
  pagetotal = {577},
  publisher = {{Cambridge University Press}},
  date = {2014},
  keywords = {Neurobiology,Neural networks (Neurobiology),Cognitive neuroscience},
  author = {Gerstner, Wulfram and Kistler, Werner M. and Naud, Richard and Paninski, Liam},
  file = {/home/fh/lib/books/Gerstner2014_Neuronal-dynamics-from-single-neurons-to-networks-and-models-of-cognition.pdf}
}

@collection{Rieke1997,
  location = {{Cambridge, Mass}},
  title = {Spikes: Exploring the Neural Code},
  isbn = {978-0-262-18174-7},
  shorttitle = {Spikes},
  pagetotal = {395},
  series = {Computational neuroscience},
  publisher = {{MIT Press}},
  date = {1997},
  keywords = {Neural transmission,Sensory neurons,low_qual},
  editor = {Rieke, Fred},
  file = {/home/fh/lib/books/Rieke1997_Spikes-exploring-the-neural-code.pdf}
}

@book{Laures2009,
  langid = {german},
  location = {{Heidelberg}},
  title = {Grundkurs Topologie},
  isbn = {978-3-8274-2040-4},
  abstract = {Die Topologie besch{\"a}ftigt sich mit den qualitativen Eigenschaften geometrischer Objekte. Ihr Begriffsapparat ist so m{\"a}chtig, dass kaum eine mathematische Struktur nicht mit Gewinn topologisiert wurde. Dieses Buch versteht sich als Br{\"u}cke von den einf{\"u}hrenden Vorlesungen der Analysis und Linearen Algebra zu den fortgeschrittenen Vorlesungen der Algebraischen und Geometrischen Topologie. Es eignet sich besonders f{\"u}r Studierende in einem Bachelor- oder Masterstudiengang der Mathematik, kann aber auch zum Selbststudium f{\"u}r mathematisch interessierte Naturwissenschaftler dienen. ... Definitionen werden stets mit vielen Beispielen unterlegt und neue Konzepte werden mit zahlreichen Bildern illustriert. {\"U}ber 170 {\"U}bungsaufgaben (mit L{\"o}sungen zu ausgew{\"a}hlten Aufgaben auf der Website zum Buch) helfen, die vermittelten Inhalte einzu{\"u}ben und zu vertiefen. (3)},
  pagetotal = {242},
  publisher = {{Spektrum, Akad. Verl}},
  date = {2009},
  keywords = {Lehrbuch,Topologie,Homotopie,Transformationsgruppe,Kompaktheit},
  author = {Laures, Gerd and Szymik, Markus},
  file = {/home/fh/lib/books/Laures2009_Grundkurs-Topologie.pdf;/home/fh/lib/books/Laures2009_Grundkurs-Topologie2.pdf}
}

@article{Wilson1972,
  langid = {english},
  title = {Excitatory and {{Inhibitory Interactions}} in {{Localized Populations}} of {{Model Neurons}}},
  volume = {12},
  issn = {0006-3495},
  url = {http://www.cell.com/article/S0006349572860685/abstract},
  doi = {10.1016/S0006-3495(72)86068-5},
  abstract = {Coupled nonlinear differential equations are derived for the dynamics of spatially localized populations containing both excitatory and inhibitory model neurons. Phase plane methods and numerical solutions are then used to investigate population responses to various types of stimuli. The results obtained show simple and multiple hysteresis phenomena and limit cycle activity. The latter is particularly interesting since the frequency of the limit cycle oscillation is found to be a monotonic function of stimulus intensity. Finally, it is proved that the existence of limit cycle dynamics in response to one class of stimuli implies the existence of multiple stable states and hysteresis in response to a different class of stimuli. The relation between these findings and a number of experiments is discussed.},
  number = {1},
  journaltitle = {Biophysical Journal},
  urldate = {2015-11-11},
  date = {1972-01-01},
  pages = {1--24},
  author = {Wilson, Hugh R. and Cowan, Jack D.},
  file = {/home/fh/lib/articles/Wilson1972.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/24GTEUF5/S0006-3495(72)86068-5.html},
  eprinttype = {pmid},
  eprint = {4332108}
}

@book{Eilenberg1952,
  langid = {english},
  title = {Foundations of Algebraic Topology},
  pagetotal = {352},
  publisher = {{Princeton University Press}},
  date = {1952},
  keywords = {Mathematics / Topology},
  author = {Eilenberg, Samuel and Steenrod, Norman Earl},
  file = {/home/fh/lib/books/Eilenberg1952_Foundations-of-algebraic-topology.pdf}
}

@book{Hatcher2002,
  location = {{Cambridge ; New York}},
  title = {Algebraic Topology},
  isbn = {978-0-521-79160-1 978-0-521-79540-1},
  pagetotal = {544},
  publisher = {{Cambridge University Press}},
  date = {2002},
  keywords = {Algebraic topology},
  author = {Hatcher, Allen},
  file = {/home/fh/lib/books/Hatcher2002_Algebraic-topology.pdf}
}

@book{Switzer2002,
  langid = {english},
  location = {{Berlin}},
  title = {Algebraic Topology - Homotopy and Homology},
  isbn = {978-3-540-42750-6},
  pagetotal = {526},
  series = {Classics in mathematics},
  publisher = {{Springer}},
  date = {2002},
  keywords = {Algebraic topology,Algebraische Topologie,28,Homotopy theory,Homology theory,\#aAlgebraic topology\#zAutomatisch aus GBV_2011-10 2012-06-07,\#aHomology theory\#zAutomatisch aus GBV_2011-10 2012-06-07,\#aHomotopy theory\#zAutomatisch aus GBV_2011-10 2012-06-07},
  author = {Switzer, Robert M.},
  file = {/home/fh/lib/books/Switzer2002_Algebraic-topology---homotopy-and-homology.djvu;/home/fh/lib/books/Switzer2002_Algebraic-topology---homotopy-and-homology.pdf}
}

@article{Rosenbaum2014,
  title = {Balanced {{Networks}} of {{Spiking Neurons}} with {{Spatially Dependent Recurrent Connections}}},
  volume = {4},
  url = {http://link.aps.org/doi/10.1103/PhysRevX.4.021039},
  doi = {10.1103/PhysRevX.4.021039},
  abstract = {Networks of model neurons with balanced recurrent excitation and inhibition capture the irregular and asynchronous spiking activity reported in cortex. While mean-field theories of spatially homogeneous balanced networks are well understood, a mean-field analysis of spatially heterogeneous balanced networks has not been fully developed. We extend the analysis of balanced networks to include a connection probability that depends on the spatial separation between neurons. In the continuum limit, we derive that stable, balanced firing rate solutions require that the spatial spread of external inputs be broader than that of recurrent excitation, which in turn must be broader than or equal to that of recurrent inhibition. Notably, this implies that network models with broad recurrent inhibition are inconsistent with the balanced state. For finite size networks, we investigate the pattern-forming dynamics arising when balanced conditions are not satisfied. Our study highlights the new challenges that balanced networks pose for the spatiotemporal dynamics of complex systems.},
  number = {2},
  journaltitle = {Physical Review X},
  shortjournal = {Phys. Rev. X},
  urldate = {2015-11-11},
  date = {2014-05-28},
  pages = {021039},
  author = {Rosenbaum, Robert and Doiron, Brent},
  file = {/home/fh/lib/articles/Rosenbaum2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/C3SXHQ76/PhysRevX.4.html}
}

@book{Spanier1981,
  langid = {english},
  location = {{New York, NY}},
  title = {Algebraic {{Topology}}},
  isbn = {978-0-387-94426-5 978-1-4684-9322-1},
  url = {http://link.springer.com/10.1007/978-1-4684-9322-1},
  publisher = {{Springer New York}},
  urldate = {2015-11-06},
  date = {1981},
  author = {Spanier, Edwin H.},
  file = {/home/fh/lib/books/Spanier1981_Algebraic-Topology.djvu}
}

@unpublished{Hoffmann2009a,
  title = {Ringe},
  date = {2009},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2009_Ringe.pdf}
}

@unpublished{Hoffmann2009b,
  title = {Algebra},
  date = {2009},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2009_Algebra.pdf}
}

@unpublished{Huber-Klawitter2010,
  title = {Kommutative {{Algebra}} Und {{Einf{\"u}hrung}} in Die Algebraische {{Geometrie}} (Mit {{Notizen}})},
  date = {2010},
  author = {Huber-Klawitter and Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Huber-Klawitter2010_Kommutative-Algebra-und-Einführung-in-die-algebraische-Geometrie-(mit-Notizen).pdf}
}

@article{Zaltieri2015,
  langid = {english},
  title = {-Synuclein and Synapsin {{III}} Cooperatively Regulate Synaptic Function in Dopamine Neurons},
  volume = {128},
  issn = {0021-9533, 1477-9137},
  url = {http://jcs.biologists.org/cgi/doi/10.1242/jcs.157867},
  doi = {10.1242/jcs.157867},
  number = {13},
  journaltitle = {Journal of Cell Science},
  urldate = {2015-11-12},
  date = {2015-07-01},
  pages = {2231--2243},
  author = {Zaltieri, M. and Grigoletto, J. and Longhena, F. and Navarria, L. and Favero, G. and Castrezzati, S. and Colivicchi, M. A. and Della Corte, L. and Rezzani, R. and Pizzi, M. and Benfenati, F. and Spillantini, M. G. and Missale, C. and Spano, P. and Bellucci, A.},
  file = {/home/fh/lib/articles/Zaltieri2015.pdf;/home/fh/lib/articles/Zaltieri20152.pdf}
}

@unpublished{Hoffmann2010a,
  title = {Kommutative {{Algebra}} - {{{\"U}bungen}}},
  date = {2010},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2010_Kommutative-Algebra---Übungen.pdf}
}

@article{Bacque-Cazenave2015,
  langid = {english},
  title = {The Effect of Sensory Feedback on Crayfish Posture and Locomotion: {{II}}. {{Neuromechanical}} Simulation of Closing the Loop},
  volume = {113},
  issn = {0022-3077, 1522-1598},
  url = {http://jn.physiology.org/lookup/doi/10.1152/jn.00870.2014},
  doi = {10.1152/jn.00870.2014},
  shorttitle = {The Effect of Sensory Feedback on Crayfish Posture and Locomotion},
  number = {6},
  journaltitle = {Journal of Neurophysiology},
  urldate = {2015-12-08},
  date = {2015-03-15},
  pages = {1772--1783},
  author = {Bacqu{\'e}-Cazenave, Julien and Chung, Bryce and Cofer, David W. and Cattaert, Daniel and Edwards, Donald H.},
  file = {/home/fh/lib/articles/Bacqué-Cazenave2015.pdf}
}

@unpublished{Hoffmann2009c,
  title = {Funktionentheorie},
  date = {2009},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2009_Funktionentheorie.pdf}
}

@book{Hastie2009,
  location = {{New York, NY}},
  title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  edition = {2nd ed},
  isbn = {978-0-387-84857-0 978-0-387-84858-7},
  shorttitle = {The Elements of Statistical Learning},
  pagetotal = {745},
  series = {Springer series in statistics},
  publisher = {{Springer}},
  date = {2009},
  keywords = {Bioinformatics,Machine learning,Statistics,Methodology,Data mining,Inference,Forecasting,Computational intelligence},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
  file = {/home/fh/lib/books/Hastie2009_The-elements-of-statistical-learning-data-mining,-inference,-and-prediction.pdf}
}

@book{MacKay2003,
  location = {{Cambridge, UK ; New York}},
  title = {Information Theory, Inference, and Learning Algorithms},
  isbn = {978-0-521-64298-9},
  pagetotal = {628},
  publisher = {{Cambridge University Press}},
  date = {2003},
  keywords = {Information theory},
  author = {MacKay, David J. C.},
  file = {/home/fh/lib/books/MacKay2003_Information-theory,-inference,-and-learning-algorithms.pdf}
}

@article{Hines2009,
  title = {{{NEURON}} and {{Python}}},
  volume = {3},
  issn = {1662-5196},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2636686/},
  doi = {10.3389/neuro.11.001.2009},
  abstract = {The NEURON simulation program now allows Python to be used, alone or in combination with NEURON's traditional Hoc interpreter. Adding Python to NEURON has the immediate benefit of making available a very extensive suite of analysis tools written for engineering and science. It also catalyzes NEURON software development by offering users a modern programming tool that is recognized for its flexibility and power to create and maintain complex programs. At the same time, nothing is lost because all existing models written in Hoc, including graphical user interface tools, continue to work without change and are also available within the Python context. An example of the benefits of Python availability is the use of the xml module in implementing NEURON's Import3D and CellBuild tools to read MorphML and NeuroML model specifications.},
  journaltitle = {Frontiers in Neuroinformatics},
  shortjournal = {Front Neuroinformatics},
  urldate = {2015-12-14},
  date = {2009-01-28},
  author = {Hines, Michael L. and Davison, Andrew P. and Muller, Eilif},
  file = {/home/fh/lib/articles/Hines2009.pdf},
  eprinttype = {pmid},
  eprint = {19198661},
  pmcid = {PMC2636686}
}

@book{Smola2008,
  title = {Introduction to {{Machine Learning}}},
  publisher = {{Cambridge University Press}},
  date = {2008},
  author = {Smola, Alex and Vishwanathan, S.V.N.},
  file = {/home/fh/lib/books/Smola2008_Introduction-to-Machine-Learning.pdf}
}

@article{Morante2008,
  title = {The Color Vision Circuit in the Medulla of {{Drosophila}}},
  volume = {18},
  issn = {0960-9822},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2430089/},
  doi = {10.1016/j.cub.2008.02.075},
  abstract = {Background
Color vision requires comparison between photoreceptors that are sensitive to different wavelengths of light. In Drosophila, this is achieved by the inner photoreceptors (R7 and R8) that contain different rhodopsins. Two types of comparisons can occur in fly color vision: between the R7 (UV-sensitive) and R8 (blue or green-sensitive) photoreceptor cells within one ommatidium (unit eye); or between different ommatidia that contain spectrally distinct inner photoreceptors. Photoreceptors project to the optic lobes: R1-6, which are involved in motion detection, project to the lamina, while R7 and R8 reach deeper in the medulla. This paper analyzes the neural network underlying color vision in the medulla.

Results
We reconstruct the neural network in the medulla, focusing on neurons likely to be involved in processing color vision. We identify the full complement of neurons in the medulla, including second order neurons that contact both R7 and R8 from a single ommatidium, or contact R7 and/or R8 from different ommatidia. We also examine third order neurons and local neurons that likely modulate information from second order neurons. Finally, we present highly specific tools that will allow us to functionally manipulate the network and test both activity and behavior.

Conclusions
This precise characterization of the medulla circuitry will allow us to understand how color vision is processed in the optic lobe of Drosophila, providing a paradigm for more complex systems in vertebrates.},
  number = {8},
  journaltitle = {Current biology : CB},
  shortjournal = {Curr Biol},
  urldate = {2015-12-02},
  date = {2008-04-22},
  pages = {553--565},
  author = {Morante, Javier and Desplan, Claude},
  file = {/home/fh/lib/articles/Morante2008.pdf;/home/fh/lib/articles/Morante2008.pdf},
  eprinttype = {pmid},
  eprint = {18403201},
  pmcid = {PMC2430089}
}

@article{Hay2011,
  langid = {english},
  title = {Models of {{Neocortical Layer}} 5b {{Pyramidal Cells Capturing}} a {{Wide Range}} of {{Dendritic}} and {{Perisomatic Active Properties}}},
  volume = {7},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1002107},
  doi = {10.1371/journal.pcbi.1002107},
  number = {7},
  journaltitle = {PLoS Computational Biology},
  urldate = {2015-11-30},
  date = {2011-07-28},
  pages = {e1002107},
  author = {Hay, Etay and Hill, Sean and Sch{\"u}rmann, Felix and Markram, Henry and Segev, Idan},
  editor = {Graham, Lyle J.},
  file = {/home/fh/lib/articles/Hay2011.pdf}
}

@article{Gulledge2012,
  langid = {english},
  title = {Electrical {{Advantages}} of {{Dendritic Spines}}},
  volume = {7},
  issn = {1932-6203},
  url = {http://dx.plos.org/10.1371/journal.pone.0036007},
  doi = {10.1371/journal.pone.0036007},
  number = {4},
  journaltitle = {PLoS ONE},
  urldate = {2015-11-30},
  date = {2012-04-20},
  pages = {e36007},
  author = {Gulledge, Allan T. and Carnevale, Nicholas T. and Stuart, Greg J.},
  editor = {Mansvelder, Huibert D.},
  file = {/home/fh/lib/articles/Gulledge2012.pdf}
}

@article{Plaza2014,
  langid = {english},
  title = {Toward Large-Scale Connectome Reconstructions},
  volume = {25},
  issn = {09594388},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S095943881400035X},
  doi = {10.1016/j.conb.2014.01.019},
  journaltitle = {Current Opinion in Neurobiology},
  urldate = {2015-12-01},
  date = {2014-04},
  pages = {201--210},
  keywords = {_tablet},
  author = {Plaza, Stephen M and Scheffer, Louis K and Chklovskii, Dmitri B},
  file = {/home/fh/lib/articles/Plaza2014.pdf}
}

@article{Britten1992,
  langid = {english},
  title = {The Analysis of Visual Motion: A Comparison of Neuronal and Psychophysical Performance},
  volume = {12},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/12/12/4745},
  shorttitle = {The Analysis of Visual Motion},
  abstract = {We compared the ability of psychophysical observers and single cortical neurons to discriminate weak motion signals in a stochastic visual display. All data were obtained from rhesus monkeys trained to perform a direction discrimination task near psychophysical threshold. The conditions for such a comparison were ideal in that both psychophysical and physiological data were obtained in the same animals, on the same sets of trials, and using the same visual display. In addition, the psychophysical task was tailored in each experiment to the physiological properties of the neuron under study; the visual display was matched to each neuron's preference for size, speed, and direction of motion. Under these conditions, the sensitivity of most MT neurons was very similar to the psychophysical sensitivity of the animal observers. In fact, the responses of single neurons typically provided a satisfactory account of both absolute psychophysical threshold and the shape of the psychometric function relating performance to the strength of the motion signal. Thus, psychophysical decisions in our task are likely to be based upon a relatively small number of neural signals. These signals could be carried by a small number of neurons if the responses of the pooled neurons are statistically independent. Alternatively, the signals may be carried by a much larger pool of neurons if their responses are partially intercorrelated.},
  number = {12},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2015-12-03},
  date = {1992-01-12},
  pages = {4745--4765},
  author = {Britten, K. H. and Shadlen, M. N. and Newsome, W. T. and Movshon, J. A.},
  file = {/home/fh/lib/articles/Britten1992.pdf},
  eprinttype = {pmid},
  eprint = {1464765}
}

@report{zotero-null-619,
  title = {Auctex},
  file = {/home/fh/lib/manuals/emacs/auctex.pdf},
  note = {manuals/emacs}
}

@book{Trappenberg2010,
  location = {{Oxford ; New York}},
  title = {Fundamentals of Computational Neuroscience},
  edition = {2nd ed},
  isbn = {978-0-19-956841-3},
  abstract = {"Computational neuroscience is the theoretical study of the brain to uncover the principles and mechanisms that guide the development, organization, information processing, and mental functions of the nervous system. Although not a new area, it is only recently that enough knowledge has been gathered to establish computational neuroscience as a scientific discipline in its own right. Given the complexity of the field, and its increasing importance in progressing our understanding of how the brain works, there has long been a need for an introductory text on what is often assumed to be an impenetrable topic. The new edition of Fundamentals of Computational Neuroscience build on the success and strengths of the first edition. It introduces the theoretical foundations of neuroscience with a focus on the nature of information processing in the brain. The book covers the introduction and motivation of simplified models of neurons that are suitable for exploring information processing in large brain-like networks. Additionally, it introduces several fundamental network architectures and discusses their relevance for information processing in the brain, giving some examples of models of higher-order cognitive functions to demonstrate the advanced insight that can be gained with such studies. Each chapter starts by introducing its topic with experimental facts and conceptual questions related to the study of brain function. An additional feature is the inclusion of simple Matlab programs that can be used to explore many of the mechanisms explained in the book. An accompanying webpage includes programs for download. The book is aimed at those within the brain and cognitive sciences, from graduate level and upwards"--Provided by publisher},
  pagetotal = {390},
  publisher = {{Oxford University Press}},
  date = {2010},
  keywords = {Neurosciences,Neurons,Physiology,Computational Biology,methods,Models; Neurological,Nerve Net,computational neuroscience,Brain},
  author = {Trappenberg, Thomas P.},
  file = {/home/fh/lib/books/Trappenberg2010_Fundamentals-of-computational-neuroscience.pdf}
}

@article{Hartmann2015,
  title = {Where's the {{Noise}}? {{Key Features}} of {{Spontaneous Activity}} and {{Neural Variability Arise}} through {{Learning}} in a {{Deterministic Network}}},
  volume = {11},
  url = {http://dx.doi.org/10.1371/journal.pcbi.1004640},
  doi = {10.1371/journal.pcbi.1004640},
  shorttitle = {Where's the {{Noise}}?},
  abstract = {Author Summary Neural recordings seem very noisy. If the exact same stimulus is shown to an animal multiple times, the neural response will vary substantially. In fact, the activity of a single neuron shows many features of a random process. Furthermore, the spontaneous activity occurring in the absence of any sensory stimulus, which is usually considered a kind of background noise, often has a magnitude comparable to the activity evoked by stimulus presentation and interacts with sensory inputs in interesting ways. Here we show that the key features of neural variability and spontaneous activity can all be accounted for by a simple and completely deterministic neural network learning a predictive model of its sensory inputs. The network's deterministic dynamics give rise to structured but variable responses matching key experimental findings obtained in different mammalian species with different recording techniques. Our results suggest that the notorious variability of neural recordings and the complex features of spontaneous brain activity could reflect the dynamics of a largely deterministic but highly adaptive network learning a predictive model of its sensory environment.},
  number = {12},
  journaltitle = {PLoS Comput Biol},
  shortjournal = {PLoS Comput Biol},
  urldate = {2016-01-08},
  date = {2015-12-29},
  pages = {e1004640},
  author = {Hartmann, Christoph and Lazar, Andreea and Nessler, Bernhard and Triesch, Jochen},
  file = {/home/fh/lib/articles/Hartmann2015.pdf}
}

@article{Barbour2007,
  langid = {english},
  title = {What Can We Learn from Synaptic Weight Distributions?},
  volume = {30},
  issn = {01662236},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0166223607002615},
  doi = {10.1016/j.tins.2007.09.005},
  number = {12},
  journaltitle = {Trends in Neurosciences},
  urldate = {2016-02-25},
  date = {2007-12},
  pages = {622--629},
  author = {Barbour, Boris and Brunel, Nicolas and Hakim, Vincent and Nadal, Jean-Pierre},
  file = {/home/fh/lib/articles/Barbour2007.pdf}
}

@report{zotero-null-627,
  title = {The {{Docker Book}}},
  file = {/home/fh/lib/manuals/docker/the_docker_book.pdf},
  note = {manuals/docker}
}

@book{Elstrodt2009,
  langid = {german},
  location = {{Berlin}},
  title = {Ma{\ss}- und Integrationstheorie},
  edition = {6., korrigierte Aufl},
  isbn = {978-3-540-89727-9 978-3-540-89728-6},
  pagetotal = {434},
  series = {Springer-Lehrbuch},
  publisher = {{Springer}},
  date = {2009},
  keywords = {Lehrbuch,Integrationstheorie,Maßtheorie,Borel-Menge,Sigma-Algebra,Lebesgue-Maß,Cantor-Diskontinuum,Konvergenztheorie,Borel-Maß,Integrals; generalized,Measure theory},
  author = {Elstrodt, J{\"u}rgen},
  file = {/home/fh/lib/books/Elstrodt2009_Maß--und-Integrationstheorie.pdf}
}

@article{Boettiger2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1410.0846},
  title = {An Introduction to {{Docker}} for Reproducible Research, with Examples from the {{R}} Environment},
  volume = {49},
  issn = {01635980},
  url = {http://arxiv.org/abs/1410.0846},
  doi = {10.1145/2723872.2723882},
  abstract = {As computational work becomes more and more integral to many aspects of scientific research, computational reproducibility has become an issue of increasing importance to computer systems researchers and domain scientists alike. Though computational reproducibility seems more straight forward than replicating physical experiments, the complex and rapidly changing nature of computer environments makes being able to reproduce and extend such work a serious challenge. In this paper, I explore common reasons that code developed for one research project cannot be successfully executed or extended by subsequent researchers. I review current approaches to these issues, including virtual machines and workflow systems, and their limitations. I then examine how the popular emerging technology Docker combines several areas from systems research - such as operating system virtualization, cross-platform portability, modular re-usable elements, versioning, and a `DevOps' philosophy, to address these challenges. I illustrate this with several examples of Docker use with a focus on the R statistical environment.},
  number = {1},
  journaltitle = {ACM SIGOPS Operating Systems Review},
  urldate = {2016-01-09},
  date = {2015-01-20},
  pages = {71--79},
  keywords = {Computer Science - Software Engineering},
  author = {Boettiger, Carl},
  file = {/home/fh/lib/articles/Boettiger2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/9EXWD4S3/1410.html}
}

@article{Borst2015,
  title = {Common Circuit Design in Fly and Mammalian Motion Vision},
  volume = {18},
  issn = {1097-6256, 1546-1726},
  url = {http://www.nature.com/doifinder/10.1038/nn.4050},
  doi = {10.1038/nn.4050},
  number = {8},
  journaltitle = {Nature Neuroscience},
  urldate = {2015-12-04},
  date = {2015-06-29},
  pages = {1067--1076},
  author = {Borst, Alexander and Helmstaedter, Moritz},
  file = {/home/fh/lib/articles/Borst2015.pdf}
}

@article{Borst2010,
  langid = {english},
  title = {Fly {{Motion Vision}}},
  volume = {33},
  issn = {0147-006X, 1545-4126},
  url = {http://www.annualreviews.org/doi/abs/10.1146/annurev-neuro-060909-153155},
  doi = {10.1146/annurev-neuro-060909-153155},
  number = {1},
  journaltitle = {Annual Review of Neuroscience},
  urldate = {2015-12-04},
  date = {2010-06},
  pages = {49--70},
  author = {Borst, Alexander and Haag, Juergen and Reiff, Dierk F.},
  file = {/home/fh/lib/articles/Borst2010.pdf}
}

@article{Maisak2013,
  title = {A Directional Tuning Map of {{Drosophila}} Elementary Motion Detectors},
  volume = {500},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/nature12320},
  doi = {10.1038/nature12320},
  number = {7461},
  journaltitle = {Nature},
  urldate = {2015-12-04},
  date = {2013-08-07},
  pages = {212--216},
  author = {Maisak, Matthew S. and Haag, Juergen and Ammer, Georg and Serbe, Etienne and Meier, Matthias and Leonhardt, Aljoscha and Schilling, Tabea and Bahl, Armin and Rubin, Gerald M. and Nern, Aljoscha and Dickson, Barry J. and Reiff, Dierk F. and Hopp, Elisabeth and Borst, Alexander},
  file = {/home/fh/lib/articles/Maisak2013.pdf}
}

@article{Markram2015,
  title = {Reconstruction and {{Simulation}} of {{Neocortical Microcircuitry}}},
  volume = {163},
  issn = {0092-8674},
  url = {http://www.sciencedirect.com/science/article/pii/S0092867415011915},
  doi = {10.1016/j.cell.2015.09.029},
  abstract = {Summary
We present a first-draft digital reconstruction of the microcircuitry of somatosensory cortex of juvenile rat. The reconstruction uses cellular and synaptic organizing principles to algorithmically reconstruct detailed anatomy and physiology from sparse experimental data. An objective anatomical method defines a neocortical volume of 0.29 $\pm$ 0.01~mm3 containing $\sim$31,000 neurons, and patch-clamp studies identify 55 layer-specific morphological and 207 morpho-electrical neuron subtypes. When digitally reconstructed neurons are positioned in the volume and synapse formation is restricted to biological bouton densities and numbers of synapses per connection, their overlapping arbors form $\sim$8 million connections with $\sim$37 million synapses. Simulations reproduce an~array of in~vitro and in~vivo experiments without parameter tuning. Additionally, we find a spectrum of network states with a sharp transition from synchronous to asynchronous activity, modulated by physiological mechanisms. The spectrum of network states, dynamically reconfigured around this transition, supports diverse information processing strategies.
PaperClip

Video Abstract},
  number = {2},
  journaltitle = {Cell},
  shortjournal = {Cell},
  urldate = {2015-12-04},
  date = {2015-10-08},
  pages = {456--492},
  author = {Markram, Henry and Muller, Eilif and Ramaswamy, Srikanth and Reimann, Michael W. and Abdellah, Marwan and Sanchez, Carlos Aguado and Ailamaki, Anastasia and Alonso-Nanclares, Lidia and Antille, Nicolas and Arsever, Selim and Kahou, Guy Antoine Atenekeng and Berger, Thomas K. and Bilgili, Ahmet and Buncic, Nenad and Chalimourda, Athanassia and Chindemi, Giuseppe and Courcol, Jean-Denis and Delalondre, Fabien and Delattre, Vincent and Druckmann, Shaul and Dumusc, Raphael and Dynes, James and Eilemann, Stefan and Gal, Eyal and Gevaert, Michael Emiel and Ghobril, Jean-Pierre and Gidon, Albert and Graham, Joe W. and Gupta, Anirudh and Haenel, Valentin and Hay, Etay and Heinis, Thomas and Hernando, Juan B. and Hines, Michael and Kanari, Lida and Keller, Daniel and Kenyon, John and Khazen, Georges and Kim, Yihwa and King, James G. and Kisvarday, Zoltan and Kumbhar, Pramod and Lasserre, S{\'e}bastien and Le B{\'e}, Jean-Vincent and Magalh{\~a}es, Bruno R. C. and Merch{\'a}n-P{\'e}rez, Angel and Meystre, Julie and Morrice, Benjamin Roy and Muller, Jeffrey and Mu{\~n}oz-C{\'e}spedes, Alberto and Muralidhar, Shruti and Muthurasa, Keerthan and Nachbaur, Daniel and Newton, Taylor H. and Nolte, Max and Ovcharenko, Aleksandr and Palacios, Juan and Pastor, Luis and Perin, Rodrigo and Ranjan, Rajnish and Riachi, Imad and Rodr{\'\i}guez, Jos{\'e}-Rodrigo and Riquelme, Juan Luis and R{\"o}ssert, Christian and Sfyrakis, Konstantinos and Shi, Ying and Shillcock, Julian C. and Silberberg, Gilad and Silva, Ricardo and Tauheed, Farhan and Telefont, Martin and Toledo-Rodriguez, Maria and Tr{\"a}nkler, Thomas and Van Geit, Werner and D{\'\i}az, Jafet Villafranca and Walker, Richard and Wang, Yun and Zaninetta, Stefano M. and DeFelipe, Javier and Hill, Sean L. and Segev, Idan and Sch{\"u}rmann, Felix},
  file = {/home/fh/lib/articles/Markram2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/26VCGNB9/S0092867415011915.html}
}

@article{Gidon2012,
  title = {Principles {{Governing}} the {{Operation}} of {{Synaptic Inhibition}} in {{Dendrites}}},
  volume = {75},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627312004813},
  doi = {10.1016/j.neuron.2012.05.015},
  abstract = {Summary
Synaptic inhibition plays a key role in shaping the dynamics of neuronal networks and selecting cell assemblies. Typically, an inhibitory axon contacts a particular dendritic subdomain of its target neuron, where it often makes 10\textendash{}20 synapses, sometimes on very distal branches. The functional implications of such a connectivity pattern are not well understood. Our experimentally based theoretical study highlights several new and counterintuitive principles for dendritic inhibition. We show that distal ``off-path'' rather than proximal ``on-path'' inhibition effectively dampens proximal excitable dendritic ``hotspots,'' thus powerfully controlling the neuron's output. Additionally, with multiple synaptic contacts, inhibition operates globally, spreading centripetally hundreds of micrometers from the inhibitory synapses. Consequently, inhibition in regions lacking inhibitory synapses may exceed that at the synaptic sites themselves. These results offer new insights into the synergetic effect of dendritic inhibition in controlling dendritic excitability and plasticity and in dynamically molding functional dendritic subdomains and their output.},
  number = {2},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2015-12-08},
  date = {2012-07-26},
  pages = {330--341},
  author = {Gidon, Albert and Segev, Idan},
  file = {/home/fh/lib/articles/Gidon2012.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/TRNR34ZS/S0896627312004813.html}
}

@article{Turrigiano2008,
  title = {The {{Self}}-{{Tuning Neuron}}: {{Synaptic Scaling}} of {{Excitatory Synapses}}},
  volume = {135},
  issn = {0092-8674},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2834419/},
  doi = {10.1016/j.cell.2008.10.008},
  shorttitle = {The {{Self}}-{{Tuning Neuron}}},
  abstract = {Homeostatic synaptic scaling is a form of synaptic plasticity that adjusts the strength of all of a neuron's excitatory synapses up or down to stabilize firing. Current evidence suggests that neurons detect changes in their own firing rates through a set of calcium-dependent sensors that then regulate receptor trafficking to increase or decrease the accumulation of glutamate receptors at synaptic sites. Additional mechanisms may allow local or network-wide changes in activity to be sensed through parallel pathways, generating a nested set of homeostatic mechanisms that operate over different temporal and spatial scales.},
  number = {3},
  journaltitle = {Cell},
  shortjournal = {Cell},
  urldate = {2016-03-15},
  date = {2008-10-31},
  pages = {422--435},
  author = {Turrigiano, Gina G.},
  file = {/home/fh/lib/articles/Turrigiano2008.pdf},
  eprinttype = {pmid},
  eprint = {18984155},
  pmcid = {PMC2834419}
}

@article{Rall1973,
  title = {Branch {{Input Resistance}} and {{Steady Attenuation}} for {{Input}} to {{One Branch}} of a {{Dendritic Neuron Model}}},
  volume = {13},
  issn = {0006-3495},
  url = {http://www.sciencedirect.com/science/article/pii/S000634957386014X},
  doi = {10.1016/S0006-3495(73)86014-X},
  abstract = {Mathematical solutions and numerical illustrations are presented for the steady-state distribution of membrane potential in an extensively branched neuron model, when steady electric current is injected into only one dendritic branch. Explicit expressions are obtained for input resistance at the branch input site and for voltage attenuation from the input site to the soma; expressions for AC steady-state input impedance and attenuation are also presented. The theoretical model assumes passive membrane properties and the equivalent cylinder constraint on branch diameters. Numerical examples illustrate how branch input resistance and steady attenuation depend upon the following: the number of dendritic trees, the orders of dendritic branching, the electrotonic length of the dendritic trees, the location of the dendritic input site, and the input resistance at the soma. The application to cat spinal motoneurons, and to other neuron types, is discussed. The effect of a large dendritic input resistance upon the amount of local membrane depolarization at the synaptic site, and upon the amount of depolarization reaching the soma, is illustrated and discussed; simple proportionality with input resistance does not hold, in general. Also, branch input resistance is shown to exceed the input resistance at the soma by an amount that is always less than the sum of core resistances along the path from the input site to the soma.},
  number = {7},
  journaltitle = {Biophysical Journal},
  shortjournal = {Biophysical Journal},
  urldate = {2015-12-08},
  date = {1973-07},
  pages = {648--688},
  author = {Rall, Wilfrid and Rinzel, John},
  file = {/home/fh/lib/articles/Rall1973.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/NDHZCP2S/S000634957386014X.html}
}

@article{Branco2010,
  langid = {english},
  title = {Dendritic {{Discrimination}} of {{Temporal Input Sequences}} in {{Cortical Neurons}}},
  volume = {329},
  issn = {0036-8075, 1095-9203},
  url = {http://www.sciencemag.org/content/329/5999/1671},
  doi = {10.1126/science.1189664},
  abstract = {The detection and discrimination of temporal sequences is fundamental to brain function and underlies perception, cognition, and motor output. By applying patterned, two-photon glutamate uncaging, we found that single dendrites of cortical pyramidal neurons exhibit sensitivity to the sequence of synaptic activation. This sensitivity is encoded by both local dendritic calcium signals and somatic depolarization, leading to sequence-selective spike output. The mechanism involves dendritic impedance gradients and nonlinear synaptic N-methyl-d-aspartate receptor activation and is generalizable to dendrites in different neuronal types. This enables discrimination of patterns delivered to a single dendrite, as well as patterns distributed randomly across the dendritic tree. Pyramidal cell dendrites can thus act as processing compartments for the detection of synaptic sequences, thereby implementing a fundamental cortical computation.},
  number = {5999},
  journaltitle = {Science},
  shortjournal = {Science},
  urldate = {2015-12-08},
  date = {2010-09-24},
  pages = {1671--1675},
  author = {Branco, Tiago and Clark, Beverley A. and H{\"a}usser, Michael},
  file = {/home/fh/lib/articles/Branco2010.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/2INIKHAX/1671.html},
  eprinttype = {pmid},
  eprint = {20705816}
}

@collection{Sterratt2011,
  location = {{Cambridge ; New York}},
  title = {Principles of Computational Modelling in Neuroscience},
  isbn = {978-0-521-87795-4},
  pagetotal = {390},
  publisher = {{Cambridge University Press}},
  date = {2011},
  keywords = {Synaptic Transmission,Models; Neurological,Computer Simulation,Neural Conduction,computational neuroscience},
  editor = {Sterratt, David},
  file = {/home/fh/lib/books/Sterratt2011_Principles-of-computational-modelling-in-neuroscience.pdf}
}

@article{Lu2015,
  title = {Neuroscience: {{Forgetfulness}} Illuminated},
  volume = {525},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/nature15211},
  doi = {10.1038/nature15211},
  shorttitle = {Neuroscience},
  number = {7569},
  journaltitle = {Nature},
  urldate = {2015-12-09},
  date = {2015-09-09},
  pages = {324--325},
  author = {Lu, Ju and Zuo, Yi},
  file = {/home/fh/lib/articles/Lu2015.pdf}
}

@article{Mainen1996,
  langid = {english},
  title = {Influence of Dendritic Structure on Firing Pattern in Model Neocortical Neurons},
  volume = {382},
  url = {http://www.nature.com/nature/journal/v382/n6589/abs/382363a0.html},
  doi = {10.1038/382363a0},
  number = {6589},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2015-12-10},
  date = {1996-07-25},
  pages = {363--366},
  author = {Mainen, Zachary F. and Sejnowski, Terrence J.},
  file = {/home/fh/lib/articles/Mainen1996.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/64K6STJ5/382363a0.html}
}

@article{Hayashi-Takagi2015,
  title = {Labelling and Optical Erasure of Synaptic Memory Traces in the Motor Cortex},
  volume = {525},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/nature15257},
  doi = {10.1038/nature15257},
  number = {7569},
  journaltitle = {Nature},
  urldate = {2015-12-09},
  date = {2015-09-09},
  pages = {333--338},
  author = {Hayashi-Takagi, Akiko and Yagishita, Sho and Nakamura, Mayumi and Shirai, Fukutoshi and Wu, Yi I. and Loshbaugh, Amanda L. and Kuhlman, Brian and Hahn, Klaus M. and Kasai, Haruo},
  file = {/home/fh/lib/articles/Hayashi-Takagi2015.pdf}
}

@article{Sachdev2012,
  title = {Surround Suppression and Sparse Coding in Visual and Barrel Cortices},
  volume = {6},
  issn = {1662-5110},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3389675/},
  doi = {10.3389/fncir.2012.00043},
  abstract = {During natural vision the entire retina is stimulated. Likewise, during natural tactile behaviors, spatially extensive regions of the somatosensory surface are co-activated. The large spatial extent of naturalistic stimulation means that surround suppression, a phenomenon whose neural mechanisms remain a matter of debate, must arise during natural behavior. To identify common neural motifs that might instantiate surround suppression across modalities, we review models of surround suppression and compare the evidence supporting the competing ideas that surround suppression has either cortical or sub-cortical origins in visual and barrel cortex. In the visual system there is general agreement lateral inhibitory mechanisms contribute to surround suppression, but little direct experimental evidence that intracortical inhibition plays a major role. Two intracellular recording studies of V1, one using naturalistic stimuli (Haider et al., ), the other sinusoidal gratings (Ozeki et al., ), sought to identify the causes of reduced activity in V1 with increasing stimulus size, a hallmark of surround suppression. The former attributed this effect to increased inhibition, the latter to largely balanced withdrawal of excitation and inhibition. In rodent primary somatosensory barrel cortex, multi-whisker responses are generally weaker than single whisker responses, suggesting multi-whisker stimulation engages similar surround suppressive mechanisms. The origins of suppression in S1 remain elusive: studies have implicated brainstem lateral/internuclear interactions and both thalamic and cortical inhibition. Although the anatomical organization and instantiation of surround suppression in the visual and somatosensory systems differ, we consider the idea that one common function of surround suppression, in both modalities, is to remove the statistical redundancies associated with natural stimuli by increasing the sparseness or selectivity of sensory responses.},
  journaltitle = {Frontiers in Neural Circuits},
  shortjournal = {Front Neural Circuits},
  urldate = {2015-12-10},
  date = {2012-07-05},
  author = {Sachdev, Robert N. S. and Krause, Matthew R. and Mazer, James A.},
  file = {/home/fh/lib/articles/Sachdev2012.pdf},
  eprinttype = {pmid},
  eprint = {22783169},
  pmcid = {PMC3389675}
}

@report{zotero-null-670,
  title = {{{FreeNAS}} 9.3},
  file = {/home/fh/lib/manuals/server/freenas_9.3.pdf},
  note = {manuals/server}
}

@article{Miner2015,
  title = {[{{Preprint}}] {{Plasticity}}-{{Driven Self}}-{{Organization}} under {{Topological Constraints Accounts}} for {{Non}}-{{Random Features}} of {{Cortical Synaptic Wiring}}.},
  date = {2015},
  author = {Miner and Triesch},
  file = {/home/fh/lib/articles/Miner2015.pdf}
}

@article{Duijnhouwer2001,
  title = {Influence of Dendritic Topology on Firing Patterns in Model Neurons},
  volume = {38\textendash{}40},
  issn = {0925-2312},
  url = {http://www.sciencedirect.com/science/article/pii/S0925231201005021},
  doi = {10.1016/S0925-2312(01)00502-1},
  abstract = {Neuronal electrophysiology is influenced by both channel distribution and morphology. Distinguishing two sources of morphological variability\textemdash{}metrics and topology\textemdash{}we show that model neurons sharing the same channel densities and anatomical size can derive functional differentiation from their dendritic topology. Firing frequencies in these metrically reduced neurons show a strong correlation with both mean path length and total electrotonic transformed size of the dendritic tree. This dependency of spiking behaviour is robust to different modes of stimulation, different tapering powers, and the absence of active dendritic channels.},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  series = {Computational Neuroscience: Trends in Research 2001},
  urldate = {2016-01-15},
  date = {2001-06},
  pages = {183--189},
  keywords = {morphology,Path length,Topology,Firing patterns,Morpho-electrotonic transform},
  author = {Duijnhouwer, Jacob and Remme, Michiel W. H and van Ooyen, Arjen and van Pelt, Jaap},
  options = {useprefix=true},
  file = {/home/fh/lib/articles/Duijnhouwer2001.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/R93ASR8H/S0925231201005021.html}
}

@article{Stevenson2008,
  title = {Inferring Functional Connections between Neurons},
  volume = {18},
  issn = {0959-4388},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2706692/},
  doi = {10.1016/j.conb.2008.11.005},
  abstract = {A central question in neuroscience is how interactions between neurons give rise to behavior. In many electrophysiological experiments, the activity of a set of neurons is recorded while sensory stimuli or movement tasks are varied. Tools that aim to reveal underlying interactions between neurons from such data can be extremely useful. Traditionally, neuroscientists have studied these interactions using purely descriptive statistics (cross-correlograms or joint peri-stimulus time histograms). However, the interpretation of such data is often difficult, particularly as the number of recorded neurons grows. Recent research suggests that model-based, maximum likelihood methods can improve these analyses. In addition to estimating neural interactions, application of these techniques has improved decoding of external variables, created novel interpretations of existing electrophysiological data, and may provide new insight into how the brain represents information.},
  number = {6},
  journaltitle = {Current opinion in neurobiology},
  shortjournal = {Curr Opin Neurobiol},
  urldate = {2016-03-04},
  date = {2008-12},
  pages = {582--588},
  author = {Stevenson, Ian H. and Rebesco, James M. and Miller, Lee E. and K{\"o}rding, Konrad P.},
  file = {/home/fh/lib/articles/Stevenson2008.pdf},
  eprinttype = {pmid},
  eprint = {19081241},
  pmcid = {PMC2706692}
}

@article{deSousa2015,
  langid = {english},
  title = {Dendritic Morphology Predicts Pattern Recognition Performance in Multi-Compartmental Model Neurons with and without Active Conductances},
  volume = {38},
  issn = {0929-5313, 1573-6873},
  url = {http://link.springer.com/10.1007/s10827-014-0537-1},
  doi = {10.1007/s10827-014-0537-1},
  number = {2},
  journaltitle = {Journal of Computational Neuroscience},
  urldate = {2016-01-15},
  date = {2015-04},
  pages = {221--234},
  author = {de Sousa, Giseli and Maex, Reinoud and Adams, Rod and Davey, Neil and Steuber, Volker},
  options = {useprefix=true},
  file = {/home/fh/lib/articles/de Sousa2015.pdf}
}

@article{Fox2005,
  title = {A {{Comparison}} of {{Experience}}-{{Dependent Plasticity}} in the {{Visual}} and {{Somatosensory Systems}}},
  volume = {48},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627305008779},
  doi = {10.1016/j.neuron.2005.10.013},
  abstract = {Summary
In the visual and somatosensory systems, maturation of neuronal circuits continues for days to weeks after sensory stimulation occurs. Deprivation of sensory input at various stages of development can induce physiological, and often structural, changes that modify the circuitry of these sensory systems. Recent studies also reveal a surprising degree of plasticity in the mature visual and somatosensory pathways. Here, we compare and contrast the effects of sensory experience on the connectivity and function of these pathways and discuss what is known to date concerning the structural, physiological, and molecular mechanisms underlying their plasticity.},
  number = {3},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2015-12-10},
  date = {2005-11-03},
  pages = {465--477},
  author = {Fox, Kevin and Wong, Rachel O. L.},
  file = {/home/fh/lib/articles/Fox2005.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/69R9XRVF/S0896627305008779.html}
}

@article{Sadeh2015a,
  title = {Emergence of {{Functional Specificity}} in {{Balanced Networks}} with {{Synaptic Plasticity}}},
  volume = {11},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004307},
  doi = {10.1371/journal.pcbi.1004307},
  abstract = {Author Summary   In primary visual cortex of mammals, neurons are selective to the orientation of contrast edges. In some species, as cats and monkeys, neurons preferring similar orientations are adjacent on the cortical surface, leading to smooth orientation maps. In rodents, in contrast, such spatial orientation maps do not exist, and neurons of different specificities are mixed in a salt-and-pepper fashion. During development, however, a ``functional'' map of orientation selectivity emerges, where connections between neurons of similar preferred orientations are selectively enhanced. Here we show how such feature-specific connectivity can arise in realistic neocortical networks of excitatory and inhibitory neurons. Our results demonstrate how recurrent dynamics can work in cooperation with synaptic plasticity to form networks where neurons preferring similar stimulus features connect more strongly together. Such networks, in turn, are known to enhance the specificity of neuronal responses to a stimulus. Our study thus reveals how self-organizing connectivity in neuronal networks enable them to achieve new or enhanced functions, and it underlines the essential role of recurrent inhibition and plasticity in this process.},
  number = {6},
  journaltitle = {PLOS Comput Biol},
  shortjournal = {PLOS Comput Biol},
  urldate = {2016-03-17},
  date = {2015-06-19},
  pages = {e1004307},
  keywords = {neural networks,Neurons,synaptic plasticity,Neuronal plasticity,Synapses,Membrane potential,Neuronal tuning,Learning curves},
  author = {Sadeh, Sadra and Clopath, Claudia and Rotter, Stefan},
  file = {/home/fh/lib/articles/Sadeh2015_2.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/ESB8499U/article.html}
}

@article{Borst1999,
  langid = {english},
  title = {Information Theory and Neural Coding},
  volume = {2},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v2/n11/full/nn1199_947.html},
  doi = {10.1038/14731},
  abstract = {Information theory quantifies how much information a neural response carries about the stimulus. This can be compared to the information transferred in particular models of the stimulus-response function and to maximum possible information transfer. Such comparisons are crucial because they validate assumptions present in any neurophysiological analysis. Here we review information-theory basics before demonstrating its use in neural coding. We show how to use information theory to validate simple stimulus-response models of neural coding of dynamic stimuli. Because these models require specification of spike timing precision, they can reveal which time scales contain information in neural coding. This approach shows that dynamic stimuli can be encoded efficiently by single neurons and that each spike contributes to information transmission. We argue, however, that the data obtained so far do not suggest a temporal code, in which the placement of spikes relative to each other yields additional information.},
  number = {11},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2015-12-10},
  date = {1999-11},
  pages = {947--957},
  author = {Borst, Alexander and Theunissen, Fr{\'e}d{\'e}ric E.},
  file = {/home/fh/lib/articles/Borst1999.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/IU66V685/nn1199_947.html}
}

@article{Rall2009,
  langid = {english},
  title = {Rall Model},
  volume = {4},
  issn = {1941-6016},
  url = {http://www.scholarpedia.org/article/Rall_model},
  doi = {10.4249/scholarpedia.1369},
  number = {4},
  journaltitle = {Scholarpedia},
  urldate = {2015-12-16},
  date = {2009},
  pages = {1369},
  author = {Rall, Wilfrid}
}

@collection{Galizia2013,
  location = {{Heidelberg ; New York}},
  title = {Neurosciences: From Molecule to Behavior: A University Textbook},
  isbn = {978-3-642-10768-9},
  shorttitle = {Neurosciences},
  pagetotal = {736},
  publisher = {{Springer Spektrum}},
  date = {2013},
  keywords = {Neurosciences,Nervous System Physiological Phenomena},
  editor = {Galizia, C. Giovanni and Lledo, Pierre-Marie},
  file = {/home/fh/lib/books/Galizia2013_Neurosciences-from-molecule-to-behavior-a-university-textbook.pdf}
}

@article{Mazzoni2015,
  title = {Computing the {{Local Field Potential}} ({{LFP}}) from {{Integrate}}-and-{{Fire Network Models}}},
  volume = {11},
  url = {http://dx.doi.org/10.1371/journal.pcbi.1004584},
  doi = {10.1371/journal.pcbi.1004584},
  abstract = {Author Summary Leaky integrate-and-fire (LIF) networks are often used to model neural network activity. The spike trains they produce, however, cannot be directly compared to the local field potentials (LFPs) that are measured by low-pass filtering the potential recorded from extracellular electrodes. This is because LFPs are generated by neurons with spatial extensions, while LIF networks typically consist of point neurons. In order to still be able to approximately predict LFPs from LIF network simulations, we here explore simple proxies for computing LFPs based on standard output from LIF network simulations. Predictions from the various LFP proxies were compared with ``ground-truth'' LFPs computed by means of well-established volume conduction theory where synaptic currents corresponding to the LIF network simulation were injected into populations of multi-compartmental neurons with realistic morphologies. We found that a simple weighted sum of the LIF synaptic currents with a single universally applicable set of weights excellently capture the time course of the LFP signal when the LFP predominantly is generated by a single population of pyramidal cells. Our study therefore provides a simple formula by which the LFP signal can be estimated directly from the LIF network activity, providing a missing quantitative link between simple neural models and LFP measures in vivo.},
  number = {12},
  journaltitle = {PLoS Comput Biol},
  shortjournal = {PLoS Comput Biol},
  urldate = {2015-12-16},
  date = {2015-12-14},
  pages = {e1004584},
  author = {Mazzoni, Alberto and Lind{\'e}n, Henrik and Cuntz, Hermann and Lansner, Anders and Panzeri, Stefano and Einevoll, Gaute T.},
  file = {/home/fh/lib/articles/Mazzoni2015.pdf}
}

@article{Shannon1948,
  title = {A Mathematical Theory of Communication},
  volume = {27},
  issn = {0005-8580},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist1 and Hartley2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.},
  number = {3},
  journaltitle = {Bell System Technical Journal, The},
  date = {1948-07},
  pages = {379--423},
  author = {Shannon, C.E.},
  file = {/home/fh/lib/articles/Shannon1948.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/T34Q7RJT/articleDetails.html}
}

@article{Lee2016a,
  title = {Anatomy and Function of an Excitatory Network in the Visual Cortex},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/nature17192},
  doi = {10.1038/nature17192},
  journaltitle = {Nature},
  urldate = {2016-04-03},
  date = {2016-03-28},
  author = {Lee, Wei-Chung Allen and Bonin, Vincent and Reed, Michael and Graham, Brett J. and Hood, Greg and Glattfelder, Katie and Reid, R. Clay},
  file = {/home/fh/lib/articles/Lee2016.pdf}
}

@article{Medinilla2013,
  title = {Features of Proximal and Distal Excitatory Synaptic Inputs to Layer {{V}} Neurons of the Rat Medial Entorhinal Cortex},
  volume = {591},
  issn = {0022-3751},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3630779/},
  doi = {10.1113/jphysiol.2012.237172},
  abstract = {The entorhinal cortex (EC) has a fundamental function in transferring information between the hippocampus and the neocortex. EC layer V principal neurons are the main recipients of the hippocampal output and send processed information to the neocortex, likely playing an important role in memory processing and consolidation. Most of these neurons have apical dendrites that extend to the superficial layers and are rich in spines, which could be the targets of excitatory inputs from fibres innervating that region. We have used electrical stimulation of afferent fibres coupled with whole-cell patch-clamp somatic recordings to study the features of distal excitatory inputs and compare them with those of proximal ones. The amplitude of putative unitary excitatory responses was $\sim$1.5 times larger for distal compared with proximal inputs. The responses were purely glutamatergic, as they were abolished by a combination of AMPA and NMDA glutamatergic receptor antagonists. Blockade of Ih by 4-ethylphenylamino-1,2-dimethyl-6-methylaminopyrimidinium chloride (ZD7288) increased temporal summation; the increase was comparable for proximal and distal inputs. Proximal inputs initiated a somatic spike more reliably than distal ones; in some instances, somatic action potentials triggered by distal stimulation were preceded by dendritic spikes that fully propagated to the soma. Altogether, our results show that medial layer V entorhinal neurons receive excitatory synapses at distal dendritic locations, which gives them access to information encoded by inputs to the superficial layers as well as the deep layers. These findings are fundamentally relevant to understanding the role of the EC in the formation and consolidation of episodic memory.},
  issue = {Pt 1},
  journaltitle = {The Journal of Physiology},
  shortjournal = {J Physiol},
  urldate = {2015-12-18},
  date = {2013-01-01},
  pages = {169--183},
  author = {Medinilla, Virginia and Johnson, Oralee and Gasparini, Sonia},
  file = {/home/fh/lib/articles/Medinilla2013.pdf},
  eprinttype = {pmid},
  eprint = {23006478},
  pmcid = {PMC3630779}
}

@article{Petz1986,
  langid = {english},
  title = {On the Equality in {{Jensen}}'s Inequality for Operator Convex Functions},
  volume = {9},
  issn = {0378-620X, 1420-8989},
  url = {http://link.springer.com/article/10.1007/BF01195811},
  doi = {10.1007/BF01195811},
  abstract = {Let A and B be C*-algebras with unit and assume that $\phi:$A$\rightarrow$B is a positive unit preserving linear mapping. Choi proved thatf($\Phi$(a))$\leqq\Phi$(f(a))f($\backslash$Phi (a)) $\backslash$leqq $\backslash$Phi (f(a)) if a=a*$\in$A and Sp(a)$\subset$($\alpha$, $\beta$) for every operator convex function f: ($\alpha$, $\beta$) $\rightarrow$ $\mathbb{R}$. We prove that the equality holds if and only if $\phi$ restricted to the subalgebra generated by \{a\} is multiplicative. An example is shown as an application.},
  number = {5},
  journaltitle = {Integral Equations and Operator Theory},
  shortjournal = {Integr equ oper theory},
  urldate = {2016-02-15},
  date = {1986-09},
  pages = {744--747},
  keywords = {Analysis},
  author = {Petz, D{\'e}nes},
  file = {/home/fh/lib/articles/Petz1986.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/VVTVF936/10.html}
}

@article{Roxin2011,
  langid = {english},
  title = {On the {{Distribution}} of {{Firing Rates}} in {{Networks}} of {{Cortical Neurons}}},
  volume = {31},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/31/45/16217},
  doi = {10.1523/JNEUROSCI.1677-11.2011},
  abstract = {The distribution of in vivo average firing rates within local cortical networks has been reported to be highly skewed and long tailed. The distribution of average single-cell inputs, conversely, is expected to be Gaussian by the central limit theorem. This raises the issue of how a skewed distribution of firing rates might result from a symmetric distribution of inputs. We argue that skewed rate distributions are a signature of the nonlinearity of the in vivo f\textendash{}I curve. During in vivo conditions, ongoing synaptic activity produces significant fluctuations in the membrane potential of neurons, resulting in an expansive nonlinearity of the f\textendash{}I curve for low and moderate inputs. Here, we investigate the effects of single-cell and network parameters on the shape of the f\textendash{}I curve and, by extension, on the distribution of firing rates in randomly connected networks.},
  number = {45},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2016-02-25},
  date = {2011-09-11},
  pages = {16217--16226},
  author = {Roxin, Alex and Brunel, Nicolas and Hansel, David and Mongillo, Gianluigi and van Vreeswijk, Carl},
  file = {/home/fh/lib/articles/Roxin2011_2.pdf},
  eprinttype = {pmid},
  eprint = {22072673}
}

@article{Beggs2012,
  title = {Being {{Critical}} of {{Criticality}} in the {{Brain}}},
  volume = {3},
  issn = {1664-042X},
  url = {http://journal.frontiersin.org/article/10.3389/fphys.2012.00163/abstract},
  doi = {10.3389/fphys.2012.00163},
  journaltitle = {Frontiers in Physiology},
  urldate = {2016-01-19},
  date = {2012},
  author = {Beggs, John M. and Timme, Nicholas},
  file = {/home/fh/lib/articles/Beggs2012.pdf}
}

@article{Klaus2011,
  title = {Statistical {{Analyses Support Power Law Distributions Found}} in {{Neuronal Avalanches}}},
  volume = {6},
  url = {http://dx.doi.org/10.1371/journal.pone.0019779},
  doi = {10.1371/journal.pone.0019779},
  abstract = {The size distribution of neuronal avalanches in cortical networks has been reported to follow a power law distribution with exponent close to -1.5, which is a reflection of long-range spatial correlations in spontaneous neuronal activity. However, identifying power law scaling in empirical data can be difficult and sometimes controversial. In the present study, we tested the power law hypothesis for neuronal avalanches by using more stringent statistical analyses. In particular, we performed the following steps: (i) analysis of finite-size scaling to identify scale-free dynamics in neuronal avalanches, (ii) model parameter estimation to determine the specific exponent of the power law, and (iii) comparison of the power law to alternative model distributions. Consistent with critical state dynamics, avalanche size distributions exhibited robust scaling behavior in which the maximum avalanche size was limited only by the spatial extent of sampling (``finite size'' effect). This scale-free dynamics suggests the power law as a model for the distribution of avalanche sizes. Using both the Kolmogorov-Smirnov statistic and a maximum likelihood approach, we found the slope to be close to -1.5, which is in line with previous reports. Finally, the power law model for neuronal avalanches was compared to the exponential and to various heavy-tail distributions based on the Kolmogorov-Smirnov distance and by using a log-likelihood ratio test. Both the power law distribution without and with exponential cut-off provided significantly better fits to the cluster size distributions in neuronal avalanches than the exponential, the lognormal and the gamma distribution. In summary, our findings strongly support the power law scaling in neuronal avalanches, providing further evidence for critical state dynamics in superficial layers of cortex.},
  number = {5},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS ONE},
  urldate = {2016-01-19},
  date = {2011-05-26},
  pages = {e19779},
  author = {Klaus, Andreas and Yu, Shan and Plenz, Dietmar},
  file = {/home/fh/lib/articles/Klaus2011.pdf}
}

@article{Mazzoni2007,
  title = {On the {{Dynamics}} of the {{Spontaneous Activity}} in {{Neuronal Networks}}},
  volume = {2},
  url = {http://dx.plos.org/10.1371/journal.pone.0000439},
  doi = {10.1371/journal.pone.0000439},
  abstract = {Most neuronal networks, even in the absence of external stimuli, produce spontaneous bursts of spikes separated by periods of reduced activity. The origin and functional role of these neuronal events are still unclear. The present work shows that the spontaneous activity of two very different networks, intact leech ganglia and dissociated cultures of rat hippocampal neurons, share several features. Indeed, in both networks: i) the inter-spike intervals distribution of the spontaneous firing of single neurons is either regular or periodic or bursting, with the fraction of bursting neurons depending on the network activity; ii) bursts of spontaneous spikes have the same broad distributions of size and duration; iii) the degree of correlated activity increases with the bin width, and the power spectrum of the network firing rate has a 1/f behavior at low frequencies, indicating the existence of long-range temporal correlations; iv) the activity of excitatory synaptic pathways mediated by NMDA receptors is necessary for the onset of the long-range correlations and for the presence of large bursts; v) blockage of inhibitory synaptic pathways mediated by GABAA receptors causes instead an increase in the correlation among neurons and leads to a burst distribution composed only of very small and very large bursts. These results suggest that the spontaneous electrical activity in neuronal networks with different architectures and functions can have very similar properties and common dynamics.},
  number = {5},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS ONE},
  urldate = {2016-01-19},
  date = {2007-05-09},
  pages = {e439},
  author = {Mazzoni, Alberto and Broccard, Fr{\'e}d{\'e}ric D. and Garcia-Perez, Elizabeth and Bonifazi, Paolo and Ruaro, Maria Elisabetta and Torre, Vincent},
  file = {/home/fh/lib/articles/Mazzoni2007.pdf}
}

@article{Mukai2015,
  langid = {english},
  title = {Molecular {{Substrates}} of {{Altered Axonal Growth}} and {{Brain Connectivity}} in a {{Mouse Model}} of {{Schizophrenia}}},
  volume = {86},
  issn = {08966273},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627315003323},
  doi = {10.1016/j.neuron.2015.04.003},
  number = {3},
  journaltitle = {Neuron},
  urldate = {2016-01-28},
  date = {2015-05},
  pages = {680--695},
  author = {Mukai, Jun and Tamura, Makoto and F{\'e}nelon, Karine and Rosen, Andrew M. and Spellman, Timothy J. and Kang, Rujun and MacDermott, Amy B. and Karayiorgou, Maria and Gordon, Joshua A. and Gogos, Joseph A.},
  file = {/home/fh/lib/articles/Mukai2015.pdf}
}

@article{Triesch2007,
  title = {Synergies {{Between Intrinsic}} and {{Synaptic Plasticity Mechanisms}}},
  volume = {19},
  issn = {0899-7667},
  url = {http://dx.doi.org/10.1162/neco.2007.19.4.885},
  doi = {10.1162/neco.2007.19.4.885},
  abstract = {We propose a model of intrinsic plasticity for a continuous activation model neuron based on information theory. We then show how intrinsic and synaptic plasticity mechanisms interact and allow the neuron to discover heavy-tailed directions in the input. We also demonstrate that intrinsic plasticity may be an alternative explanation for the sliding threshold postulated in the BCM theory of synaptic plasticity. We present a theoretical analysis of the interaction of intrinsic plasticity with different Hebbian learning rules for the case of clustered inputs. Finally, we perform experiments on the ``bars'' problem, a popular nonlinear independent component analysis problem.},
  number = {4},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2016-01-19},
  date = {2007-03-09},
  pages = {885--909},
  author = {Triesch, Jochen},
  file = {/home/fh/lib/articles/Triesch2007.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/HXDKH37S/neco.2007.19.4.html}
}

@article{Pallotto2015,
  langid = {english},
  title = {Extracellular Space Preservation Aids the Connectomic Analysis of Neural Circuits},
  issn = {2050-084X},
  url = {http://elifesciences.org/content/early/2015/12/24/eLife.08206},
  doi = {10.7554/eLife.08206},
  abstract = {Dense connectomic mapping of neuronal circuits is limited by the time and effort required to analyze 3D electron microscopy (EM) datasets. Algorithms designed to automate image segmentation suffer from substantial error rates and require significant manual error correction. Any improvement in segmentation error rates would therefore directly reduce the time required to analyze 3D EM data. We explored preserving extracellular space (ECS) during chemical tissue fixation to improve the ability to segment neurites and to identify synaptic contacts. ECS preserved tissue is easier to segment using machine learning algorithms, leading to significantly reduced error rates. In addition, we observed that electrical synapses are readily identified in ECS preserved tissue. Finally, we determined that antibodies penetrate deep into ECS preserved tissue with only minimal permeabilization, thereby enabling correlated light microscopy (LM) and EM studies. We conclude that preservation of ECS benefits multiple aspects of the connectomic analysis of neural circuits.To Top
Dense connectomic mapping of neuronal circuits is limited by the time and effort required to analyze 3D electron microscopy (EM) datasets. Algorithms designed to automate image segmentation suffer from substantial error rates and require significant manual error correction. Any improvement in segmentation error rates would therefore directly reduce the time required to analyze 3D EM data. We explored preserving extracellular space (ECS) during chemical tissue fixation to improve the ability to segment neurites and to identify synaptic contacts. ECS preserved tissue is easier to segment using machine learning algorithms, leading to significantly reduced error rates. In addition, we observed that electrical synapses are readily identified in ECS preserved tissue. Finally, we determined that antibodies penetrate deep into ECS preserved tissue with only minimal permeabilization, thereby enabling correlated light microscopy (LM) and EM studies. We conclude that preservation of ECS benefits multiple aspects of the connectomic analysis of neural circuits.},
  journaltitle = {eLife},
  shortjournal = {eLife Sciences},
  urldate = {2016-01-22},
  date = {2015-12-24},
  pages = {e08206},
  author = {Pallotto, Marta and Watkins, Paul V. and Fubara, Boma and Singer, Joshua H. and Briggman, Kevin L.},
  file = {/home/fh/lib/articles/Pallotto2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/DB6X59MS/eLife.html}
}

@article{vanElburg2010,
  langid = {english},
  title = {Impact of {{Dendritic Size}} and {{Dendritic Topology}} on {{Burst Firing}} in {{Pyramidal Cells}}},
  volume = {6},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1000781},
  doi = {10.1371/journal.pcbi.1000781},
  number = {5},
  journaltitle = {PLoS Computational Biology},
  urldate = {2016-01-20},
  date = {2010-05-13},
  pages = {e1000781},
  author = {van Elburg, Ronald A. J. and van Ooyen, Arjen},
  editor = {Graham, Lyle J.},
  options = {useprefix=true},
  file = {/home/fh/lib/articles/van Elburg2010.pdf}
}

@article{Ferrante2013,
  langid = {english},
  title = {Functional {{Impact}} of {{Dendritic Branch}}-{{Point Morphology}}},
  volume = {33},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.3495-12.2013},
  doi = {10.1523/JNEUROSCI.3495-12.2013},
  number = {5},
  journaltitle = {Journal of Neuroscience},
  urldate = {2016-01-20},
  date = {2013-01-30},
  pages = {2156--2165},
  author = {Ferrante, M. and Migliore, M. and Ascoli, G. A.},
  file = {/home/fh/lib/articles/Ferrante2013.pdf}
}

@article{Briggman2011,
  langid = {english},
  title = {Wiring Specificity in the Direction-Selectivity Circuit of the Retina},
  volume = {471},
  issn = {0028-0836},
  url = {http://www.nature.com/nature/journal/v471/n7337/abs/nature09818.html},
  doi = {10.1038/nature09818},
  abstract = {The proper connectivity between neurons is essential for the implementation of the algorithms used in neural computations, such as the detection of directed motion by the retina. The analysis of neuronal connectivity is possible with electron microscopy, but technological limitations have impeded the acquisition of high-resolution data on a large enough scale. Here we show, using serial block-face electron microscopy and two-photon calcium imaging, that the dendrites of mouse starburst amacrine cells make highly specific synapses with direction-selective ganglion cells depending on the ganglion cell's preferred direction. Our findings indicate that a structural (wiring) asymmetry contributes to the computation of direction selectivity. The nature of this asymmetry supports some models of direction selectivity and rules out others. It also puts constraints on the developmental mechanisms behind the formation of synaptic connections. Our study demonstrates how otherwise intractable neurobiological questions can be addressed by combining functional imaging with the analysis of neuronal connectivity using large-scale electron microscopy.
View full text},
  number = {7337},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2016-01-22},
  date = {2011-03-10},
  pages = {183--188},
  keywords = {Neuroscience},
  author = {Briggman, Kevin L. and Helmstaedter, Moritz and Denk, Winfried},
  file = {/home/fh/lib/articles/Briggman2011.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/QJ2KW5S4/nature09818.html}
}

@article{Sigurdsson2015,
  langid = {english},
  title = {Neural Circuit Dysfunction in Schizophrenia: {{Insights}} from Animal Models},
  issn = {03064522},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0306452215006004},
  doi = {10.1016/j.neuroscience.2015.06.059},
  shorttitle = {Neural Circuit Dysfunction in Schizophrenia},
  journaltitle = {Neuroscience},
  urldate = {2016-01-28},
  date = {2015-07},
  author = {Sigurdsson, T.},
  file = {/home/fh/lib/articles/Sigurdsson2015.pdf}
}

@book{Seung2013,
  location = {{Boston}},
  title = {Connectome: How the Brain's Wiring Makes Us Who We Are},
  edition = {First Mariner Books edition},
  isbn = {978-0-547-67859-7},
  shorttitle = {Connectome},
  pagetotal = {359},
  publisher = {{Mariner Books, Houghton Mifflin Harcourt}},
  date = {2013},
  author = {Seung, Sebastian},
  file = {/home/fh/lib/books/Seung2013_Connectome-how-the-brain's-wiring-makes-us-who-we-are.epub}
}

@article{Nestler2010,
  title = {Animal Models of Neuropsychiatric Disorders},
  volume = {13},
  issn = {1097-6256, 1546-1726},
  url = {http://www.nature.com/doifinder/10.1038/nn.2647},
  doi = {10.1038/nn.2647},
  number = {10},
  journaltitle = {Nature Neuroscience},
  urldate = {2016-01-28},
  date = {2010-10},
  pages = {1161--1169},
  author = {Nestler, Eric J and Hyman, Steven E},
  file = {/home/fh/lib/articles/Nestler2010.pdf}
}

@book{Amthor2012,
  langid = {english},
  location = {{Mississauga}},
  title = {Neuroscience for Dummies: [Making Everything Easier!]},
  isbn = {978-1-118-08968-2 978-1-118-08686-5 978-1-118-08967-5},
  shorttitle = {Neuroscience for Dummies},
  abstract = {Neuroscience for Dummies gives the reader an understanding of the brain's structure and function, as well as a look into the relationship between memory, learning, emotions, and the brain. Providing insight into the biology of mental illness and a glimpse at future treatments and applications of neuroscience--},
  pagetotal = {366},
  series = {for dummies},
  publisher = {{Wiley}},
  date = {2012},
  keywords = {Neurosciences,Brain,Neurowissenschaften,Neurosciences--Popular works},
  author = {Amthor, Frank},
  file = {/home/fh/lib/books/Amthor2012_Neuroscience-for-dummies-[making-everything-easier!]2.pdf}
}

@report{zotero-null-750,
  title = {Pro {{Git}}},
  file = {/home/fh/lib/manuals/git/pro_git.pdf},
  note = {manuals/git}
}

@article{Yang2013,
  title = {Presynaptic Long-Term Plasticity},
  volume = {5},
  issn = {1663-3563},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3797957/},
  doi = {10.3389/fnsyn.2013.00008},
  abstract = {Long-term synaptic plasticity is a major cellular substrate for learning, memory, and behavioral adaptation. Although early examples of long-term synaptic plasticity described a mechanism by which postsynaptic signal transduction was potentiated, it is now apparent that there is a vast array of mechanisms for long-term synaptic plasticity that involve modifications to either or both the presynaptic terminal and postsynaptic site. In this article, we discuss current and evolving approaches to identify presynaptic mechanisms as well as discuss their limitations. We next provide examples of the diverse circuits in which presynaptic forms of long-term synaptic plasticity have been described and discuss the potential contribution this form of plasticity might add to circuit function. Finally, we examine the present evidence for the molecular pathways and cellular events underlying presynaptic long-term synaptic plasticity.},
  journaltitle = {Frontiers in Synaptic Neuroscience},
  shortjournal = {Front Synaptic Neurosci},
  urldate = {2016-03-19},
  date = {2013-10-17},
  author = {Yang, Ying and Calakos, Nicole},
  file = {/home/fh/lib/articles/Yang2013.pdf},
  eprinttype = {pmid},
  eprint = {24146648},
  pmcid = {PMC3797957}
}

@article{Miner2016,
  langid = {english},
  title = {Plasticity-{{Driven Self}}-{{Organization}} under {{Topological Constraints Accounts}} for {{Non}}-Random {{Features}} of {{Cortical Synaptic Wiring}}},
  volume = {12},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1004759},
  doi = {10.1371/journal.pcbi.1004759},
  number = {2},
  journaltitle = {PLOS Computational Biology},
  urldate = {2016-02-17},
  date = {2016-02-11},
  pages = {e1004759},
  keywords = {structural_plasticity},
  author = {Miner, Daniel and Triesch, Jochen},
  editor = {Sporns, Olaf},
  file = {/home/fh/lib/articles/Miner2016.pdf}
}

@article{Effenberger2015,
  langid = {english},
  title = {Self-Organization in {{Balanced State Networks}} by {{STDP}} and {{Homeostatic Plasticity}}},
  volume = {11},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1004420},
  doi = {10.1371/journal.pcbi.1004420},
  number = {9},
  journaltitle = {PLOS Computational Biology},
  urldate = {2016-02-24},
  date = {2015-09-03},
  pages = {e1004420},
  keywords = {_tablet},
  author = {Effenberger, Felix and Jost, J{\"u}rgen and Levina, Anna},
  editor = {Morrison, Abigail},
  file = {/home/fh/lib/articles/Effenberger2015.PDF}
}

@article{Ostojic2014,
  langid = {english},
  title = {Two Types of Asynchronous Activity in Networks of Excitatory and Inhibitory Spiking Neurons},
  volume = {17},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v17/n4/full/nn.3658.html},
  doi = {10.1038/nn.3658},
  abstract = {Asynchronous activity in balanced networks of excitatory and inhibitory neurons is believed to constitute the primary medium for the propagation and transformation of information in the neocortex. Here we show that an unstructured, sparsely connected network of model spiking neurons can display two fundamentally different types of asynchronous activity that imply vastly different computational properties. For weak synaptic couplings, the network at rest is in the well-studied asynchronous state, in which individual neurons fire irregularly at constant rates. In this state, an external input leads to a highly redundant response of different neurons that favors information transmission but hinders more complex computations. For strong couplings, we find that the network at rest displays rich internal dynamics, in which the firing rates of individual neurons fluctuate strongly in time and across neurons. In this regime, the internal dynamics interact with incoming stimuli to provide a substrate for complex information processing and learning.},
  number = {4},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2016-02-25},
  date = {2014-04},
  pages = {594--600},
  keywords = {Network models},
  author = {Ostojic, Srdjan},
  file = {/home/fh/lib/articles/Ostojic2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/6P6TSIAX/nn.3658.html}
}

@article{Morrison2007,
  title = {Spike-{{Timing}}-{{Dependent Plasticity}} in {{Balanced Random Networks}}},
  volume = {19},
  issn = {0899-7667},
  url = {http://dx.doi.org/10.1162/neco.2007.19.6.1437},
  doi = {10.1162/neco.2007.19.6.1437},
  abstract = {The balanced random network model attracts considerable interest because it explains the irregular spiking activity at low rates and large membrane potential fluctuations exhibited by cortical neurons in vivo. In this article, we investigate to what extent this model is also compatible with the experimentally observed phenomenon of spike-timing-dependent plasticity (STDP).},
  number = {6},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2016-02-25},
  date = {2007-04-19},
  pages = {1437--1467},
  author = {Morrison, Abigail and Aertsen, Ad and Diesmann, Markus},
  file = {/home/fh/lib/articles/Morrison2007.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/AN5ZM2ER/neco.2007.19.6.html}
}

@article{Feldman2009,
  title = {Synaptic {{Mechanisms}} for {{Plasticity}} in {{Neocortex}}},
  volume = {32},
  issn = {0147-006X},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3071739/},
  doi = {10.1146/annurev.neuro.051508.135516},
  abstract = {Sensory experience and learning alter sensory representations in cerebral cortex. The synaptic mechanisms underlying sensory cortical plasticity have long been sought. Recent work indicates that long-term cortical plasticity is a complex, multicomponent process involving multiple synaptic and cellular mechanisms. Sensory use, disuse, and training drive long-term potentiation and depression (LTP and LTD), homeostatic synaptic plasticity and plasticity of intrinsic excitability, and structural changes including formation, removal, and morphological remodeling of cortical synapses and dendritic spines. Both excitatory and inhibitory circuits are strongly regulated by experience. This review summarizes these findings and proposes that these mechanisms map onto specific functional components of plasticity, which occur in common across the primary somatosensory, visual, and auditory cortices.},
  journaltitle = {Annual review of neuroscience},
  shortjournal = {Annu Rev Neurosci},
  urldate = {2016-03-08},
  date = {2009},
  pages = {33--55},
  author = {Feldman, Daniel E.},
  file = {/home/fh/lib/articles/Feldman2009.pdf},
  eprinttype = {pmid},
  eprint = {19400721},
  pmcid = {PMC3071739}
}

@article{Duarte2014,
  title = {Dynamic Stability of Sequential Stimulus Representations in Adapting Neuronal Networks},
  volume = {8},
  issn = {1662-5188},
  url = {http://journal.frontiersin.org/article/10.3389/fncom.2014.00124/abstract},
  doi = {10.3389/fncom.2014.00124},
  journaltitle = {Frontiers in Computational Neuroscience},
  urldate = {2016-02-25},
  date = {2014-10-22},
  author = {Duarte, Renato C. F. and Morrison, Abigail},
  file = {/home/fh/lib/articles/Duarte2014.pdf}
}

@article{Noguchi2011,
  langid = {english},
  title = {In Vivo Two-Photon Uncaging of Glutamate Revealing the Structure\textendash{}function Relationships of Dendritic Spines in the Neocortex of Adult Mice},
  volume = {589},
  issn = {1469-7793},
  url = {http://onlinelibrary.wiley.com/doi/10.1113/jphysiol.2011.207100/abstract},
  doi = {10.1113/jphysiol.2011.207100},
  abstract = {Non-technical summary\hspace{0.6em} Neurons communicate with each other with synapses using chemical messengers. The major synapses in the cerebral cortex utilize glutamate as a messenger and are made on special submicron structures, called dendritic spines. Dendritic spines are diverse in their size and densely packed in the cortex. Therefore, an optical technique for application of glutamate to single spines (two-photon (TP) uncaging) has been intensively used to clarify their functions in vitro. We have here extended 2P uncaging to living adult brain, and found that spine sizes display tight correlations with their functions, such as rapid glutamate sensing and an increase in cytosolic Ca2+ concentrations, even in vivo, as they were reported for in vitro preparations. Our data suggest that the structure and motility of dendritic spines play a key role in the adult brain function.},
  number = {10},
  journaltitle = {The Journal of Physiology},
  urldate = {2016-03-09},
  date = {2011-05-15},
  pages = {2447--2457},
  author = {Noguchi, Jun and Nagaoka, Akira and Watanabe, Satoshi and Ellis-Davies, Graham C. R. and Kitamura, Kazuo and Kano, Masanobu and Matsuzaki, Masanori and Kasai, Haruo},
  file = {/home/fh/lib/articles/Noguchi2011.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/WF425SE2/abstract.html}
}

@article{Kim2007,
  langid = {english},
  title = {Ubiquitous {{Plasticity}} and {{Memory Storage}}},
  volume = {56},
  issn = {0896-6273},
  url = {http://www.cell.com/article/S0896627307008264/abstract},
  doi = {10.1016/j.neuron.2007.10.030},
  abstract = {To date, most hypotheses of memory storage in the mammalian brain have focused upon long-term synaptic potentiation and depression (LTP and LTD) of fast glutamatergic excitatory postsynaptic currents (EPSCs). In recent years, it has become clear that many additional electrophysiological components of neurons, from electrical synapses to glutamate transporters to voltage-sensitive ion channels, can also undergo use-dependent long-term plasticity. Models of memory storage that incorporate this full range of demonstrated electrophysiological plasticity are better able to account for both the storage of memory in neuronal networks and the complexities of memory storage, indexing, and recall as measured behaviorally.},
  number = {4},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2016-03-08},
  date = {2007-11-21},
  pages = {582--592},
  author = {Kim, Sang Jeong and Linden, David J.},
  file = {/home/fh/lib/articles/Kim2007.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/27N4IGEE/S0896-6273(07)00826-4.html},
  eprinttype = {pmid},
  eprint = {18031678}
}

@book{Schleif1993,
  location = {{Baltimore}},
  title = {Genetics and Molecular Biology},
  edition = {2nd ed},
  isbn = {978-0-8018-4673-1 978-0-8018-4674-8},
  pagetotal = {698},
  publisher = {{Johns Hopkins University Press}},
  date = {1993},
  keywords = {Molecular genetics},
  author = {Schleif, Robert F.},
  file = {/home/fh/lib/books/Schleif1993_Genetics-and-molecular-biology.pdf}
}

@article{Softky1993,
  langid = {english},
  title = {The Highly Irregular Firing of Cortical Cells Is Inconsistent with Temporal Integration of Random {{EPSPs}}},
  volume = {13},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/13/1/334},
  number = {1},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2016-03-14},
  date = {1993-01-01},
  pages = {334--350},
  author = {Softky, W. R. and Koch, C.},
  file = {/home/fh/lib/articles/Softky1993.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/KW2DQJ7Z/334.html},
  eprinttype = {pmid},
  eprint = {8423479}
}

@article{Zheng2013,
  title = {Network {{Self}}-{{Organization Explains}} the {{Statistics}} and {{Dynamics}} of {{Synaptic Connection Strengths}} in {{Cortex}}},
  volume = {9},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002848},
  doi = {10.1371/journal.pcbi.1002848},
  abstract = {Author Summary  The computations that brain circuits can perform depend on their wiring. While a wiring diagram is still out of reach for major brain structures such as the neocortex and hippocampus, data on the overall distribution of synaptic connection strengths and the temporal fluctuations of individual synapses have recently become available. Specifically, there exists a small population of very strong and stable synaptic connections, which may form the physiological substrate of life-long memories. This population coexists with a big and ever changing population of much smaller and strongly fluctuating synaptic connections. So far it has remained unclear how these properties of networks in neocortex and hippocampus arise. Here we present a computational model that explains these fundamental properties of neural circuits as a consequence of network self-organization resulting from the combined action of different forms of neuronal plasticity. This self-organization is driven by a rich-get-richer effect induced by an associative synaptic learning mechanism which is kept in check by several homeostatic plasticity mechanisms stabilizing the network. The model highlights the role of self-organization in the formation of brain circuits and parsimoniously explains a range of recent findings about their fundamental properties.},
  number = {1},
  journaltitle = {PLOS Comput Biol},
  shortjournal = {PLOS Comput Biol},
  urldate = {2016-03-14},
  date = {2013-01-03},
  pages = {e1002848},
  keywords = {neural networks,Neurons,synaptic plasticity,Neuronal plasticity,Synapses,Hippocampus,Neural pathways,Homeostatic mechanisms},
  author = {Zheng, Pengsheng and Dimitrakakis, Christos and Triesch, Jochen},
  file = {/home/fh/lib/articles/Zheng2013_2.pdf;/home/fh/lib/articles/Zheng2013_3.pdf;/home/fh/lib/articles/Zheng2013.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/6DV2FIZD/article.html}
}

@article{Turrigiano1998,
  langid = {english},
  title = {Activity-Dependent Scaling of Quantal Amplitude in Neocortical Neurons},
  volume = {391},
  issn = {0028-0836},
  url = {http://www.nature.com/nature/journal/v391/n6670/full/391892a0.html},
  doi = {10.1038/36103},
  abstract = {Information is stored in neural circuits through long-lasting changes in synaptic strengths,. Most studies of information storage have focused on mechanisms such as long-term potentiation and depression (LTP and LTD), in which synaptic strengths change in a synapse-specific manner,. In contrast, little attention has been paid to mechanisms that regulate the total synaptic strength of a neuron. Here we describe a new form of synaptic plasticity that increases or decreases the strength of all of a neuron's synaptic inputs as a function of activity. Chronic blockade of cortical culture activity increased the amplitude of miniature excitatory postsynaptic currents (mEPSCs) without changing their kinetics. Conversely, blocking GABA (-aminutyric acid)-mediated inhibition initially raised firing rates, but over a 48-hour period mESPC amplitudes decreased and firing rates returned to close to control values. These changes were at least partly due to postsynaptic alterations in the response to glutamate, and apparently affected each synapse in proportion to its initial strength. Such 'synaptic scaling' may help to ensure that firing rates do not become saturated during developmental changes in the number and strength of synaptic inputs, as well as stabilizing synaptic strengths during Hebbian modification, and facilitating competition between synapses.},
  number = {6670},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2016-03-15},
  date = {1998-02-26},
  pages = {892--896},
  author = {Turrigiano, Gina G. and Leslie, Kenneth R. and Desai, Niraj S. and Rutherford, Lana C. and Nelson, Sacha B.},
  file = {/home/fh/lib/articles/Turrigiano1998.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/R2J24MZS/391892a0.html},
  note = {synaptic scaling}
}

@article{Kleberg2014,
  title = {Excitatory and Inhibitory {{STDP}} Jointly Tune Feedforward Neural Circuits to Selectively Propagate Correlated Spiking Activity},
  volume = {8},
  url = {http://journal.frontiersin.org/article/10.3389/fncom.2014.00053/abstract},
  doi = {10.3389/fncom.2014.00053},
  abstract = {Spike-timing-dependent plasticity (STDP) has been well established between excitatory neurons and several computational functions have been proposed in various neural systems. Despite some recent efforts, however, there is a significant lack of functional understanding of inhibitory STDP (iSTDP) and its interplay with excitatory STDP (eSTDP). Here, we demonstrate by analytical and numerical methods that iSTDP contributes crucially to the balance of excitatory and inhibitory weights for the selection of a specific signaling pathway among other pathways in a feedforward circuit. This pathway selection is based on the high sensitivity of STDP to correlations in spike times, which complements a recent proposal for the role of iSTDP in firing-rate based selection. Our model predicts that asymmetric anti-Hebbian iSTDP exceeds asymmetric Hebbian iSTDP for supporting pathway-specific balance, which we show is useful for propagating transient neuronal responses. Furthermore, we demonstrate how STDPs at excitatory\textendash{}excitatory, excitatory\textendash{}inhibitory, and inhibitory\textendash{}excitatory synapses cooperate to improve the pathway selection. We propose that iSTDP is crucial for shaping the network structure that achieves efficient processing of synchronous spikes.},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci},
  urldate = {2016-03-14},
  date = {2014},
  pages = {53},
  keywords = {Correlation,STDP,plasticity,spike-timing,inhibition,disynaptic,excitation–inhibition balance},
  author = {Kleberg, Florence I. and Fukai, Tomoki and Gilson, Matthieu},
  file = {/home/fh/lib/articles/Kleberg2014.pdf}
}

@article{Ocker2015,
  title = {Self-{{Organization}} of {{Microcircuits}} in {{Networks}} of {{Spiking Neurons}} with {{Plastic Synapses}}},
  volume = {11},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004458},
  doi = {10.1371/journal.pcbi.1004458},
  abstract = {Author Summary   The connectivity of mammalian brains exhibits structure at a wide variety of spatial scales, from the broad (which brain areas connect to which) to the extremely fine (where synapses form on the morphology of individual neurons). Recent experimental work in the neocortex has highlighted structure at the level of microcircuits: different patterns of connectivity between small groups of neurons are either more or less abundant than would be expected by chance. A central question in systems neuroscience is how this structure emerges. Attempts to answer this question are confounded by the mutual interaction of network structure and spiking activity. Synaptic connections influence spiking statistics, while individual synapses are highly plastic and become stronger or weaker depending on the activity of the pre- and postsynaptic neurons. We present a self-consistent theory for how activity-dependent synaptic plasticity leads to the emergence of neuronal microcircuits. We use this theory to show how the form of the plasticity rule can govern the promotion or suppression of different connectivity patterns. Our work provides a foundation for understanding how cortical circuits, and not just individual synapses, are malleable in response to inputs both external and internal to a network.},
  number = {8},
  journaltitle = {PLOS Comput Biol},
  shortjournal = {PLOS Comput Biol},
  urldate = {2016-03-15},
  date = {2015-08-20},
  pages = {e1004458},
  keywords = {neural networks,Neurons,synaptic plasticity,Action potentials,Network motifs,Neuronal plasticity,Covariance,Synapses},
  author = {Ocker, Gabriel Koch and Litwin-Kumar, Ashok and Doiron, Brent},
  file = {/home/fh/lib/articles/Ocker2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/PQTSXKI5/article.html}
}

@incollection{Clopath2015,
  langid = {english},
  title = {Long {{Term Plasticity}}, {{Biophysical Models}}},
  isbn = {978-1-4614-6674-1 978-1-4614-6675-8},
  url = {http://link.springer.com/referenceworkentry/10.1007/978-1-4614-6675-8_351},
  booktitle = {Encyclopedia of {{Computational Neuroscience}}},
  publisher = {{Springer New York}},
  urldate = {2016-03-16},
  date = {2015},
  pages = {1628--1640},
  keywords = {Neurosciences,Neurobiology,Computation by Abstract Devices},
  author = {Clopath, Claudia},
  editor = {Jaeger, Dieter and Jung, Ranu},
  file = {/home/fh/lib/book_sections/Clopath2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/D7QFNP75/978-1-4614-6675-8_351.html},
  doi = {10.1007/978-1-4614-6675-8_351}
}

@article{Caze2015,
  langid = {english},
  title = {Synaptic Clustering or Scattering? {{Insight}} from a Model of Synaptic Plasticity in Dendrites},
  url = {http://biorxiv.org/content/early/2015/10/16/029330},
  doi = {10.1101/029330},
  shorttitle = {Synaptic Clustering or Scattering?},
  abstract = {A large body of theoretical work has shown how dendrites can increase the computational capacity of the neuron. This work predicted that synapses active together should be close together in space, a phenomenon called synaptic clustering. Experimental evidence has shown that, in the absence of sensory stimulation, synapses nearby on the same dendrite tend to be active together more than expected by chance. Synaptic clustering, however, does not seem to be ubiquitous: other groups have reported that nearby synapses can respond to different features of a stimulus during sensory evoked activity. In other words, synapses that are active together during sensory evoked activity can be far apart in space, a phenomenon we term synaptic scattering. To unify these apparently inconsistent experimental results, we use a computational framework to study the formation of a synaptic architecture -- a set of synaptic weights -- displaying both synaptic clustering and scattering. We present three conditions under which a neuron can learn such synaptic architecture: (i) presynaptic inputs are organized into correlated groups of neurons; (ii) the postsynaptic neuron is compartmentalized in subunits representing dendrites; and (iii) the synaptic plasticity rule is local within a subunit. Importantly, we show that given the same synaptic architecture, synaptic clustering is expressed during spontaneous activity, i.e. in the absence of sensory evoked activity, whereas synaptic scattering is present under evoked activity. Interestingly, reduced dendritic morphology in our model leads to a pathological hyper-excitability, as observed for instance in Alzheimer's Disease. This work therefore unifies a seemingly contradictory set of experimental observations: we demonstrate that the same synaptic architecture can lead to synaptic clustering and scattering depending on the input structure.},
  journaltitle = {bioRxiv},
  urldate = {2016-03-16},
  date = {2015-10-16},
  pages = {029330},
  author = {Caze, Romain Daniel and Clopath, Claudia and Schultz, Simon R.},
  file = {/home/fh/lib/articles/Caze2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/5JV6KQCM/029330.html}
}

@article{Okun2009,
  langid = {english},
  title = {Balance of Excitation and Inhibition},
  volume = {4},
  issn = {1941-6016},
  url = {http://www.scholarpedia.org/article/Balance_of_excitation_and_inhibition},
  doi = {10.4249/scholarpedia.7467},
  number = {8},
  journaltitle = {Scholarpedia},
  urldate = {2016-03-20},
  date = {2009},
  pages = {7467},
  author = {Okun, Michael and Lampl, Ilan},
  file = {/home/fh/lib/articles/Okun2009.pdf}
}

@article{Gjorgjieva2014,
  title = {Intrinsic {{Neuronal Properties Switch}} the {{Mode}} of {{Information Transmission}} in {{Networks}}},
  volume = {10},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003962},
  doi = {10.1371/journal.pcbi.1003962},
  abstract = {Author Summary   Differences in ion channel composition endow different neuronal types with distinct computational properties. Understanding how these biophysical differences affect network-level computation is an important frontier. We focus on a set of biophysical properties, experimentally observed in developing cortical neurons, that allow these neurons to efficiently encode their inputs despite time-varying changes in the statistical context. Large-scale propagating waves are autonomously generated by the developing brain even before the onset of sensory experience. Using multi-layered feedforward networks, we examine how changes in intrinsic properties can lead to changes in the network's ability to represent and transmit information on multiple timescales. We demonstrate that measured changes in the computational properties of immature single neurons enable the propagation of slow-varying wave-like inputs. In contrast, neurons with more mature properties are more sensitive to fast fluctuations, which modulate the slow-varying information. While slow events are transmitted with high fidelity in initial network layers, noise degrades transmission in downstream network layers. Our results show how short-term adaptation and modulation of the neurons' input-output firing curves by background synaptic noise determine the ability of neural networks to transmit information on multiple timescales.},
  number = {12},
  journaltitle = {PLOS Comput Biol},
  shortjournal = {PLOS Comput Biol},
  urldate = {2016-03-17},
  date = {2014-12-04},
  pages = {e1003962},
  keywords = {neural networks,Neurons,Action potentials,Single neuron function,Wave propagation,Signaling networks,Membrane potential,Biophysics},
  author = {Gjorgjieva, Julijana and Mease, Rebecca A. and Moody, William J. and Fairhall, Adrienne L.},
  file = {/home/fh/lib/articles/Gjorgjieva2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/P4TMUFZM/article.html}
}

@article{Regehr2012,
  langid = {english},
  title = {Short-{{Term Presynaptic Plasticity}}},
  volume = {4},
  issn = {, 1943-0264},
  url = {http://cshperspectives.cshlp.org/content/4/7/a005702},
  doi = {10.1101/cshperspect.a005702},
  abstract = {Different types of synapses are specialized to interpret spike trains in their own way by virtue of the complement of short-term synaptic plasticity mechanisms they possess. Numerous types of short-term, use-dependent synaptic plasticity regulate neurotransmitter release. Short-term depression is prominent after a single conditioning stimulus and recovers in seconds. Sustained presynaptic activation can result in more profound depression that recovers more slowly. An enhancement of release known as facilitation is prominent after single conditioning stimuli and lasts for hundreds of milliseconds. Finally, tetanic activation can enhance synaptic strength for tens of seconds to minutes through processes known as augmentation and posttetantic potentiation. Progress in clarifying the properties, mechanisms, and functional roles of these forms of short-term plasticity is reviewed here.},
  number = {7},
  journaltitle = {Cold Spring Harbor Perspectives in Biology},
  shortjournal = {Cold Spring Harb Perspect Biol},
  urldate = {2016-03-18},
  date = {2012-01-07},
  pages = {a005702},
  author = {Regehr, Wade G.},
  file = {/home/fh/lib/articles/Regehr2012.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/FM3RNA9F/a005702.html},
  eprinttype = {pmid},
  eprint = {22751149}
}

@article{Hennig2013,
  title = {Theoretical Models of Synaptic Short Term Plasticity},
  volume = {7},
  url = {http://journal.frontiersin.org/article/10.3389/fncom.2013.00045/full},
  doi = {10.3389/fncom.2013.00045},
  abstract = {Short term plasticity is a highly abundant form of rapid, activity-dependent modulation of synaptic efficacy. A shared set of mechanisms can cause both depression and enhancement of the postsynaptic response at different synapses, with important consequences for information processing. Mathematical models have been extensively used to study the mechanisms and roles of short term plasticity. This review provides an overview of existing models and their biological basis, and of their main properties. Special attention will be given to slow processes such as calcium channel inactivation and the effect of activation of presynaptic autoreceptors.},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci},
  urldate = {2016-03-18},
  date = {2013},
  pages = {45},
  keywords = {Synaptic Transmission,short term plasticity,mathematical model,synaptic depression,synaptic facilitation},
  author = {Hennig, Matthias H.},
  file = {/home/fh/lib/articles/Hennig2013.pdf}
}

@article{Markram1998,
  langid = {english},
  title = {Differential Signaling via the Same Axon of Neocortical Pyramidal Neurons},
  volume = {95},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/95/9/5323},
  abstract = {The nature of information stemming from a single neuron and conveyed simultaneously to several hundred target neurons is not known. Triple and quadruple neuron recordings revealed that each synaptic connection established by neocortical pyramidal neurons is potentially unique. Specifically, synaptic connections onto the same morphological class differed in the numbers and dendritic locations of synaptic contacts, their absolute synaptic strengths, as well as their rates of synaptic depression and recovery from depression. The same axon of a pyramidal neuron innervating another pyramidal neuron and an interneuron mediated frequency-dependent depression and facilitation, respectively, during high frequency discharges of presynaptic action potentials, suggesting that the different natures of the target neurons underlie qualitative differences in synaptic properties. Facilitating-type synaptic connections established by three pyramidal neurons of the same class onto a single interneuron, were all qualitatively similar with a combination of facilitation and depression mechanisms. The time courses of facilitation and depression, however, differed for these convergent connections, suggesting that different pre-postsynaptic interactions underlie quantitative differences in synaptic properties. Mathematical analysis of the transfer functions of frequency-dependent synapses revealed supra-linear, linear, and sub-linear signaling regimes in which mixtures of presynaptic rates, integrals of rates, and derivatives of rates are transferred to targets depending on the precise values of the synaptic parameters and the history of presynaptic action potential activity. Heterogeneity of synaptic transfer functions therefore allows multiple synaptic representations of the same presynaptic action potential train and suggests that these synaptic representations are regulated in a complex manner. It is therefore proposed that differential signaling is a key mechanism in neocortical information processing, which can be regulated by selective synaptic modifications.},
  number = {9},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  urldate = {2016-03-18},
  date = {1998-04-28},
  pages = {5323--5328},
  author = {Markram, Henry and Wang, Yun and Tsodyks, Misha},
  file = {/home/fh/lib/articles/Markram1998.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/JZAV2BJ6/5323.html},
  eprinttype = {pmid},
  eprint = {9560274}
}

@article{Haas2011,
  langid = {english},
  title = {Activity-Dependent Long-Term Depression of Electrical Synapses},
  volume = {334},
  issn = {1095-9203},
  doi = {10.1126/science.1207502},
  abstract = {Use-dependent forms of synaptic plasticity have been extensively characterized at chemical synapses, but a relationship between natural activity and strength at electrical synapses remains elusive. The thalamic reticular nucleus (TRN), a brain area rich in gap-junctional (electrical) synapses, regulates cortical attention to the sensory surround and participates in shifts between arousal states; plasticity of electrical synapses may be a key mechanism underlying these processes. We observed long-term depression resulting from coordinated burst firing in pairs of coupled TRN neurons. Changes in gap-junctional communication were asymmetrical, indicating that regulation of connectivity depends on the direction of use. Modification of electrical synapses resulting from activity in coupled neurons is likely to be a widespread and powerful mechanism for dynamic reorganization of electrically coupled neuronal networks.},
  number = {6054},
  journaltitle = {Science (New York, N.Y.)},
  shortjournal = {Science},
  date = {2011-10-21},
  pages = {389--393},
  keywords = {Animals,In Vitro Techniques,Neurons,Rats,Rats; Sprague-Dawley,Nerve Net,Action potentials,Electrical Synapses,Intralaminar Thalamic Nuclei,Long-Term Synaptic Depression,Membrane Potentials,Patch-Clamp Techniques,Sodium,Tetrodotoxin},
  author = {Haas, Julie S. and Zavala, Baltazar and Landisman, Carole E.},
  file = {/home/fh/lib/articles/Haas2011.pdf},
  eprinttype = {pmid},
  eprint = {22021860}
}

@article{Gjorgjieva2011a,
  langid = {english},
  title = {A Triplet Spike-Timing-Dependent Plasticity Model Generalizes the {{Bienenstock}}-{{Cooper}}-{{Munro}} Rule to Higher-Order Spatiotemporal Correlations},
  volume = {108},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1105933108},
  doi = {10.1073/pnas.1105933108},
  number = {48},
  journaltitle = {Proceedings of the National Academy of Sciences},
  urldate = {2016-03-21},
  date = {2011-11-29},
  pages = {19383--19388},
  author = {Gjorgjieva, J. and Clopath, C. and Audet, J. and Pfister, J.-P.},
  file = {/home/fh/lib/articles/Gjorgjieva2011.pdf}
}

@article{Platschek2016,
  title = {A General Homeostatic Principle Following Lesion Induced Dendritic Remodeling},
  volume = {4},
  issn = {2051-5960},
  url = {http://dx.doi.org/10.1186/s40478-016-0285-8},
  doi = {10.1186/s40478-016-0285-8},
  abstract = {Neuronal death and subsequent denervation of target areas are hallmarks of many neurological disorders. Denervated neurons lose part of their dendritic tree, and are considered "atrophic", i.e. pathologically altered and damaged. The functional consequences of this phenomenon are poorly understood.},
  journaltitle = {Acta Neuropathologica Communications},
  shortjournal = {Acta Neuropathologica Communications},
  urldate = {2016-04-26},
  date = {2016},
  pages = {19},
  keywords = {Computer Simulation,Electrotonic analysis,Compartmental modeling,Morphological modeling,Voltage attenuation,Backpropagating action potential,Homeostatic plasticity,Granule cell},
  author = {Platschek, Steffen and Cuntz, Hermann and Vuksic, Mario and Deller, Thomas and Jedlicka, Peter},
  file = {/home/fh/lib/articles/Platschek2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/2J8NJJ2T/s40478-016-0285-8.html}
}

@article{Hu2013,
  title = {Motif Statistics and Spike Correlations in Neuronal Networks},
  volume = {2013},
  issn = {1742-5468},
  url = {http://stacks.iop.org/1742-5468/2013/i=03/a=P03012?key=crossref.745a7a0bacb5280671da5078bb3778f8},
  doi = {10.1088/1742-5468/2013/03/P03012},
  number = {03},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  urldate = {2016-04-27},
  date = {2013-03-12},
  pages = {P03012},
  author = {Hu, Yu and Trousdale, James and Josi{\'c}, Kre{\v s}imir and Shea-Brown, Eric},
  file = {/home/fh/lib/articles/Hu2013.pdf}
}

@book{McFarland2009,
  location = {{Beijing ; Sebastopol}},
  title = {{{CSS}}: The Missing Manual},
  edition = {2nd ed},
  isbn = {978-0-596-80244-8},
  shorttitle = {{{CSS}}},
  pagetotal = {538},
  series = {The missing manual},
  publisher = {{Pogue Press/O'Reilly}},
  date = {2009},
  keywords = {Cascading style sheets,Web site development,Computer graphics,Web sites,Design,Web publishing},
  author = {McFarland, David Sawyer},
  file = {/home/fh/lib/books/McFarland2009_CSS-the-missing-manual.pdf},
  note = {manuals/html\_css}
}

@article{Erdos1959,
  title = {On Random Graphs, {{I}}},
  volume = {6},
  url = {http://www.renyi.hu/\%0003p_erdos/Erdos.html#1959-11},
  journaltitle = {Publicationes Mathematicae (Debrecen)},
  date = {1959},
  pages = {290--297},
  keywords = {graphs,random},
  author = {Erd{\H o}s, P. and R{\'e}nyi, A.},
  file = {/home/fh/lib/articles/Erd˝os1959.pdf}
}

@article{Eyherabide2013,
  langid = {english},
  title = {When and {{Why Noise Correlations Are Important}} in {{Neural Decoding}}},
  volume = {33},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/33/45/17921},
  doi = {10.1523/JNEUROSCI.0357-13.2013},
  abstract = {Information may be encoded both in the individual activity of neurons and in the correlations between their activities. Understanding whether knowledge of noise correlations is required to decode all the encoded information is fundamental for constructing computational models, brain\textendash{}machine interfaces, and neuroprosthetics. If correlations can be ignored with tolerable losses of information, the readout of neural signals is simplified dramatically. To that end, previous studies have constructed decoders assuming that neurons fire independently and then derived bounds for the information that is lost. However, here we show that previous bounds were not tight and overestimated the importance of noise correlations. In this study, we quantify the exact loss of information induced by ignoring noise correlations and show why previous estimations were not tight. Further, by studying the elementary parts of the decoding process, we determine when and why information is lost on a single-response basis. We introduce the minimum decoding error to assess the distinctive role of noise correlations under natural conditions. We conclude that all of the encoded information can be decoded without knowledge of noise correlations in many more situations than previously thought.},
  number = {45},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2016-05-02},
  date = {2013-06-11},
  pages = {17921--17936},
  author = {Eyherabide, Hugo Gabriel and Samengo, In{\'e}s},
  file = {/home/fh/lib/articles/Eyherabide2013.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/78SWGSDP/17921.html},
  eprinttype = {pmid},
  eprint = {24198380}
}

@article{Klausberger2008,
  langid = {english},
  title = {Neuronal {{Diversity}} and {{Temporal Dynamics}}: {{The Unity}} of {{Hippocampal Circuit Operations}}},
  volume = {321},
  issn = {0036-8075, 1095-9203},
  url = {http://science.sciencemag.org/content/321/5885/53},
  doi = {10.1126/science.1149381},
  shorttitle = {Neuronal {{Diversity}} and {{Temporal Dynamics}}},
  abstract = {In the cerebral cortex, diverse types of neurons form intricate circuits and cooperate in time for the processing and storage of information. Recent advances reveal a spatiotemporal division of labor in cortical circuits, as exemplified in the CA1 hippocampal area. In particular, distinct GABAergic ($\gamma$-aminobutyric acid\textendash{}releasing) cell types subdivide the surface of pyramidal cells and act in discrete time windows, either on the same or on different subcellular compartments. They also interact with glutamatergic pyramidal cell inputs in a domain-specific manner and support synaptic temporal dynamics, network oscillations, selection of cell assemblies, and the implementation of brain states. The spatiotemporal specializations in cortical circuits reveal that cellular diversity and temporal dynamics coemerged during evolution, providing a basis for cognitive behavior.},
  number = {5885},
  journaltitle = {Science},
  urldate = {2016-05-03},
  date = {2008-07-04},
  pages = {53--57},
  author = {Klausberger, Thomas and Somogyi, Peter},
  file = {/home/fh/lib/articles/Klausberger2008.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/HQ6M4WI6/53.html},
  eprinttype = {pmid},
  eprint = {18599766}
}

@article{Petreanu2009,
  langid = {english},
  title = {The Subcellular Organization of Neocortical Excitatory Connections},
  volume = {457},
  issn = {0028-0836},
  url = {http://www.nature.com/nature/journal/v457/n7233/full/nature07709.html},
  doi = {10.1038/nature07709},
  abstract = {Understanding cortical circuits will require mapping the connections between specific populations of neurons, as well as determining the dendritic locations where the synapses occur. The dendrites of individual cortical neurons overlap with numerous types of local and long-range excitatory axons, but axodendritic overlap is not always a good predictor of actual connection strength. Here we developed an efficient channelrhodopsin-2 (ChR2)-assisted method to map the spatial distribution of synaptic inputs, defined by presynaptic ChR2 expression, within the dendritic arborizations of recorded neurons. We expressed ChR2 in two thalamic nuclei, the whisker motor cortex and local excitatory neurons and mapped their synapses with pyramidal neurons in layers 3, 5A and 5B (L3, L5A and L5B) in the mouse barrel cortex. Within the dendritic arborizations of L3 cells, individual inputs impinged onto distinct single domains. These domains were arrayed in an orderly, monotonic pattern along the apical axis: axons from more central origins targeted progressively higher regions of the apical dendrites. In L5 arborizations, different inputs targeted separate basal and apical domains. Input to L3 and L5 dendrites in L1 was related to whisker movement and position, suggesting that these signals have a role in controlling the gain of their target neurons. Our experiments reveal high specificity in the subcellular organization of excitatory circuits.},
  number = {7233},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2016-05-03},
  date = {2009-02-26},
  pages = {1142--1145},
  author = {Petreanu, Leopoldo and Mao, Tianyi and Sternson, Scott M. and Svoboda, Karel},
  file = {/home/fh/lib/articles/Petreanu2009.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/FEND9KR6/nature07709.html}
}

@article{Larkum2009,
  langid = {english},
  title = {Synaptic {{Integration}} in {{Tuft Dendrites}} of {{Layer}} 5 {{Pyramidal Neurons}}: {{A New Unifying Principle}}},
  volume = {325},
  issn = {0036-8075, 1095-9203},
  url = {http://science.sciencemag.org/content/325/5941/756},
  doi = {10.1126/science.1171958},
  shorttitle = {Synaptic {{Integration}} in {{Tuft Dendrites}} of {{Layer}} 5 {{Pyramidal Neurons}}},
  abstract = {Fine Dendrites Fire Differently
The pyramidal neuron is the basic computational unit in the brain cortex. Its distal tuft dendrite is heavily innervated by horizontal fibers coursing through cortical layer-I providing long-range corticocortical and thalamocortical associational input. Larkum et al. (p. 756) investigated whether the apical tuft dendrites of layer-5 neocortical pyramidal neurons, like basal dendrites, generate n-methyl-d-aspartate (NMDA) spikes using two-photon\textendash{}guided direct dendritic recording, glutamate uncaging, and modeling. NMDA spikes could be evoked in the distal tuft dendrites, while Ca2+ spikes could be triggered at the bifurcation points. Block of the hyperpolarization-activated current enhanced these NMDA spikes. Thus, the generation of NMDA spikes is a general principle of thin, basal, and tuft dendrites.
Tuft dendrites are the main target for feedback inputs innervating neocortical layer 5 pyramidal neurons, but their properties remain obscure. We report the existence of N-methyl-d-aspartate (NMDA) spikes in the fine distal tuft dendrites that otherwise did not support the initiation of calcium spikes. Both direct measurements and computer simulations showed that NMDA spikes are the dominant mechanism by which distal synaptic input leads to firing of the neuron and provide the substrate for complex parallel processing of top-down input arriving at the tuft. These data lead to a new unifying view of integration in pyramidal neurons in which all fine dendrites, basal and tuft, integrate inputs locally through the recruitment of NMDA receptor channels relative to the fixed apical calcium and axosomatic sodium integration points.
Thin tuft and basal dendrites of pyramidal neurons use N-methyl-d-aspartate spikes to sum up synaptic inputs in semi-independent compartments.
Thin tuft and basal dendrites of pyramidal neurons use N-methyl-d-aspartate spikes to sum up synaptic inputs in semi-independent compartments.},
  number = {5941},
  journaltitle = {Science},
  urldate = {2016-05-04},
  date = {2009-08-07},
  pages = {756--760},
  author = {Larkum, Matthew E. and Nevian, Thomas and Sandler, Maya and Polsky, Alon and Schiller, Jackie},
  file = {/home/fh/lib/articles/Larkum2009.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/8QSB5674/756.html},
  eprinttype = {pmid},
  eprint = {19661433}
}

@article{Jensen1906,
  langid = {french},
  title = {Sur les fonctions convexes et les in{\'e}galit{\'e}s entre les valeurs moyennes},
  volume = {30},
  issn = {0001-5962, 1871-2509},
  url = {http://link.springer.com/article/10.1007/BF02418571},
  doi = {10.1007/BF02418571},
  number = {1},
  journaltitle = {Acta Mathematica},
  shortjournal = {Acta Math.},
  urldate = {2016-05-05},
  date = {1906-12},
  pages = {175--193},
  keywords = {Mathematics; general},
  author = {Jensen, J. L. W. V.},
  file = {/home/fh/lib/articles/Jensen1906.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/FK4A423H/10.html}
}

@article{Brunel2016,
  title = {Is Cortical Connectivity Optimized for Storing Information?},
  volume = {19},
  issn = {1097-6256, 1546-1726},
  url = {http://www.nature.com/doifinder/10.1038/nn.4286},
  doi = {10.1038/nn.4286},
  number = {5},
  journaltitle = {Nature Neuroscience},
  urldate = {2016-05-09},
  date = {2016-04-11},
  pages = {749--755},
  author = {Brunel, Nicolas},
  file = {/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/4BGJFC9Z/Brunel2016_S1.pdf;/home/fh/lib/articles/Brunel2016.pdf}
}

@article{Fino2011,
  langid = {english},
  title = {Dense {{Inhibitory Connectivity}} in {{Neocortex}}},
  volume = {69},
  issn = {0896-6273},
  url = {http://www.cell.com/article/S0896627311001231/abstract},
  doi = {10.1016/j.neuron.2011.02.025},
  abstract = {The connectivity diagram of neocortical circuits is still unknown, and there are conflicting data as to whether cortical neurons are wired specifically or not. To investigate the basic structure of cortical microcircuits, we use a two-photon photostimulation technique that enables the systematic mapping of synaptic connections with single-cell resolution. We map the inhibitory connectivity between upper layers somatostatin-positive GABAergic interneurons and pyramidal cells in mouse frontal cortex. Most, and sometimes all, inhibitory neurons are locally connected to every sampled pyramidal cell. This dense inhibitory connectivity is found at both young and mature developmental ages. Inhibitory innervation of neighboring pyramidal cells is similar, regardless of whether they are connected among themselves or not. We conclude that local inhibitory connectivity is promiscuous, does not form subnetworks, and can approach the theoretical limit of a completely connected synaptic matrix.},
  number = {6},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2016-05-09},
  date = {2011-03-24},
  pages = {1188--1203},
  author = {Fino, Elodie and Yuste, Rafael},
  file = {/home/fh/lib/articles/Fino2011_2.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/4AF7GN5F/S0896-6273(11)00123-1.html},
  eprinttype = {pmid},
  eprint = {21435562,\%002021435562}
}

@article{Hofer2011,
  langid = {english},
  title = {Differential Connectivity and Response Dynamics of Excitatory and Inhibitory Neurons in Visual Cortex},
  volume = {14},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v14/n8/full/nn.2876.html#access},
  doi = {10.1038/nn.2876},
  abstract = {Neuronal responses during sensory processing are influenced by both the organization of intracortical connections and the statistical features of sensory stimuli. How these intrinsic and extrinsic factors govern the activity of excitatory and inhibitory populations is unclear. Using two-photon calcium imaging in vivo and intracellular recordings in vitro, we investigated the dependencies between synaptic connectivity, feature selectivity and network activity in pyramidal cells and fast-spiking parvalbumin-expressing (PV) interneurons in mouse visual cortex. In pyramidal cell populations, patterns of neuronal correlations were largely stimulus-dependent, indicating that their responses were not strongly dominated by functionally biased recurrent connectivity. By contrast, visual stimulation only weakly modified co-activation patterns of fast-spiking PV cells, consistent with the observation that these broadly tuned interneurons received very dense and strong synaptic input from nearby pyramidal cells with diverse feature selectivities. Therefore, feedforward and recurrent network influences determine the activity of excitatory and inhibitory ensembles in fundamentally different ways.},
  number = {8},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2016-05-09},
  date = {2011-08},
  pages = {1045--1052},
  keywords = {Neuronal physiology,Visual system},
  author = {Hofer, Sonja B. and Ko, Ho and Pichler, Bruno and Vogelstein, Joshua and Ros, Hana and Zeng, Hongkui and Lein, Ed and Lesica, Nicholas A. and Mrsic-Flogel, Thomas D.},
  file = {/home/fh/lib/articles/Hofer2011.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/9VGBDD7G/nn.2876.html}
}

@article{Bienenstock1982,
  langid = {english},
  title = {Theory for the Development of Neuron Selectivity: Orientation Specificity and Binocular Interaction in Visual Cortex},
  volume = {2},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/2/1/32},
  shorttitle = {Theory for the Development of Neuron Selectivity},
  abstract = {The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework. A synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents compete. The change in the efficacy of a given synapse depends not only on instantaneous pre- and postsynaptic activities but also on a slowly varying time-averaged value of the postsynaptic activity. Assuming an appropriate nonlinear form for this dependence, development of selectivity is obtained under quite general conditions on the sensory environment. One does not require nonlinearity of the neuron's integrative power nor does one need to assume any particular form for intracortical circuitry. This is first illustrated in simple cases, e.g., when the environment consists of only two different stimuli presented alternately in a random manner. The following formal statement then holds: the state of the system converges with probability 1 to points of maximum selectivity in the state space. We next consider the problem of early development of orientation selectivity and binocular interaction in primary visual cortex. Giving the environment an appropriate form, we obtain orientation tuning curves and ocular dominance comparable to what is observed in normally reared adult cats or monkeys. Simulations with binocular input and various types of normal or altered environments show good agreement with the relevant experimental data. Experiments are suggested that could test our theory further.},
  number = {1},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2016-05-10},
  date = {1982-01-01},
  pages = {32--48},
  author = {Bienenstock, E. L. and Cooper, L. N. and Munro, P. W.},
  file = {/home/fh/lib/articles/Bienenstock1982.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/4U4QA72I/32.html},
  eprinttype = {pmid},
  eprint = {7054394}
}

@article{Bush2010,
  title = {Reconciling the {{STDP}} and {{BCM Models}} of {{Synaptic Plasticity}} in a {{Spiking Recurrent Neural Network}}},
  volume = {22},
  issn = {0899-7667},
  url = {http://dx.doi.org/10.1162/NECO_a_00003-Bush},
  doi = {10.1162/NECO_a_00003-Bush},
  abstract = {Rate-coded Hebbian learning, as characterized by the BCM formulation, is an established computational model of synaptic plasticity. Recently it has been demonstrated that changes in the strength of synapses in vivo can also depend explicitly on the relative timing of pre- and postsynaptic firing. Computational modeling of this spike-timing-dependent plasticity (STDP) has demonstrated that it can provide inherent stability or competition based on local synaptic variables. However, it has also been demonstrated that these properties rely on synaptic weights being either depressed or unchanged by an increase in mean stochastic firing rates, which directly contradicts empirical data. Several analytical studies have addressed this apparent dichotomy and identified conditions under which distinct and disparate STDP rules can be reconciled with rate-coded Hebbian learning. The aim of this research is to verify, unify, and expand on these previous findings by manipulating each element of a standard computational STDP model in turn. This allows us to identify the conditions under which this plasticity rule can replicate experimental data obtained using both rate and temporal stimulation protocols in a spiking recurrent neural network. Our results describe how the relative scale of mean synaptic weights and their dependence on stochastic pre- or postsynaptic firing rates can be manipulated by adjusting the exact profile of the asymmetric learning window and temporal restrictions on spike pair interactions respectively. These findings imply that previously disparate models of rate-coded autoassociative learning and temporally coded heteroassociative learning, mediated by symmetric and asymmetric connections respectively, can be implemented in a single network using a single plasticity rule. However, we also demonstrate that forms of STDP that can be reconciled with rate-coded Hebbian learning do not generate inherent synaptic competition, and thus some additional mechanism is required to guarantee long-term input-output selectivity.},
  number = {8},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2016-05-10},
  date = {2010-05-03},
  pages = {2059--2085},
  author = {Bush, Daniel and Philippides, Andrew and Husbands, Phil and O'Shea, Michael},
  file = {/home/fh/lib/articles/Bush2010.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/QX568NHZ/NECO_a_00003-Bush.html}
}

@article{Tannenbaum2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.03005},
  primaryClass = {q-bio},
  title = {Shaping Neural Circuits by High Order Synaptic Interactions},
  url = {http://arxiv.org/abs/1605.03005},
  abstract = {Spike timing dependent plasticity (STDP) is believed to play an important role in shaping the structure of neural circuits. Here we show that STDP generates effective interactions between synapses of different neurons, which were neglected in previous theoretical treatments, and can be described as a sum over contributions from structural motifs. These interactions can have a pivotal influence on the connectivity patterns that emerge under the influence of STDP. In particular, we consider two highly ordered forms of structure: wide synfire chains, in which groups of neurons project to each other sequentially, and self connected assemblies. We show that high order synaptic interactions can enable the formation of both structures, depending on the form of the STDP function and the time course of synaptic currents. Furthermore, within a certain regime of biophysical parameters, emergence of the ordered connectivity occurs robustly and autonomously in a stochastic network of spiking neurons, without a need to expose the neural network to structured inputs during learning.},
  urldate = {2016-05-17},
  date = {2016-05-10},
  keywords = {Quantitative Biology - Neurons and Cognition},
  author = {Tannenbaum, Neta Ravid and Burak, Yoram},
  file = {/home/fh/lib/articles/Tannenbaum2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/25TVUBIS/1605.html}
}

@book{Hogg1978,
  location = {{New York}},
  title = {Introduction to Mathematical Statistics},
  edition = {4th ed},
  isbn = {978-0-02-355710-1},
  pagetotal = {438},
  publisher = {{Macmillan}},
  date = {1978},
  keywords = {Mathematical statistics},
  author = {Hogg, Robert V. and Craig, Allen T.},
  file = {/home/fh/lib/books/Hogg1978_Introduction-to-mathematical-statistics.pdf}
}

@article{Kampa2006,
  langid = {english},
  title = {Cortical Feed-Forward Networks for Binding Different Streams of Sensory Information},
  volume = {9},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v9/n12/full/nn1798.html},
  doi = {10.1038/nn1798},
  abstract = {Different streams of sensory information are transmitted to the cortex where they are merged into a percept in a process often termed 'binding.' Using recordings from triplets of rat cortical layer 2/3 and layer 5 pyramidal neurons, we show that specific subnetworks within layer 5 receive input from different layer 2/3 subnetworks. This cortical microarchitecture may represent a mechanism that enables the main output of the cortex (layer 5) to bind different features of a sensory stimulus.},
  number = {12},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2016-05-23},
  date = {2006-12},
  pages = {1472--1473},
  author = {Kampa, Bj{\"o}rn M. and Letzkus, Johannes J. and Stuart, Greg J.},
  file = {/home/fh/lib/articles/Kampa2006.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/4D22ZBS3/nn1798.html}
}

@article{Hawkes1971,
  langid = {english},
  title = {Spectra of Some Self-Exciting and Mutually Exciting Point Processes},
  volume = {58},
  issn = {0006-3444, 1464-3510},
  url = {http://biomet.oxfordjournals.org/content/58/1/83},
  doi = {10.1093/biomet/58.1.83},
  abstract = {SUMMARY In recent years methods of data analysis for point processes have received some attention, for example, by Cox \& Lewis (1966) and Lewis (1964). In particular Bartlett (1963a, b) has introduced methods of analysis based on the point spectrum. Theoretical models are relatively sparse. In this paper the theoretical properties of a class of processes with particular reference to the point spectrum or corresponding covariance density functions are discussed. A particular result is a self-exciting process with the same second-order properties as a certain doubly stochastic process. These are not distinguishable by methods of data analysis based on these properties.},
  number = {1},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  urldate = {2016-05-30},
  date = {1971-01-04},
  pages = {83--90},
  keywords = {Point process,Self-exciting point process,Spectrum of point process,Covariance density},
  author = {Hawkes, Alan G.},
  file = {/home/fh/lib/articles/Hawkes1971.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/MCS8DJQM/83.html}
}

@article{Hawkes1971a,
  eprinttype = {jstor},
  eprint = {2984686},
  title = {Point {{Spectra}} of {{Some Mutually Exciting Point Processes}}},
  volume = {33},
  issn = {0035-9246},
  abstract = {The point spectral matrix is obtained for a class of mutually exciting point processes. The solution makes use of methods similar to those used in solving Wiener-Hopf integral equations.},
  number = {3},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
  shortjournal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  date = {1971},
  pages = {438--443},
  author = {Hawkes, Alan G.},
  file = {/home/fh/lib/articles/Hawkes1971_2.pdf}
}

@article{Kempter1999,
  title = {Hebbian Learning and Spiking Neurons},
  volume = {59},
  url = {http://link.aps.org/doi/10.1103/PhysRevE.59.4498},
  doi = {10.1103/PhysRevE.59.4498},
  abstract = {A correlation-based (``Hebbian'') learning rule at a spike level with millisecond resolution is formulated, mathematically analyzed, and compared with learning in a firing-rate description. The relative timing of presynaptic and postsynaptic spikes influences synaptic weights via an asymmetric ``learning window.'' A differential equation for the learning dynamics is derived under the assumption that the time scales of learning and neuronal spike dynamics can be separated. The differential equation is solved for a Poissonian neuron model with stochastic spike arrival. It is shown that correlations between input and output spikes tend to stabilize structure formation. With an appropriate choice of parameters, learning leads to an intrinsic normalization of the average weight and the output firing rate. Noise generates diffusion-like spreading of synaptic weights.},
  number = {4},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  urldate = {2016-05-30},
  date = {1999-04-01},
  pages = {4498--4514},
  author = {Kempter, Richard and Gerstner, Wulfram and van Hemmen, J. Leo},
  options = {useprefix=true},
  file = {/home/fh/lib/articles/Kempter1999.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/HEP9ASTP/PhysRevE.59.html}
}

@article{Jovanovic2015,
  title = {Cumulants of {{Hawkes}} Point Processes},
  volume = {91},
  url = {http://link.aps.org/doi/10.1103/PhysRevE.91.042802},
  doi = {10.1103/PhysRevE.91.042802},
  abstract = {We derive explicit, closed-form expressions for the cumulant densities of a multivariate, self-exciting Hawkes point process, generalizing a result of Hawkes in his earlier work on the covariance density and Bartlett spectrum of such processes. To do this, we represent the Hawkes process in terms of a Poisson cluster process and show how the cumulant density formulas can be derived by enumerating all possible ``family trees,'' representing complex interactions between point events. We also consider the problem of computing the integrated cumulants, characterizing the average measure of correlated activity between events of different types, and derive the relevant equations.},
  number = {4},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  urldate = {2016-05-30},
  date = {2015-04-07},
  pages = {042802},
  author = {Jovanovi{\'c}, Stojan and Hertz, John and Rotter, Stefan},
  file = {/home/fh/lib/articles/Jovanović2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/48C3H7JM/PhysRevE.91.html}
}

@article{Babadi2013,
  langid = {english},
  title = {Pairwise {{Analysis Can Account}} for {{Network Structures Arising}} from {{Spike}}-{{Timing Dependent Plasticity}}},
  volume = {9},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1002906},
  doi = {10.1371/journal.pcbi.1002906},
  number = {2},
  journaltitle = {PLoS Computational Biology},
  urldate = {2016-05-30},
  date = {2013-02-21},
  pages = {e1002906},
  author = {Babadi, Baktash and Abbott, L. F.},
  editor = {Gutkin, Boris S.},
  file = {/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/MPS3AIF6/Babadi2013_S1.pdf;/home/fh/lib/articles/Babadi2013.pdf}
}

@article{Gilson2009,
  langid = {english},
  title = {Emergence of Network Structure Due to Spike-Timing-Dependent Plasticity in Recurrent Neuronal Networks. {{I}}. {{Input}} Selectivity\textendash{}strengthening Correlated Input Pathways},
  volume = {101},
  issn = {0340-1200, 1432-0770},
  url = {http://link.springer.com/article/10.1007/s00422-009-0319-4},
  doi = {10.1007/s00422-009-0319-4},
  abstract = {Spike-timing-dependent plasticity (STDP) determines the evolution of the synaptic weights according to their pre- and post-synaptic activity, which in turn changes the neuronal activity. In this paper, we extend previous studies of input selectivity induced by (STDP) for single neurons to the biologically interesting case of a neuronal network with fixed recurrent connections and plastic connections from external pools of input neurons. We use a theoretical framework based on the Poisson neuron model to analytically describe the network dynamics (firing rates and spike-time correlations) and thus the evolution of the synaptic weights. This framework incorporates the time course of the post-synaptic potentials and synaptic delays. Our analysis focuses on the asymptotic states of a network stimulated by two homogeneous pools of ``steady'' inputs, namely Poisson spike trains which have fixed firing rates and spike-time correlations. The (STDP) model extends rate-based learning in that it can implement, at the same time, both a stabilization of the individual neuron firing rates and a slower weight specialization depending on the input spike-time correlations. When one input pathway has stronger within-pool correlations, the resulting synaptic dynamics induced by (STDP) are shown to be similar to those arising in the case of a purely feed-forward network: the weights from the more correlated inputs are potentiated at the expense of the remaining input connections.},
  number = {2},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol Cybern},
  urldate = {2016-05-30},
  date = {2009-06-18},
  pages = {81--102},
  keywords = {Neurosciences,learning,STDP,Complexity,Bioinformatics,Computer Appl. in Life Sciences,Neurobiology,Recurrent neuronal network,Spike-time correlation},
  author = {Gilson, Matthieu and Burkitt, Anthony N. and Grayden, David B. and Thomas, Doreen A. and van Hemmen, J. Leo},
  file = {/home/fh/lib/articles/Gilson2009.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/IKQAMS8M/10.html}
}

@article{Trousdale2012,
  title = {Impact of {{Network Structure}} and {{Cellular Response}} on {{Spike Time Correlations}}},
  volume = {8},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002408},
  doi = {10.1371/journal.pcbi.1002408},
  abstract = {Author Summary          Is neural activity more than the sum of its individual parts? What is the impact of cooperative, or  correlated , spiking among multiple cells? We can start addressing these questions, as rapid advances in experimental techniques allow simultaneous recordings from ever-increasing populations. However, we still lack a general understanding of the origin and consequences of the joint activity that is revealed. The challenge is compounded by the fact that both the intrinsic dynamics of single cells and the correlations among then vary depending on the overall state of the network. Here, we develop a toolbox that addresses this issue. Specifically, we show how  linear response theory  allows for the expression of correlations explicitly in terms of the underlying network connectivity and known single-cell properties \textendash{} and that the predictions of this theory accurately match simulations of a touchstone, nonlinear model in computational neuroscience, the general integrate-and-fire cell. Thus, our theory should help unlock the relationship between network architecture, single-cell dynamics, and correlated activity in diverse neural circuits.},
  number = {3},
  journaltitle = {PLOS Comput Biol},
  shortjournal = {PLOS Comput Biol},
  urldate = {2016-05-31},
  date = {2012-03-22},
  pages = {e1002408},
  keywords = {neural networks,Neurons,Action potentials,Synapses,Membrane potential,Convolution,Fourier analysis,White noise},
  author = {Trousdale, James and Hu, Yu and Shea-Brown, Eric and Josi{\'c}, Kre{\v s}imir},
  file = {/home/fh/lib/articles/Trousdale2012.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/AWS46G94/article.html}
}

@article{Burkitt2007,
  langid = {english},
  title = {Spike-Timing-Dependent Plasticity for Neurons with Recurrent Connections},
  volume = {96},
  issn = {0340-1200, 1432-0770},
  url = {http://link.springer.com/article/10.1007/s00422-007-0148-2},
  doi = {10.1007/s00422-007-0148-2},
  abstract = {The dynamics of the learning equation, which describes the evolution of the synaptic weights, is derived in the situation where the network contains recurrent connections. The derivation is carried out for the Poisson neuron model. The spiking-rates of the recurrently connected neurons and their cross-correlations are determined self- consistently as a function of the external synaptic inputs. The solution of the learning equation is illustrated by the analysis of the particular case in which there is no external synaptic input. The general learning equation and the fixed-point structure of its solutions is discussed.},
  number = {5},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol Cybern},
  urldate = {2016-06-03},
  date = {2007-04-06},
  pages = {533--546},
  keywords = {Neurosciences,Bioinformatics,Computer Appl. in Life Sciences,Neurobiology},
  author = {Burkitt, A. N. and Gilson, M. and van Hemmen, J. L.},
  file = {/home/fh/lib/articles/Burkitt2007.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/36XKZT22/10.html}
}

@article{Gilson2009a,
  langid = {english},
  title = {Emergence of Network Structure Due to Spike-Timing-Dependent Plasticity in Recurrent Neuronal Networks. {{II}}. {{Input}} Selectivity\textemdash{}symmetry Breaking},
  volume = {101},
  issn = {0340-1200, 1432-0770},
  url = {http://link.springer.com/article/10.1007/s00422-009-0320-y},
  doi = {10.1007/s00422-009-0320-y},
  abstract = {Spike-timing-dependent plasticity (STDP) is believed to structure neuronal networks by slowly changing the strengths (or weights) of the synaptic connections between neurons depending upon their spiking activity, which in turn modifies the neuronal firing dynamics. In this paper, we investigate the change in synaptic weights induced by STDP in a recurrently connected network in which the input weights are plastic but the recurrent weights are fixed. The inputs are divided into two pools with identical constant firing rates and equal within-pool spike-time correlations, but with no between-pool correlations. Our analysis uses the Poisson neuron model in order to predict the evolution of the input synaptic weights and focuses on the asymptotic weight distribution that emerges due to STDP. The learning dynamics induces a symmetry breaking for the individual neurons, namely for sufficiently strong within-pool spike-time correlation each neuron specializes to one of the input pools. We show that the presence of fixed excitatory recurrent connections between neurons induces a group symmetry-breaking effect, in which neurons tend to specialize to the same input pool. Consequently STDP generates a functional structure on the input connections of the network.},
  number = {2},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol Cybern},
  urldate = {2016-06-05},
  date = {2009-06-18},
  pages = {103--114},
  keywords = {Neurosciences,learning,STDP,Complexity,Bioinformatics,Computer Appl. in Life Sciences,Neurobiology,Recurrent neuronal network,Spike-time correlation,Symmetry breaking},
  author = {Gilson, Matthieu and Burkitt, Anthony N. and Grayden, David B. and Thomas, Doreen A. and van Hemmen, J. Leo},
  file = {/home/fh/lib/articles/Gilson2009_2.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/IM5DF9GS/s00422-009-0320-y.html}
}

@article{Gilson2009b,
  langid = {english},
  title = {Emergence of Network Structure Due to Spike-Timing-Dependent Plasticity in Recurrent Neuronal Networks {{III}}: {{Partially}} Connected Neurons Driven by Spontaneous Activity},
  volume = {101},
  issn = {0340-1200, 1432-0770},
  url = {http://link.springer.com/article/10.1007/s00422-009-0343-4},
  doi = {10.1007/s00422-009-0343-4},
  shorttitle = {Emergence of Network Structure Due to Spike-Timing-Dependent Plasticity in Recurrent Neuronal Networks {{III}}},
  abstract = {In contrast to a feed-forward architecture, the weight dynamics induced by spike-timing-dependent plasticity (STDP) in a recurrent neuronal network is not yet well understood. In this article, we extend a previous study of the impact of additive STDP in a recurrent network that is driven by spontaneous activity (no external stimulating inputs) from a fully connected network to one that is only partially connected. The asymptotic state of the network is analyzed, and it is found that the equilibrium and stability conditions for the firing rates are similar for both full and partial connectivity: STDP causes the firing rates to converge toward the same value and remain quasi-homogeneous. However, when STDP induces strong weight competition, the connectivity affects the weight dynamics in that the distribution of the weights disperses more quickly for lower density than for higher density. The asymptotic weight distribution strongly depends upon that at the beginning of the learning epoch; consequently, homogeneous connectivity alone is not sufficient to obtain homogeneous neuronal activity. In the absence of external inputs, STDP can nevertheless generate structure in the network through autocorrelation effects, for example, by introducing asymmetry in network topology.},
  number = {5-6},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol Cybern},
  urldate = {2016-06-05},
  date = {2009-11-24},
  pages = {411--426},
  keywords = {Neurosciences,learning,STDP,Bioinformatics,Computer Appl. in Life Sciences,Neurobiology,Recurrent neuronal network,Statistical Physics; Dynamical Systems and Complexity,Partial connectivity},
  author = {Gilson, Matthieu and Burkitt, Anthony N. and Grayden, David B. and Thomas, Doreen A. and van Hemmen, J. Leo},
  file = {/home/fh/lib/articles/Gilson2009_3.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/MDSWEWJT/s00422-009-0343-4.html}
}

@article{Gilson2009c,
  langid = {english},
  title = {Emergence of Network Structure Due to Spike-Timing-Dependent Plasticity in Recurrent Neuronal Networks {{IV}}},
  volume = {101},
  issn = {0340-1200, 1432-0770},
  url = {http://link.springer.com/article/10.1007/s00422-009-0346-1},
  doi = {10.1007/s00422-009-0346-1},
  abstract = {In neuronal networks, the changes of synaptic strength (or weight) performed by spike-timing-dependent plasticity (STDP) are hypothesized to give rise to functional network structure. This article investigates how this phenomenon occurs for the excitatory recurrent connections of a network with fixed input weights that is stimulated by external spike trains. We develop a theoretical framework based on the Poisson neuron model to analyze the interplay between the neuronal activity (firing rates and the spike-time correlations) and the learning dynamics, when the network is stimulated by correlated pools of homogeneous Poisson spike trains. STDP can lead to both a stabilization of all the neuron firing rates (homeostatic equilibrium) and a robust weight specialization. The pattern of specialization for the recurrent weights is determined by a relationship between the input firing-rate and correlation structures, the network topology, the STDP parameters and the synaptic response properties. We find conditions for feed-forward pathways or areas with strengthened self-feedback to emerge in an initially homogeneous recurrent network.},
  number = {5-6},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol Cybern},
  urldate = {2016-06-05},
  date = {2009-11-24},
  pages = {427--444},
  keywords = {Neurosciences,learning,STDP,Bioinformatics,Computer Appl. in Life Sciences,Neurobiology,Recurrent neuronal network,Spike-time correlation,Statistical Physics; Dynamical Systems and Complexity},
  author = {Gilson, Matthieu and Burkitt, Anthony N. and Grayden, David B. and Thomas, Doreen A. and van Hemmen, J. Leo},
  file = {/home/fh/lib/articles/Gilson2009_4.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/FQA9BMDT/s00422-009-0346-1.html}
}

@article{Gilson2010,
  langid = {english},
  title = {Emergence of Network Structure Due to Spike-Timing-Dependent Plasticity in Recurrent Neuronal Networks {{V}}: Self-Organization Schemes and Weight Dependence},
  volume = {103},
  issn = {0340-1200, 1432-0770},
  url = {http://link.springer.com/article/10.1007/s00422-010-0405-7},
  doi = {10.1007/s00422-010-0405-7},
  shorttitle = {Emergence of Network Structure Due to Spike-Timing-Dependent Plasticity in Recurrent Neuronal Networks {{V}}},
  abstract = {Spike-timing-dependent plasticity (STDP) determines the evolution of the synaptic weights according to their pre- and post-synaptic activity, which in turn changes the neuronal activity on a (much) slower time scale. This paper examines the effect of STDP in a recurrently connected network stimulated by external pools of input spike trains, where both input and recurrent synapses are plastic. Our previously developed theoretical framework is extended to incorporate weight-dependent STDP and dendritic delays. The weight dynamics is determined by an interplay between the neuronal activation mechanisms, the input spike-time correlations, and the learning parameters. For the case of two external input pools, the resulting learning scheme can exhibit a symmetry breaking of the input connections such that two neuronal groups emerge, each specialized to one input pool only. In addition, we show how the recurrent connections within each neuronal group can be strengthened by STDP at the expense of those between the two groups. This neuronal self-organization can be seen as a basic dynamical ingredient for the emergence of neuronal maps induced by activity-dependent plasticity.},
  number = {5},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol Cybern},
  urldate = {2016-06-05},
  date = {2010-09-29},
  pages = {365--386},
  keywords = {Neurosciences,learning,Bioinformatics,Computer Appl. in Life Sciences,Neurobiology,Recurrent neuronal network,Spike-time correlation,Statistical Physics; Dynamical Systems and Complexity,Weight-dependent STDP,Self-organization},
  author = {Gilson, Matthieu and Burkitt, Anthony N. and Grayden, David B. and Thomas, Doreen A. and van Hemmen, J. Leo},
  file = {/home/fh/lib/articles/Gilson2010.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/G5H2D8IF/s00422-010-0405-7.html}
}

@article{delaRocha2007,
  title = {Correlation between Neural Spike Trains Increases with Firing Rate},
  volume = {448},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/nature06028},
  doi = {10.1038/nature06028},
  number = {7155},
  journaltitle = {Nature},
  urldate = {2016-06-07},
  date = {2007-08-16},
  pages = {802--806},
  author = {de la Rocha, Jaime and Doiron, Brent and Shea-Brown, Eric and Josi{\'c}, Kre{\v s}imir and Reyes, Alex},
  options = {useprefix=true},
  file = {/home/fh/lib/articles/de la Rocha2007.pdf}
}

@article{Hong2012,
  langid = {english},
  title = {Single {{Neuron Firing Properties Impact Correlation}}-{{Based Population Coding}}},
  volume = {32},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.3735-11.2012},
  doi = {10.1523/JNEUROSCI.3735-11.2012},
  number = {4},
  journaltitle = {Journal of Neuroscience},
  urldate = {2016-06-07},
  date = {2012-01-25},
  pages = {1413--1428},
  author = {Hong, S. and Ratte, S. and Prescott, S. A. and De Schutter, E.},
  file = {/home/fh/lib/articles/Hong2012.pdf}
}

@book{Gabbiani2010,
  langid = {english},
  location = {{Amsterdam}},
  title = {Mathematics for Neuroscientists},
  edition = {1. ed},
  isbn = {978-0-12-374882-9},
  pagetotal = {486},
  publisher = {{Elsevier, Acad. Press}},
  date = {2010},
  keywords = {Neurosciences Mathematics},
  author = {Gabbiani, Fabrizio and Cox, Steven},
  file = {/home/fh/lib/books/Gabbiani2010_Mathematics-for-neuroscientists_2.pdf},
  note = {OCLC: 837352028}
}

@article{Uhlenbeck1930,
  title = {On the {{Theory}} of the {{Brownian Motion}}},
  volume = {36},
  url = {http://link.aps.org/doi/10.1103/PhysRev.36.823},
  doi = {10.1103/PhysRev.36.823},
  abstract = {With a method first indicated by Ornstein the mean values of all the powers of the velocity u and the displacement s of a free particle in Brownian motion are calculated. It is shown that u-u0exp(-$\beta$t) and s-u0$\beta$[1-exp(-$\beta$t)] where u0 is the initial velocity and $\beta$ the friction coefficient divided by the mass of the particle, follow the normal Gaussian distribution law. For s this gives the exact frequency distribution corresponding to the exact formula for s2 of Ornstein and F{\"u}rth. Discussion is given of the connection with the Fokker-Planck partial differential equation. By the same method exact expressions are obtained for the square of the deviation of a harmonically bound particle in Brownian motion as a function of the time and the initial deviation. Here the periodic, aperiodic and overdamped cases have to be treated separately. In the last case, when $\beta$ is much larger than the frequency and for values of t$\gg\beta$-1, the formula takes the form of that previously given by Smoluchowski.},
  number = {5},
  journaltitle = {Physical Review},
  shortjournal = {Phys. Rev.},
  urldate = {2016-06-09},
  date = {1930-09-01},
  pages = {823--841},
  author = {Uhlenbeck, G. E. and Ornstein, L. S.},
  file = {/home/fh/lib/articles/Uhlenbeck1930.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/JTG2ZQDB/PhysRev.36.html}
}

@book{Gardiner2004,
  langid = {english},
  location = {{Berlin}},
  title = {Handbook of Stochastic Methods for Physics, Chemistry and the Natural Sciences},
  edition = {3. ed},
  isbn = {978-3-540-20882-2},
  abstract = {This valuable and highly-praised reference collects and explains, in simple language and reasonably deductive form, those formulas and methods and their applications used in modern Statistical Physics, including the foundations of Markov systems, stochastic differential equations, Fokker-Planck equations, approximation methods, chemical master equations, and quantum- mechanical Markov processes. The practical orientation and broad coverage appeal to researchers and academics working in theoretical physics, physical chemistry, and related fields. In the third edition of this classic the chapter on quantum Marcov processes has been replaced by a chapter on numerical treatment of stochastic differential equations to make the book even more valuable for practitioners. From the reviews: "Extremely well written and informative... clear, complete, and fairly rigorous treatment of a larger number of very basic concepts in stochastic theory." (Journal of Quantum Electronics) "A first class book." (Optica Acta) "Ideal for people who need a clear introduction to stochastic mathematics and their applications in physical sciences? an excellent self study and reference book." (Quantnotes.com) "This well-established volume takes a supreme position [among the many books on the subject].. This extremely valuable contribution to the field of applied stochastic methods can be recommended to graduate students, researchers, and university teachers." (Optimization)},
  pagetotal = {415},
  series = {Springer series in synergetics},
  publisher = {{Springer}},
  date = {2004},
  keywords = {Stochastischer Prozess,Stochastic processes,Markov-Prozess,Stochastisches Integral},
  author = {Gardiner, Crispin W.},
  file = {/home/fh/lib/books/Gardiner2004_Handbook-of-stochastic-methods-for-physics,-chemistry-and-the-natural-sciences.djvu},
  note = {OCLC: 249252869}
}

@book{Kampen2008,
  langid = {english},
  location = {{Amsterdam}},
  title = {Stochastic Processes in Physics and Chemistry},
  edition = {3. ed., repr},
  isbn = {978-0-444-52965-7},
  pagetotal = {463},
  series = {North-Holland personal library},
  publisher = {{Elsevier}},
  date = {2008},
  keywords = {Lehrbuch,Stochastischer Prozess,Stochastic processes,Fluktuation,Physik,Chemie,Statistical physics,Chemistry; Physical and theoretical Statistical methods},
  author = {van Kampen, Nicolaas Godfried},
  file = {/home/fh/lib/books/Kampen2008_Stochastic-processes-in-physics-and-chemistry.djvu},
  note = {OCLC: 315990030}
}

@article{Butz2009,
  langid = {english},
  title = {Activity-Dependent Structural Plasticity},
  volume = {60},
  issn = {01650173},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0165017308001513},
  doi = {10.1016/j.brainresrev.2008.12.023},
  number = {2},
  journaltitle = {Brain Research Reviews},
  urldate = {2016-06-12},
  date = {2009-05},
  pages = {287--305},
  author = {Butz, Markus and W{\"o}rg{\"o}tter, Florentin and van Ooyen, Arjen},
  options = {useprefix=true},
  file = {/home/fh/lib/articles/Butz2009.pdf}
}

@article{Knoblauch2009,
  title = {Memory {{Capacities}} for {{Synaptic}} and {{Structural Plasticity}}},
  volume = {22},
  issn = {0899-7667},
  url = {http://dx.doi.org/10.1162/neco.2009.08-07-588},
  doi = {10.1162/neco.2009.08-07-588},
  abstract = {Neural associative networks with plastic synapses have been proposed as computational models of brain functions and also for applications such as pattern recognition and information retrieval. To guide biological models and optimize technical applications, several definitions of memory capacity have been used to measure the efficiency of associative memory. Here we explain why the currently used performance measures bias the comparison between models and cannot serve as a theoretical benchmark. We introduce fair measures for information-theoretic capacity in associative memory that also provide a theoretical benchmark.},
  number = {2},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2016-06-12},
  date = {2009-11-19},
  pages = {289--341},
  author = {Knoblauch, Andreas and Palm, G{\"u}nther and Sommer, Friedrich T.},
  file = {/home/fh/lib/articles/Knoblauch2009.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/QRIN68CX/neco.2009.html}
}

@article{Hiratani2016,
  title = {Hebbian {{Wiring Plasticity Generates Efficient Network Structures}} for {{Robust Inference}} with {{Synaptic Weight Plasticity}}},
  url = {http://journal.frontiersin.org/article/10.3389/fncir.2016.00041/full},
  doi = {10.3389/fncir.2016.00041},
  abstract = {In the adult mammalian cortex, a small fraction of spines are created and eliminated every day, and the resultant synaptic connection structure is highly nonrandom, even in local circuits. However, it remains unknown whether a particular synaptic connection structure is functionally advantageous in local circuits, and why creation and elimination of synaptic connections is necessary in addition to rich synaptic weight plasticity. To answer these questions, we studied an inference task model through theoretical and numerical analyses. We demonstrate that a robustly beneficial network structure naturally emerges by combining Hebbian-type synaptic weight plasticity and wiring plasticity. Especially in a sparsely connected network, wiring plasticity achieves reliable computation by enabling efficient information transmission. Furthermore, the proposed rule reproduces experimental observed correlation between spine dynamics and task performance.},
  journaltitle = {Frontiers in Neural Circuits},
  shortjournal = {Front. Neural Circuits},
  urldate = {2016-06-12},
  date = {2016},
  pages = {41},
  keywords = {synaptic plasticity,synaptogenesis,neural decoding,computational model,connectomics},
  author = {Hiratani, Naoki and Fukai, Tomoki},
  file = {/home/fh/lib/articles/Hiratani2016.pdf}
}

@article{Deger2012a,
  title = {Spike-{{Timing Dependence}} of {{Structural Plasticity Explains Cooperative Synapse Formation}} in the {{Neocortex}}},
  volume = {8},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002689},
  doi = {10.1371/journal.pcbi.1002689},
  abstract = {Author Summary  Structural plasticity has been observed even in the adult mammalian neocortex \textendash{} in seemingly static neuronal circuits structural remodeling is continuously at work. Still, it has been shown that the connection patterns between pairs of neurons are not random. In contrast, there is evidence that the synaptic contacts between a pair of neurons cooperate: several experimental studies report either zero or about 3\textendash{}6 synapses between neuron pairs. The mechanism by which the synapses cooperate, however, has not yet been identified. Here we propose a model for structural plasticity that relies on local processes at the dendritic spine. We combine and extend the previous models and determine the equilibrium probability distribution of synaptic contact numbers of the model. By optimizing the parameters numerically for each of three reference datasets, we obtain equilibrium contact number distributions that fit the references very well. We conclude that the local dendritic mechanisms that we assume suffice to explain the cooperative synapse formation in the neocortex.},
  number = {9},
  journaltitle = {PLOS Comput Biol},
  shortjournal = {PLOS Comput Biol},
  urldate = {2016-06-12},
  date = {2012-09-20},
  pages = {e1002689},
  keywords = {neocortex,Neurons,synaptic plasticity,Action potentials,Neuronal plasticity,Synapses,Neuronal dendrites,Probability distribution},
  author = {Deger, Moritz and Helias, Moritz and Rotter, Stefan and Diesmann, Markus},
  file = {/home/fh/lib/articles/Deger2012_2.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/48TNV728/article.html}
}

@article{Fauth2015a,
  title = {The {{Formation}} of {{Multi}}-Synaptic {{Connections}} by the {{Interaction}} of {{Synaptic}} and {{Structural Plasticity}} and {{Their Functional Consequences}}},
  volume = {11},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004031},
  doi = {10.1371/journal.pcbi.1004031},
  abstract = {Author Summary   The connectivity between neurons is modified by different mechanisms. On a time scale of minutes to hours one finds synaptic plasticity, whereas mechanisms for structural changes at axons or dendrites may take days. One main factor determining structural changes is the weight of a connection, which, in turn, is adapted by synaptic plasticity. Both mechanisms, synaptic and structural plasticity, are influenced and determined by the activity pattern in the network. Hence, it is important to understand how activity and the different plasticity mechanisms influence each other. Especially how activity influences rewiring in adult networks is still an open question.   We present a model, which captures these complex interactions by abstracting structural plasticity with weight-dependent probabilities. This allows for calculating the distribution of the number of synapses between two neurons analytically. We report that biologically realistic connection patterns for different cortical layers generically arise with synaptic plasticity rules in which the synaptic weights grow with postsynaptic activity. The connectivity patterns also lead to different activity levels resembling those found in the different cortical layers. Interestingly such a system exhibits a hysteresis by which connections remain stable longer than expected, which may add to the stability of information storage in the network.},
  number = {1},
  journaltitle = {PLOS Comput Biol},
  shortjournal = {PLOS Comput Biol},
  urldate = {2016-06-12},
  date = {2015-01-15},
  pages = {e1004031},
  keywords = {neural networks,Neurons,synaptic plasticity,Neuronal plasticity,Synapses,Neuronal dendrites,Probability distribution,Dynamical systems},
  author = {Fauth, Michael and W{\"o}rg{\"o}tter, Florentin and Tetzlaff, Christian},
  file = {/home/fh/lib/articles/Fauth2015.pdf}
}

@article{Fares2009,
  langid = {english},
  title = {Cooperative Synapse Formation in the Neocortex},
  volume = {106},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/106/38/16463},
  doi = {10.1073/pnas.0813265106},
  abstract = {Neuron morphology plays an important role in defining synaptic connectivity. Clearly, only pairs of neurons with closely positioned axonal and dendritic branches can be synaptically coupled. For excitatory neurons in the cerebral cortex, such axo-dendritic oppositions, termed potential synapses, must be bridged by dendritic spines to form synaptic connections. To explore the rules by which synaptic connections are formed within the constraints imposed by neuron morphology, we compared the distributions of the numbers of actual and potential synapses between pre- and postsynaptic neurons forming different laminar projections in rat barrel cortex. Quantitative comparison explicitly ruled out the hypothesis that individual synapses between neurons are formed independently of each other. Instead, the data are consistent with a cooperative scheme of synapse formation where multiple-synaptic connections between neurons are stabilized while neurons that do not establish a critical number of synapses are not likely to remain synaptically coupled.},
  number = {38},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  urldate = {2016-06-12},
  date = {2009-09-22},
  pages = {16463--16468},
  keywords = {barrel cortex,connectivity,pyramidal cell,morphology,potential synapse},
  author = {Fares, Tarec and Stepanyants, Armen},
  file = {/home/fh/lib/articles/Fares2009.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/XUKKCNPW/16463.html},
  eprinttype = {pmid},
  eprint = {19805321}
}

@article{Navlakha2015,
  title = {Decreasing-{{Rate Pruning Optimizes}} the {{Construction}} of {{Efficient}} and {{Robust Distributed Networks}}},
  volume = {11},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004347},
  doi = {10.1371/journal.pcbi.1004347},
  abstract = {Author Summary   During development of neural circuits in the brain, synapses are massively over-produced and then pruned-back over time. This is a fundamental process that occurs in many brain regions and organisms, yet, despite decades of study of this process, the rate of synapse elimination, and how such rates affect the function and structure of networks, has not been studied. We performed large-scale brain imaging experiments to quantify synapse elimination rates in the developing mouse cortex and found that the rate is decreasing over time (i.e. aggressive elimination occurs early, followed by a longer phase of slow elimination). We show that such rates optimize the efficiency and robustness of distributed routing networks under several models. We also present an application of this strategy to improve the design of airline networks.},
  number = {7},
  journaltitle = {PLOS Comput Biol},
  shortjournal = {PLOS Comput Biol},
  urldate = {2016-06-12},
  date = {2015-07-28},
  pages = {e1004347},
  keywords = {neural networks,electron microscopy,Synapses,Signaling networks,Network analysis,Algorithms,Graphs,Imaging techniques},
  author = {Navlakha, Saket and Barth, Alison L. and Bar-Joseph, Ziv},
  file = {/home/fh/lib/articles/Navlakha2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/WMNBMRUV/article.html}
}

@article{Bourjaily2011,
  title = {Excitatory, Inhibitory, and Structural Plasticity Produce Correlated Connectivity in Random Networks Trained to Solve Paired-Stimulus Tasks},
  volume = {5},
  url = {http://journal.frontiersin.org/article/10.3389/fncom.2011.00037/full},
  doi = {10.3389/fncom.2011.00037},
  abstract = {The pattern of connections among cortical excitatory cells with overlapping arbors is non-random. In particular, correlations among connections produce clustering \textendash{} cells in cliques connect to each other with high probability, but with lower probability to cells in other spatially intertwined cliques. In this study, we model initially randomly connected sparse recurrent networks of spiking neurons with random, overlapping inputs, to investigate what functional and structural synaptic plasticity mechanisms sculpt network connections into the patterns measured in vitro. Our Hebbian implementation of structural plasticity causes a removal of connections between uncorrelated excitatory cells, followed by their random replacement. To model a biconditional discrimination task, we stimulate the network via pairs (A + B, C + D, A + D, and C + B) of four inputs (A, B, C, and D). We find networks that produce neurons most responsive to specific paired inputs \textendash{} a building block of computation and essential role for cortex \textendash{} contain the excessive clustering of excitatory synaptic connections observed in cortical slices. The same networks produce the best performance in a behavioral readout of the networks' ability to complete the task. A plasticity mechanism operating on inhibitory connections, long-term potentiation of inhibition, when combined with structural plasticity, indirectly enhances clustering of excitatory cells via excitatory connections. A rate-dependent (triplet) form of spike-timing-dependent plasticity (STDP) between excitatory cells is less effective and basic STDP is detrimental. Clustering also arises in networks stimulated with single stimuli and in networks undergoing raised levels of spontaneous activity when structural plasticity is combined with functional plasticity. In conclusion, spatially intertwined clusters or cliques of connected excitatory cells can arise via a Hebbian form of structural plasticity operating in initially randomly connected networks.},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci.},
  urldate = {2016-06-13},
  date = {2011},
  pages = {37},
  keywords = {STDP,connectivity,structural plasticity,Hebbian learning,network,simulation,correlations,inhibitory plasticity},
  author = {Bourjaily, Mark A. and Miller, Paul},
  file = {/home/fh/lib/articles/Bourjaily2011.pdf}
}

@article{Vasilaki2014,
  title = {Emergence of {{Connectivity Motifs}} in {{Networks}} of {{Model Neurons}} with {{Short}}- and {{Long}}-{{Term Plastic Synapses}}},
  volume = {9},
  issn = {1932-6203},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0084626},
  doi = {10.1371/journal.pone.0084626},
  abstract = {Recent experimental data from the rodent cerebral cortex and olfactory bulb indicate that specific connectivity motifs are correlated with short-term dynamics of excitatory synaptic transmission. It was observed that neurons with short-term facilitating synapses form predominantly reciprocal pairwise connections, while neurons with short-term depressing synapses form predominantly unidirectional pairwise connections. The cause of these structural differences in excitatory synaptic microcircuits is unknown. We show that these connectivity motifs emerge in networks of model neurons, from the interactions between short-term synaptic dynamics (SD) and long-term spike-timing dependent plasticity (STDP). While the impact of STDP on SD was shown in simultaneous neuronal pair recordings  in vitro , the mutual interactions between STDP and SD in large networks are still the subject of intense research. Our approach combines an SD phenomenological model with an STDP model that faithfully captures long-term plasticity dependence on both spike times and frequency. As a proof of concept, we first simulate and analyze recurrent networks of spiking neurons with random initial connection efficacies and where synapses are either all short-term facilitating or all depressing. For identical external inputs to the network, and as a direct consequence of internally generated activity, we find that networks with depressing synapses evolve unidirectional connectivity motifs, while networks with facilitating synapses evolve reciprocal connectivity motifs. We then show that the same results hold for heterogeneous networks, including both facilitating and depressing synapses. This does not contradict a recent theory that proposes that motifs are shaped by external inputs, but rather complements it by examining the role of both the external inputs and the internally generated network activity. Our study highlights the conditions under which SD-STDP might explain the correlation between facilitation and reciprocal connectivity motifs, as well as between depression and unidirectional motifs.},
  number = {1},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2016-06-13},
  date = {2014-01-15},
  pages = {e84626},
  keywords = {neural networks,Neurons,synaptic plasticity,Action potentials,Network motifs,Neuronal plasticity,Synapses,Depression},
  author = {Vasilaki, Eleni and Giugliano, Michele},
  file = {/home/fh/lib/articles/Vasilaki2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/TZU9RP6D/article.html}
}

@article{George2015,
  title = {Modeling the Interplay between Structural Plasticity and Spike-Timing-Dependent Plasticity},
  volume = {16},
  issn = {1471-2202},
  url = {http://dx.doi.org/10.1186/1471-2202-16-S1-P107},
  doi = {10.1186/1471-2202-16-S1-P107},
  number = {1},
  journaltitle = {BMC Neuroscience},
  shortjournal = {BMC Neuroscience},
  urldate = {2016-06-13},
  date = {2015},
  pages = {1--2},
  author = {George, Richard M. and Diehl, Peter U. and Cook, Matthew and Mayr, Christian and Indiveri, Giacomo},
  file = {/home/fh/lib/articles/George2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/EFIMD745/1471-2202-16-S1-P107.html}
}

@article{Lee2016,
  title = {Topology of {{ON}} and {{OFF}} Inputs in Visual Cortex Enables an Invariant Columnar Architecture},
  volume = {533},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/nature17941},
  doi = {10.1038/nature17941},
  number = {7601},
  journaltitle = {Nature},
  urldate = {2016-06-14},
  date = {2016-04-27},
  pages = {90--94},
  author = {Lee, Kuo-Sheng and Huang, Xiaoying and Fitzpatrick, David},
  file = {/home/fh/lib/articles/Lee2016_2.pdf}
}

@article{Woolsey1970,
  title = {The Structural Organization of Layer {{IV}} in the Somatosensory Region ({{S I}}) of Mouse Cerebral Cortex: {{The}} Description of a Cortical Field Composed of Discrete Cytoarchitectonic Units},
  volume = {17},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/000689937090079X},
  doi = {10.1016/0006-8993(70)90079-X},
  shorttitle = {The Structural Organization of Layer {{IV}} in the Somatosensory Region ({{S I}}) of Mouse Cerebral Cortex},
  number = {2},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  urldate = {2016-06-16},
  date = {1970-01-20},
  pages = {205--242},
  author = {Woolsey, Thomas A. and Van der Loos, Hendrik},
  file = {/home/fh/lib/articles/Woolsey1970.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/7AM2G6HW/000689937090079X.html}
}

@thesis{Gjorgjieva2011,
  title = {Spontaneous Activity and Plasticity in the Developing Nervous System},
  date = {2011},
  author = {Gjorgjieva, Julijana},
  file = {/home/fh/lib/thesis/Gjorgjieva2011.pdf}
}

@article{Hu2013a,
  title = {Motif Statistics and Spike Correlations in Neuronal Networks},
  volume = {2013},
  issn = {1742-5468},
  url = {http://stacks.iop.org/1742-5468/2013/i=03/a=P03012?key=crossref.745a7a0bacb5280671da5078bb3778f8},
  doi = {10.1088/1742-5468/2013/03/P03012},
  number = {03},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  urldate = {2016-06-18},
  date = {2013-03-12},
  pages = {P03012},
  author = {Hu, Yu and Trousdale, James and Josi{\'c}, Kre{\v s}imir and Shea-Brown, Eric},
  file = {/home/fh/lib/articles/Hu2013_2.pdf}
}

@article{Babadi2016,
  langid = {english},
  title = {Stability and {{Competition}} in {{Multi}}-Spike {{Models}} of {{Spike}}-{{Timing Dependent Plasticity}}},
  volume = {12},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1004750},
  doi = {10.1371/journal.pcbi.1004750},
  number = {3},
  journaltitle = {PLOS Computational Biology},
  urldate = {2016-06-18},
  date = {2016-03-03},
  pages = {e1004750},
  author = {Babadi, Baktash and Abbott, L. F.},
  editor = {Gutkin, Boris S.},
  file = {/home/fh/lib/articles/Babadi2016.pdf}
}

@article{Sun2016,
  langid = {english},
  title = {Thalamus Provides Layer 4 of Primary Visual Cortex with Orientation- and Direction-Tuned Inputs},
  volume = {19},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v19/n2/full/nn.4196.html},
  doi = {10.1038/nn.4196},
  abstract = {Understanding the functions of a brain region requires knowing the neural representations of its myriad inputs, local neurons and outputs. Primary visual cortex (V1) has long been thought to compute visual orientation from untuned thalamic inputs, but very few thalamic inputs have been measured in any mammal. We determined the response properties of \textasciitilde{}28,000 thalamic boutons and \textasciitilde{}4,000 cortical neurons in layers 1\textendash{}5 of awake mouse V1. Using adaptive optics that allows accurate measurement of bouton activity deep in cortex, we found that around half of the boutons in the main thalamorecipient L4 carried orientation-tuned information and that their orientation and direction biases were also dominant in the L4 neuron population, suggesting that these neurons may inherit their selectivity from tuned thalamic inputs. Cortical neurons in all layers exhibited sharper tuning than thalamic boutons and a greater diversity of preferred orientations. Our results provide data-rich constraints for refining mechanistic models of cortical computation.},
  number = {2},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2016-06-20},
  date = {2016-02},
  pages = {308--315},
  keywords = {Striate cortex,Motion detection,Neural circuits,Thalamus},
  author = {Sun, Wenzhi and Tan, Zhongchao and Mensh, Brett D. and Ji, Na},
  file = {/home/fh/lib/articles/Sun2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/RMK3R2VF/nn.4196.html}
}

@report{zotero-null-1140,
  title = {Latexdiff},
  file = {/home/fh/lib/manuals/latex/latexdiff.pdf},
  note = {manuals/latex}
}

@report{zotero-null-1144,
  title = {Easylist},
  file = {/home/fh/lib/manuals/latex/easylist.pdf},
  note = {manuals/latex}
}

@article{Romani2013,
  title = {Scaling {{Laws}} of {{Associative Memory Retrieval}}},
  volume = {25},
  issn = {0899-7667},
  url = {http://dx.doi.org/10.1162/NECO_a_00499},
  doi = {10.1162/NECO_a_00499},
  abstract = {Most people have great difficulty in recalling unrelated items. For example, in free recall experiments, lists of more than a few randomly selected words cannot be accurately repeated. Here we introduce a phenomenological model of memory retrieval inspired by theories of neuronal population coding of information. The model predicts nontrivial scaling behaviors for the mean and standard deviation of the number of recalled words for lists of increasing length. Our results suggest that associative information retrieval is a dominating factor that limits the number of recalled items.},
  number = {10},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2016-07-03},
  date = {2013-06-18},
  pages = {2523--2544},
  author = {Romani, Sandro and Pinkoviezky, Itai and Rubin, Alon and Tsodyks, Misha},
  file = {/home/fh/lib/articles/Romani2013.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/I4H8SGK5/NECO_a_00499.html}
}

@article{Geisler2010,
  langid = {english},
  title = {Temporal Delays among Place Cells Determine the Frequency of Population Theta Oscillations in the Hippocampus},
  volume = {107},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/107/17/7957},
  doi = {10.1073/pnas.0912478107},
  abstract = {Driven either by external landmarks or by internal dynamics, hippocampal neurons form sequences of cell assemblies. The coordinated firing of these active cells is organized by the prominent ``theta'' oscillations in the local field potential (LFP): place cells discharge at progressively earlier theta phases as the rat crosses the respective place field (``phase precession''). The faster oscillation frequency of active neurons and the slower theta LFP, underlying phase precession, creates a paradox. How can faster oscillating neurons comprise a slower population oscillation, as reflected by the LFP? We built a mathematical model that allowed us to calculate the population activity analytically from experimentally derived parameters of the single neuron oscillation frequency, firing field size (duration), and the relationship between within-theta delays of place cell pairs and their distance representations (``compression''). The appropriate combination of these parameters generated a constant frequency population rhythm along the septo\textendash{}temporal axis of the hippocampus, while allowing individual neurons to vary their oscillation frequency and field size. Our results suggest that the faster-than-theta oscillations of pyramidal cells are inherent and that phase precession is a result of the coordinated activity of temporally shifted cell assemblies, relative to the population activity, reflected by the LFP.},
  number = {17},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  urldate = {2016-07-05},
  date = {2010-04-27},
  pages = {7957--7962},
  keywords = {assembly sequence,cell assembly,phase precession,phase coding,ventral hippocampus},
  author = {Geisler, Caroline and Diba, Kamran and Pastalkova, Eva and Mizuseki, Kenji and Royer, Sebastien and Buzs{\'a}ki, Gy{\"o}rgy},
  file = {/home/fh/lib/articles/Geisler2010.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/5MVZUMGI/7957.html},
  eprinttype = {pmid},
  eprint = {20375279}
}

@collection{Cuntz2014,
  location = {{New York}},
  title = {The Computing Dendrite: From Structure to Function},
  isbn = {978-1-4614-8093-8},
  shorttitle = {The Computing Dendrite},
  pagetotal = {510},
  number = {volume 11},
  series = {Springer series in computational neuroscience},
  publisher = {{Springer}},
  date = {2014},
  keywords = {computational neuroscience,Dendrites},
  editor = {Cuntz, Hermann and Remme, Michiel W. H. and Torben-Nielsen, Benjamin},
  file = {/home/fh/lib/books/Cuntz2014_The-computing-dendrite-from-structure-to-function.pdf},
  note = {OCLC: ocn882609497}
}

@article{Hiratani2014,
  title = {Interplay between {{Short}}- and {{Long}}-{{Term Plasticity}} in {{Cell}}-{{Assembly Formation}}},
  volume = {9},
  issn = {1932-6203},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0101535},
  doi = {10.1371/journal.pone.0101535},
  abstract = {Various hippocampal and neocortical synapses of mammalian brain show both short-term plasticity and long-term plasticity, which are considered to underlie learning and memory by the brain. According to Hebb's postulate, synaptic plasticity encodes memory traces of past experiences into cell assemblies in cortical circuits. However, it remains unclear how the various forms of long-term and short-term synaptic plasticity cooperatively create and reorganize such cell assemblies. Here, we investigate the mechanism in which the three forms of synaptic plasticity known in cortical circuits, i.e., spike-timing-dependent plasticity (STDP), short-term depression (STD) and homeostatic plasticity, cooperatively generate, retain and reorganize cell assemblies in a recurrent neuronal network model. We show that multiple cell assemblies generated by external stimuli can survive noisy spontaneous network activity for an adequate range of the strength of STD. Furthermore, our model predicts that a symmetric temporal window of STDP, such as observed in dopaminergic modulations on hippocampal neurons, is crucial for the retention and integration of multiple cell assemblies. These results may have implications for the understanding of cortical memory processes.},
  number = {7},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2016-07-09},
  date = {2014-07-09},
  pages = {e101535},
  keywords = {neural networks,Neurons,synaptic plasticity,Action potentials,Neuronal plasticity,Synapses,Network analysis,Memory},
  author = {Hiratani, Naoki and Fukai, Tomoki},
  file = {/home/fh/lib/articles/Hiratani2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/XXWQKQHX/article.html}
}

@article{Hiratani2016a,
  langid = {english},
  title = {Detailed Dendritic Excitatory/Inhibitory Balance through Heterosynaptic Spike-Timing-Dependent Plasticity},
  url = {http://biorxiv.org/content/early/2016/05/30/056093},
  doi = {10.1101/056093},
  abstract = {Balance between excitatory and inhibitory inputs is a key feature of cortical dynamics. Such balance is arguably preserved in dendritic branches, yet its underlying mechanism and functional roles are still unknown. Here, by considering computational models of heterosynaptic spike-timing-dependent plasticity (STDP), we show that the detailed excitatory/inhibitory balance on dendritic branch is robustly achieved through heterosynaptic interaction between excitatory and inhibitory synapses. The acquired dendritic balance enables neuron to perform change detection, due to functional specialization at each branch. Furthermore, heterosynaptic STDP explains how maturation of inhibitory neurons modulates selectivity of excitatory neurons in critical period plasticity of binocular matching. Our results propose heterosynaptic STDP as a critical factor in synaptic organization and resultant dendritic computation.},
  journaltitle = {bioRxiv},
  urldate = {2016-07-09},
  date = {2016-05-30},
  pages = {056093},
  author = {Hiratani, Naoki and Fukai, Tomoki},
  file = {/home/fh/lib/articles/Hiratani2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/4UZ3ARVE/056093.html}
}

@article{Destexhe2009,
  langid = {english},
  title = {Self-Sustained Asynchronous Irregular States and {{Up}}\textendash{}{{Down}} States in Thalamic, Cortical and Thalamocortical Networks of Nonlinear Integrate-and-Fire Neurons},
  volume = {27},
  issn = {0929-5313, 1573-6873},
  url = {http://link.springer.com/article/10.1007/s10827-009-0164-4},
  doi = {10.1007/s10827-009-0164-4},
  abstract = {Randomly-connected networks of integrate-and-fire (IF) neurons are known to display asynchronous irregular (AI) activity states, which resemble the discharge activity recorded in the cerebral cortex of awake animals. However, it is not clear whether such activity states are specific to simple IF models, or if they also exist in networks where neurons are endowed with complex intrinsic properties similar to electrophysiological measurements. Here, we investigate the occurrence of AI states in networks of nonlinear IF neurons, such as the adaptive exponential IF (Brette-Gerstner-Izhikevich) model. This model can display intrinsic properties such as low-threshold spike (LTS), regular spiking (RS) or fast-spiking (FS). We successively investigate the oscillatory and AI dynamics of thalamic, cortical and thalamocortical networks using such models. AI states can be found in each case, sometimes with surprisingly small network size of the order of a few tens of neurons. We show that the presence of LTS neurons in cortex or in thalamus, explains the robust emergence of AI states for relatively small network sizes. Finally, we investigate the role of spike-frequency adaptation (SFA). In cortical networks with strong SFA in RS cells, the AI state is transient, but when SFA is reduced, AI states can be self-sustained for long times. In thalamocortical networks, AI states are found when the cortex is itself in an AI state, but with strong SFA, the thalamocortical network displays Up and Down state transitions, similar to intracellular recordings during slow-wave sleep or anesthesia. Self-sustained Up and Down states could also be generated by two-layer cortical networks with LTS cells. These models suggest that intrinsic properties such as adaptation and low-threshold bursting activity are crucial for the genesis and control of AI states in thalamocortical networks.},
  number = {3},
  journaltitle = {Journal of Computational Neuroscience},
  shortjournal = {J Comput Neurosci},
  urldate = {2016-07-09},
  date = {2009-06-05},
  pages = {493--506},
  author = {Destexhe, Alain},
  file = {/home/fh/lib/articles/Destexhe2009.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/5CQ2DV2Q/10.html}
}

@article{Brunel2004,
  title = {Optimal {{Information Storage}} and the {{Distribution}} of {{Synaptic Weights}}: {{Perceptron}} versus {{Purkinje Cell}}},
  volume = {43},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627304005288},
  doi = {10.1016/j.neuron.2004.08.023},
  shorttitle = {Optimal {{Information Storage}} and the {{Distribution}} of {{Synaptic Weights}}},
  abstract = {It is widely believed that synaptic modifications underlie learning and memory. However, few studies have examined what can be deduced about the learning process from the distribution of synaptic weights. We analyze the perceptron, a prototypical feedforward neural network, and obtain the optimal synaptic weight distribution for a perceptron with excitatory synapses. It contains more than 50\% silent synapses, and this fraction increases with storage reliability: silent synapses are therefore a necessary byproduct of optimizing learning and reliability. Exploiting the classical analogy between the perceptron and the cerebellar Purkinje cell, we fitted the optimal weight distribution to that measured for granule cell-Purkinje cell synapses. The two distributions agreed well, suggesting that the Purkinje cell can learn up to 5 kilobytes of information, in the form of 40,000 input-output associations.},
  number = {5},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2016-07-18},
  date = {2004-09-02},
  pages = {745--757},
  author = {Brunel, Nicolas and Hakim, Vincent and Isope, Philippe and Nadal, Jean-Pierre and Barbour, Boris},
  file = {/home/fh/lib/articles/Brunel22.pdf}
}

@article{Izhikevich2008,
  langid = {english},
  title = {Large-Scale Model of Mammalian Thalamocortical Systems},
  volume = {105},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/105/9/3593},
  doi = {10.1073/pnas.0712231105},
  abstract = {The understanding of the structural and dynamic complexity of mammalian brains is greatly facilitated by computer simulations. We present here a detailed large-scale thalamocortical model based on experimental measures in several mammalian species. The model spans three anatomical scales. (i) It is based on global (white-matter) thalamocortical anatomy obtained by means of diffusion tensor imaging (DTI) of a human brain. (ii) It includes multiple thalamic nuclei and six-layered cortical microcircuitry based on in vitro labeling and three-dimensional reconstruction of single neurons of cat visual cortex. (iii) It has 22 basic types of neurons with appropriate laminar distribution of their branching dendritic trees. The model simulates one million multicompartmental spiking neurons calibrated to reproduce known types of responses recorded in vitro in rats. It has almost half a billion synapses with appropriate receptor kinetics, short-term plasticity, and long-term dendritic spike-timing-dependent synaptic plasticity (dendritic STDP). The model exhibits behavioral regimes of normal brain activity that were not explicitly built-in but emerged spontaneously as the result of interactions among anatomical and dynamic processes. We describe spontaneous activity, sensitivity to changes in individual neurons, emergence of waves and rhythms, and functional connectivity on different scales.},
  number = {9},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  urldate = {2016-07-18},
  date = {2008-04-03},
  pages = {3593--3598},
  keywords = {Cerebral Cortex,brain models,diffusion tensor imaging,oscillations,spike-timing-dependent synaptic plasticity},
  author = {Izhikevich, Eugene M. and Edelman, Gerald M.},
  file = {/home/fh/lib/articles/Izhikevich2008_2.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/N5NXEPP3/3593.html},
  eprinttype = {pmid},
  eprint = {18292226}
}

@article{Klampfl2013,
  langid = {english},
  title = {Emergence of {{Dynamic Memory Traces}} in {{Cortical Microcircuit Models}} through {{STDP}}},
  volume = {33},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.5044-12.2013},
  doi = {10.1523/JNEUROSCI.5044-12.2013},
  number = {28},
  journaltitle = {Journal of Neuroscience},
  urldate = {2016-07-18},
  date = {2013-07-10},
  pages = {11515--11529},
  author = {Klampfl, S. and Maass, W.},
  file = {/home/fh/lib/articles/Klampfl2013.pdf}
}

@article{Legenstein2008,
  langid = {english},
  title = {A {{Learning Theory}} for {{Reward}}-{{Modulated Spike}}-{{Timing}}-{{Dependent Plasticity}} with {{Application}} to {{Biofeedback}}},
  volume = {4},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1000180},
  doi = {10.1371/journal.pcbi.1000180},
  number = {10},
  journaltitle = {PLoS Computational Biology},
  urldate = {2016-07-18},
  date = {2008-10-10},
  pages = {e1000180},
  author = {Legenstein, Robert and Pecevski, Dejan and Maass, Wolfgang},
  editor = {Graham, Lyle J.},
  file = {/home/fh/lib/articles/Legenstein2008.pdf}
}

@article{Hansel2012,
  langid = {english},
  title = {The {{Mechanism}} of {{Orientation Selectivity}} in {{Primary Visual Cortex}} without a {{Functional Map}}},
  volume = {32},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/32/12/4049},
  doi = {10.1523/JNEUROSCI.6284-11.2012},
  abstract = {Neurons in primary visual cortex (V1) display substantial orientation selectivity even in species where V1 lacks an orientation map, such as in mice and rats. The mechanism underlying orientation selectivity in V1 with such a salt-and-pepper organization is unknown; it is unclear whether a connectivity that depends on feature similarity is required, or a random connectivity suffices. Here we argue for the latter. We study the response to a drifting grating of a network model of layer 2/3 with random recurrent connectivity and feedforward input from layer 4 neurons with random preferred orientations. We show that even though the total feedforward and total recurrent excitatory and inhibitory inputs all have a very weak orientation selectivity, strong selectivity emerges in the neuronal spike responses if the network operates in the balanced excitation/inhibition regime. This is because in this regime the (large) untuned components in the excitatory and inhibitory contributions approximately cancel. As a result the untuned part of the input into a neuron as well as its modulation with orientation and time all have a size comparable to the neuronal threshold. However, the tuning of the F0 and F1 components of the input are uncorrelated and the high-frequency fluctuations are not tuned. This is reflected in the subthreshold voltage response. Remarkably, due to the nonlinear voltage-firing rate transfer function, the preferred orientation of the F0 and F1 components of the spike response are highly correlated.},
  number = {12},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2016-07-18},
  date = {2012-03-21},
  pages = {4049--4064},
  author = {Hansel, David and van Vreeswijk, Carl},
  file = {/home/fh/lib/articles/Hansel2012.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/RBNXD9AB/4049.html},
  eprinttype = {pmid},
  eprint = {22442071}
}

@article{Hansel2013,
  langid = {english},
  title = {Short-{{Term Plasticity Explains Irregular Persistent Activity}} in {{Working Memory Tasks}}},
  volume = {33},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.3455-12.2013},
  doi = {10.1523/JNEUROSCI.3455-12.2013},
  number = {1},
  journaltitle = {Journal of Neuroscience},
  urldate = {2016-07-18},
  date = {2013-01-02},
  pages = {133--149},
  author = {Hansel, D. and Mato, G.},
  file = {/home/fh/lib/articles/Hansel2013.pdf}
}

@article{Kumar2010,
  title = {Spiking Activity Propagation in Neuronal Networks: Reconciling Different Perspectives on Neural Coding},
  volume = {11},
  issn = {1471-003X, 1471-0048},
  url = {http://www.nature.com/doifinder/10.1038/nrn2886},
  doi = {10.1038/nrn2886},
  shorttitle = {Spiking Activity Propagation in Neuronal Networks},
  number = {9},
  journaltitle = {Nature Reviews Neuroscience},
  urldate = {2016-07-18},
  date = {2010-09},
  pages = {615--627},
  author = {Kumar, Arvind and Rotter, Stefan and Aertsen, Ad},
  file = {/home/fh/lib/articles/Kumar2010.pdf}
}

@article{Mongillo2010,
  title = {Irregular {{Spiking}} and {{Multi}}-Stability in {{Recurrent Networks}} with {{Non}}-Linear {{Synaptic Transmission}}},
  date = {2010},
  author = {Mongillo, Gianluigi and van Vreeswijk, Carl and Hansel, David},
  options = {useprefix=true},
  file = {/home/fh/lib/articles/Mongillo2010.pdf}
}

@article{Ostojic2011,
  langid = {english},
  title = {From {{Spiking Neuron Models}} to {{Linear}}-{{Nonlinear Models}}},
  volume = {7},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1001056},
  doi = {10.1371/journal.pcbi.1001056},
  number = {1},
  journaltitle = {PLoS Computational Biology},
  urldate = {2016-07-18},
  date = {2011-01-20},
  pages = {e1001056},
  author = {Ostojic, Srdjan and Brunel, Nicolas},
  editor = {Latham, Peter E.},
  file = {/home/fh/lib/articles/Ostojic2011.pdf}
}

@article{Schaffer2013,
  langid = {english},
  title = {A {{Complex}}-{{Valued Firing}}-{{Rate Model That Approximates}} the {{Dynamics}} of {{Spiking Networks}}},
  volume = {9},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1003301},
  doi = {10.1371/journal.pcbi.1003301},
  number = {10},
  journaltitle = {PLoS Computational Biology},
  urldate = {2016-07-18},
  date = {2013-10-31},
  pages = {e1003301},
  author = {Schaffer, Evan S. and Ostojic, Srdjan and Abbott, L. F.},
  editor = {Ermentrout, Bard},
  file = {/home/fh/lib/articles/Schaffer2013.pdf}
}

@article{Amit1997,
  langid = {english},
  title = {Model of Global Spontaneous Activity and Local Structured Activity during Delay Periods in the Cerebral Cortex.},
  volume = {7},
  issn = {1047-3211, 1460-2199},
  url = {http://cercor.oxfordjournals.org/content/7/3/237},
  doi = {10.1093/cercor/7.3.237},
  abstract = {We investigate self-sustaining stable states (attractors) in networks of integrate-and-fire neurons. First, we study the stability of spontaneous activity in an unstructured network. It is shown that the stochastic background activity, of 1-5 spikes/s, is unstable if all neurons are excitatory. On the other hand, spontaneous activity becomes self-stabilizing in presence of local inhibition, given reasonable values of the parameters of the network. Second, in a network sustaining physiological spontaneous rates, we study the effect of learning in a local module, expressed in synaptic modifications in specific populations of synapses. We find that if the average synaptic potentiation (LTP) is too low, no stimulus specific activity manifests itself in the delay period. Instead, following the presentation and removal of any stimulus there is, in the local module, a delay activity in which all neurons selective (responding visually) to any of the stimuli presented for learning have rates which gradually increase with the amplitude of synaptic potentiation. When the average LTP increases beyond a critical value, specific local attractors (stable states) appear abruptly against the background of the global uniform spontaneous attractor. In this case the local module has two available types of collective delay activity: if the stimulus is unfamiliar, the activity is spontaneous; if it is similar to a learned stimulus, delay activity is selective. These new attractors reflect the synaptic structure developed during learning. In each of them a small population of neurons have elevated rates, which depend on the strength of LTP. The remaining neurons of the module have their activity at spontaneous rates. The predictions made in this paper could be checked by single unit recordings in delayed response experiments.},
  number = {3},
  journaltitle = {Cerebral Cortex},
  shortjournal = {Cereb. Cortex},
  urldate = {2016-07-18},
  date = {1997-01-04},
  pages = {237--252},
  author = {Amit, D. J. and Brunel, N.},
  file = {/home/fh/lib/articles/Amit1997.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/2FBK9UE2/237.html},
  eprinttype = {pmid},
  eprint = {9143444}
}

@article{Brunel2000,
  langid = {english},
  title = {Dynamics of {{Sparsely Connected Networks}} of {{Excitatory}} and {{Inhibitory Spiking Neurons}}},
  volume = {8},
  issn = {0929-5313, 1573-6873},
  url = {http://link.springer.com/article/10.1023/A%3A1008925309027},
  doi = {10.1023/A:1008925309027},
  abstract = {The dynamics of networks of sparsely connected excitatory and inhibitory integrate-and-fire neurons are studied analytically. The analysis reveals a rich repertoire of states, including synchronous states in which neurons fire regularly; asynchronous states with stationary global activity and very irregular individual cell activity; and states in which the global activity oscillates but individual cells fire irregularly, typically at rates lower than the global oscillation frequency. The network can switch between these states, provided the external frequency, or the balance between excitation and inhibition, is varied. Two types of network oscillations are observed. In the fast oscillatory state, the network frequency is almost fully controlled by the synaptic time scale. In the slow oscillatory state, the network frequency depends mostly on the membrane time constant. Finite size effects in the asynchronous state are also discussed.},
  number = {3},
  journaltitle = {Journal of Computational Neuroscience},
  shortjournal = {J Comput Neurosci},
  urldate = {2016-07-18},
  date = {2000-05},
  pages = {183--208},
  keywords = {Human Genetics,Neurology,Neurosciences,Theory of Computation,Recurrent network,Synchronization},
  author = {Brunel, Nicolas},
  file = {/home/fh/lib/articles/Brunel2000.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/ZZAZTIN8/A1008925309027.html}
}

@article{Brunel2001,
  langid = {english},
  title = {Effects of {{Synaptic Noise}} and {{Filtering}} on the {{Frequency Response}} of {{Spiking Neurons}}},
  volume = {86},
  issn = {0031-9007, 1079-7114},
  url = {http://link.aps.org/doi/10.1103/PhysRevLett.86.2186},
  doi = {10.1103/PhysRevLett.86.2186},
  number = {10},
  journaltitle = {Physical Review Letters},
  urldate = {2016-07-18},
  date = {2001-03-05},
  pages = {2186--2189},
  author = {Brunel, Nicolas and Chance, Frances S. and Fourcaud, Nicolas and Abbott, L. F.},
  file = {/home/fh/lib/articles/Brunel2001.pdf}
}

@article{Brunel1999,
  title = {Fast {{Global Oscillations}} in {{Networks}} of {{Integrate}}-and-{{Fire Neurons}} with {{Low Firing Rates}}},
  volume = {11},
  issn = {0899-7667},
  doi = {10.1162/089976699300016179},
  abstract = {We study analytically the dynamics of a network of sparsely connected inhibitory integrate-and-fire neurons in a regime where individual neurons emit spikes irregularly and at a low rate. In the limit when the number of neurons N $\rightarrow$ $\infty$, the network exhibits a sharp transition between a stationary and an oscillatory global activity regime where neurons are weakly synchronized. The activity becomes oscillatory when the inhibitory feedback is strong enough. The period of the global oscillation is found to be mainly controlled by synaptic times but depends also on the characteristics of the external input. In large but finite networks, the analysis shows that global oscillations of finite coherence time generically exist both above and below the critical inhibition threshold. Their characteristics are determined as functions of systems parameters in these two different regimes. The results are found to be in good agreement with numerical simulations.},
  number = {7},
  journaltitle = {Neural Computation},
  date = {1999-10},
  pages = {1621--1671},
  author = {Brunel, N. and Hakim, V.},
  file = {/home/fh/lib/articles/Brunel1999.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/D954Z3ZV/login.html}
}

@article{Shadlen1998,
  langid = {english},
  title = {The {{Variable Discharge}} of {{Cortical Neurons}}: {{Implications}} for {{Connectivity}}, {{Computation}}, and {{Information Coding}}},
  volume = {18},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/18/10/3870},
  shorttitle = {The {{Variable Discharge}} of {{Cortical Neurons}}},
  abstract = {Cortical neurons exhibit tremendous variability in the number and temporal distribution of spikes in their discharge patterns. Furthermore, this variability appears to be conserved over large regions of the cerebral cortex, suggesting that it is neither reduced nor expanded from stage to stage within a processing pathway. To investigate the principles underlying such statistical homogeneity, we have analyzed a model of synaptic integration incorporating a highly simplified integrate and fire mechanism with decay. We analyzed a ``high-input regime'' in which neurons receive hundreds of excitatory synaptic inputs during each interspike interval. To produce a graded response in this regime, the neuron must balance excitation with inhibition. We find that a simple integrate and fire mechanism with balanced excitation and inhibition produces a highly variable interspike interval, consistent with experimental data. Detailed information about the temporal pattern of synaptic inputs cannot be recovered from the pattern of output spikes, and we infer that cortical neurons are unlikely to transmit information in the temporal pattern of spike discharge. Rather, we suggest that quantities are represented as rate codes in ensembles of 50\textendash{}100 neurons. These column-like ensembles tolerate large fractions of common synaptic input and yet covary only weakly in their spike discharge. We find that an ensemble of 100 neurons provides a reliable estimate of rate in just one interspike interval (10\textendash{}50 msec). Finally, we derived an expression for the variance of the neural spike count that leads to a stable propagation of signal and noise in networks of neurons\textemdash{}that is, conditions that do not impose an accumulation or diminution of noise. The solution implies that single neurons perform simple algebra resembling averaging, and that more sophisticated computations arise by virtue of the anatomical convergence of novel combinations of inputs to the cortical column from external sources.},
  number = {10},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2016-07-18},
  date = {1998-05-15},
  pages = {3870--3896},
  keywords = {Correlation,visual cortex,noise,rate code,temporal coding,interspike interval,spike count variance,response variability,synaptic integration,neural model},
  author = {Shadlen, Michael N. and Newsome, William T.},
  file = {/home/fh/lib/articles/Shadlen1998.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/WXQ8K2P7/3870.html},
  eprinttype = {pmid},
  eprint = {9570816}
}

@article{Renart2010,
  langid = {english},
  title = {The {{Asynchronous State}} in {{Cortical Circuits}}},
  volume = {327},
  issn = {0036-8075, 1095-9203},
  url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1179850},
  doi = {10.1126/science.1179850},
  number = {5965},
  journaltitle = {Science},
  urldate = {2016-07-18},
  date = {2010-01-29},
  pages = {587--590},
  keywords = {_tablet},
  author = {Renart, A. and de la Rocha, J. and Bartho, P. and Hollender, L. and Parga, N. and Reyes, A. and Harris, K. D.},
  options = {useprefix=true},
  file = {/home/fh/lib/articles/Renart2010.pdf}
}

@article{Lerchner2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.00411},
  primaryClass = {q-bio},
  title = {A Unifying Framework for Understanding State-Dependent Network Dynamics in Cortex},
  url = {http://arxiv.org/abs/1511.00411},
  abstract = {Activity in neocortex exhibits a range of behaviors, from irregular to temporally precise, and from weakly to strongly correlated. So far there has been no single theoretical framework that could explain all these behaviors, leaving open the possibility that they are a signature of radically different mechanisms. Here, we suggest that this is not the case. Instead, we show that a single theory can account for a broad spectrum of experimental observations, including specifics such as the fine temporal details of subthreshold cross-correlations. For the model underlying our theory, we need only assume a small number of well-established properties common to all local cortical networks. When these assumptions are combined with realistically structured input, they produce exactly the repertoire of behaviors that is observed experimentally, and lead to a number of testable predictions.},
  urldate = {2016-07-18},
  date = {2015-11-02},
  keywords = {Quantitative Biology - Neurons and Cognition},
  author = {Lerchner, Alexander and Latham, Peter E.},
  file = {/home/fh/lib/articles/Lerchner2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/XVKVFFQ8/1511.html}
}

@article{Hennequin2014,
  langid = {english},
  title = {Optimal {{Control}} of {{Transient Dynamics}} in {{Balanced Networks Supports Generation}} of {{Complex Movements}}},
  volume = {82},
  issn = {08966273},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627314003602},
  doi = {10.1016/j.neuron.2014.04.045},
  number = {6},
  journaltitle = {Neuron},
  urldate = {2016-07-18},
  date = {2014-06},
  pages = {1394--1406},
  author = {Hennequin, Guillaume and Vogels, Tim P. and Gerstner, Wulfram},
  file = {/home/fh/lib/articles/Hennequin2014.pdf}
}

@article{Haider2006,
  langid = {english},
  title = {Neocortical {{Network Activity In Vivo Is Generated}} through a {{Dynamic Balance}} of {{Excitation}} and {{Inhibition}}},
  volume = {26},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.5297-05.2006},
  doi = {10.1523/JNEUROSCI.5297-05.2006},
  number = {17},
  journaltitle = {Journal of Neuroscience},
  urldate = {2016-07-18},
  date = {2006-04-26},
  pages = {4535--4545},
  author = {Haider, B.},
  file = {/home/fh/lib/articles/Haider2006.pdf}
}

@article{Ozeki2009,
  langid = {english},
  title = {Inhibitory {{Stabilization}} of the {{Cortical Network Underlies Visual Surround Suppression}}},
  volume = {62},
  issn = {08966273},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627309002876},
  doi = {10.1016/j.neuron.2009.03.028},
  number = {4},
  journaltitle = {Neuron},
  urldate = {2016-07-18},
  date = {2009-05},
  pages = {578--592},
  author = {Ozeki, Hirofumi and Finn, Ian M. and Schaffer, Evan S. and Miller, Kenneth D. and Ferster, David},
  file = {/home/fh/lib/articles/Ozeki2009.pdf}
}

@article{Tan2013,
  langid = {english},
  title = {A Spontaneous State of Weakly Correlated Synaptic Excitation and Inhibition in Visual Cortex},
  volume = {247},
  issn = {03064522},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S030645221300465X},
  doi = {10.1016/j.neuroscience.2013.05.037},
  journaltitle = {Neuroscience},
  urldate = {2016-07-18},
  date = {2013-09},
  pages = {364--375},
  author = {Tan, A.Y.Y. and Andoni, S. and Priebe, N.J.},
  file = {/home/fh/lib/articles/Tan2013.pdf}
}

@article{Graupner2013,
  langid = {english},
  title = {Synaptic {{Input Correlations Leading}} to {{Membrane Potential Decorrelation}} of {{Spontaneous Activity}} in {{Cortex}}},
  volume = {33},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.0347-13.2013},
  doi = {10.1523/JNEUROSCI.0347-13.2013},
  number = {38},
  journaltitle = {Journal of Neuroscience},
  urldate = {2016-07-18},
  date = {2013-09-18},
  pages = {15075--15085},
  author = {Graupner, M. and Reyes, A. D.},
  file = {/home/fh/lib/articles/Graupner2013.pdf}
}

@article{Granot-Atedgi2013,
  langid = {english},
  title = {Stimulus-Dependent {{Maximum Entropy Models}} of {{Neural Population Codes}}},
  volume = {9},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1002922},
  doi = {10.1371/journal.pcbi.1002922},
  number = {3},
  journaltitle = {PLoS Computational Biology},
  urldate = {2016-07-18},
  date = {2013-03-14},
  pages = {e1002922},
  author = {Granot-Atedgi, Einat and Tka{\v c}ik, Ga{\v s}per and Segev, Ronen and Schneidman, Elad},
  editor = {Sporns, Olaf},
  file = {/home/fh/lib/articles/Granot-Atedgi2013.pdf}
}

@article{Koster2014,
  langid = {english},
  title = {Modeling {{Higher}}-{{Order Correlations}} within {{Cortical Microcolumns}}},
  volume = {10},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1003684},
  doi = {10.1371/journal.pcbi.1003684},
  number = {7},
  journaltitle = {PLoS Computational Biology},
  urldate = {2016-07-18},
  date = {2014-07-03},
  pages = {e1003684},
  author = {K{\"o}ster, Urs and Sohl-Dickstein, Jascha and Gray, Charles M. and Olshausen, Bruno A.},
  editor = {Macke, Jakob H.},
  file = {/home/fh/lib/articles/Köster2014.pdf}
}

@article{Pillow2008,
  title = {Spatio-Temporal Correlations and Visual Signalling in a Complete Neuronal Population},
  volume = {454},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/nature07140},
  doi = {10.1038/nature07140},
  number = {7207},
  journaltitle = {Nature},
  urldate = {2016-07-18},
  date = {2008-08-21},
  pages = {995--999},
  author = {Pillow, Jonathan W. and Shlens, Jonathon and Paninski, Liam and Sher, Alexander and Litke, Alan M. and Chichilnisky, E. J. and Simoncelli, Eero P.},
  file = {/home/fh/lib/articles/Pillow2008.pdf}
}

@article{Schneidman2006,
  title = {Weak Pairwise Correlations Imply Strongly Correlated Network States in a Neural Population},
  volume = {440},
  issn = {0028-0836, 1476-4679},
  url = {http://www.nature.com/doifinder/10.1038/nature04701},
  doi = {10.1038/nature04701},
  number = {7087},
  journaltitle = {Nature},
  urldate = {2016-07-18},
  date = {2006-04-20},
  pages = {1007--1012},
  keywords = {_tablet},
  author = {Schneidman, Elad and Berry, Michael J. and Segev, Ronen and Bialek, William},
  file = {/home/fh/lib/articles/Schneidman2006.pdf}
}

@article{Tkacik2015,
  langid = {english},
  title = {Thermodynamics and Signatures of Criticality in a Network of Neurons},
  volume = {112},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/112/37/11508},
  doi = {10.1073/pnas.1514188112},
  abstract = {The activity of a neural network is defined by patterns of spiking and silence from the individual neurons. Because spikes are (relatively) sparse, patterns of activity with increasing numbers of spikes are less probable, but, with more spikes, the number of possible patterns increases. This tradeoff between probability and numerosity is mathematically equivalent to the relationship between entropy and energy in statistical physics. We construct this relationship for populations of up to N = 160 neurons in a small patch of the vertebrate retina, using a combination of direct and model-based analyses of experiments on the response of this network to naturalistic movies. We see signs of a thermodynamic limit, where the entropy per neuron approaches a smooth function of the energy per neuron as N increases. The form of this function corresponds to the distribution of activity being poised near an unusual kind of critical point. We suggest further tests of criticality, and give a brief discussion of its functional significance.},
  number = {37},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  urldate = {2016-07-18},
  date = {2015-09-15},
  pages = {11508--11513},
  keywords = {Correlation,neural networks,entropy,information,Monte Carlo},
  author = {Tka{\v c}ik, Ga{\v s}per and Mora, Thierry and Marre, Olivier and Amodei, Dario and Palmer, Stephanie E. and Berry, Michael J. and Bialek, William},
  file = {/home/fh/lib/articles/Tkačik2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/26688B5E/11508.html},
  eprinttype = {pmid},
  eprint = {26330611}
}

@article{Tkacik2014,
  langid = {english},
  title = {Searching for {{Collective Behavior}} in a {{Large Network}} of {{Sensory Neurons}}},
  volume = {10},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1003408},
  doi = {10.1371/journal.pcbi.1003408},
  number = {1},
  journaltitle = {PLoS Computational Biology},
  urldate = {2016-07-18},
  date = {2014-01-02},
  pages = {e1003408},
  author = {Tka{\v c}ik, Ga{\v s}per and Marre, Olivier and Amodei, Dario and Schneidman, Elad and Bialek, William and Berry, Michael J.},
  editor = {Sporns, Olaf},
  file = {/home/fh/lib/articles/Tkačik2014.pdf}
}

@article{Gollo2014,
  title = {Mechanisms of {{Zero}}-{{Lag Synchronization}} in {{Cortical Motifs}}},
  volume = {10},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003548},
  doi = {10.1371/journal.pcbi.1003548},
  abstract = {Author Summary   Understanding large-scale neuronal dynamics \textendash{} and how they relate to the cortical anatomy \textendash{} is one of the key areas of neuroscience research. Despite a wealth of recent research, the key principles of this relationship have yet to be established. Here we employ computational modeling to study neuronal dynamics on small subgraphs \textendash{} or motifs \textendash{} across a hierarchy of spatial scales. We establish a novel organizing principle that we term a ``resonance pair'' (two mutually coupled nodes), which promotes stable, zero-lag synchrony amongst motif nodes. The bidirectional coupling between a resonance pair acts to mutually adjust their dynamics onto a common and relatively stable synchronized regime, which then propagates and stabilizes the synchronization of other nodes within the motif. Remarkably, we find that this effect can propagate along chains of coupled nodes and hence holds the potential to promote stable zero-lag synchrony in larger sub-networks of cortical systems. Our findings hence suggest a potential unifying account of the existence of zero-lag synchrony, an important phenomenon that may underlie crucial cognitive processes in the brain. Moreover, such pairs of mutually coupled oscillators are found in a wide variety of physical and biological systems suggesting a new, broadly relevant and unifying principle.},
  number = {4},
  journaltitle = {PLOS Comput Biol},
  shortjournal = {PLOS Comput Biol},
  urldate = {2016-07-19},
  date = {2014-04-24},
  pages = {e1003548},
  keywords = {neural networks,Neurons,Action potentials,Network motifs,Synapses,Single neuron function,Membrane potential,Dynamical systems},
  author = {Gollo, Leonardo L. and Mirasso, Claudio and Sporns, Olaf and Breakspear, Michael},
  file = {/home/fh/lib/articles/Gollo2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/F3GUIIHF/article.html}
}

@report{zotero-null-1286,
  title = {Caption},
  file = {/home/fh/lib/manuals/latex/caption.pdf},
  note = {manuals/latex}
}

@book{Buzsaki2006,
  location = {{Oxford ; New York}},
  title = {Rhythms of the Brain},
  isbn = {978-0-19-530106-9 978-0-19-982823-4},
  pagetotal = {448},
  publisher = {{Oxford University Press}},
  date = {2006},
  keywords = {Physiology,Brain,Periodicity,oscillations,Biological rhythms,Cortical Synchronization},
  author = {Buzs{\'a}ki, G.},
  file = {/home/fh/lib/books/Buzsáki2006_Rhythms-of-the-brain.pdf},
  note = {OCLC: ocm63279497}
}

@article{Thomson2002,
  langid = {english},
  title = {Synaptic {{Connections}} and {{Small Circuits Involving Excitatory}} and {{Inhibitory Neurons}} in {{Layers}} 2\textendash{}5 of {{Adult Rat}} and {{Cat Neocortex}}: {{Triple Intracellular Recordings}} and {{Biocytin Labelling In Vitro}}},
  volume = {12},
  issn = {1047-3211, 1460-2199},
  url = {http://cercor.oxfordjournals.org/content/12/9/936},
  doi = {10.1093/cercor/12.9.936},
  shorttitle = {Synaptic {{Connections}} and {{Small Circuits Involving Excitatory}} and {{Inhibitory Neurons}} in {{Layers}} 2\textendash{}5 of {{Adult Rat}} and {{Cat Neocortex}}},
  abstract = {Dual and triple intracellular recordings with biocytin labelling in slices of adult neocortex explored small circuits of synaptically connected neurons. 679 paired recordings in rat and 319 in cat yielded 135 and 42 excitatory postsynaptic potentials (EPSPs) and 37 and 26 inhibitory postsynaptic potentials (IPSPs), respectively. Patterns of connectivity and synaptic properties were similar in the two species, although differences of scale and in the range of morphologies were observed. Excitatory `forward' projections from layer 4 to 3, like those from layer 3 to 5, targeted pyramidal cells and a small proportion of interneurons, while excitatory `back' projections from layer 3 to 4 selected interneurons, including parvalbumin immuno-positive basket cells. Layer 4 interneurons that inhibited layer 3 pyramidal cells included both basket cells and dendrite-targeting cells. Large interneurons, resembling cells previously described as large basket cells, in layers 4 and 3 (cat), with long myelinated horizontal axon collaterals received frequent excitatory inputs from both layers. A very high rate of connectivity was observed between pairs of interneurons, often with quite different morphologies, and the resultant IPSPs, like the EPSPs recorded in interneurons, were brief compared with those recorded in pyramidal and spiny stellate cells.},
  number = {9},
  journaltitle = {Cerebral Cortex},
  shortjournal = {Cereb. Cortex},
  urldate = {2016-08-08},
  date = {2002-01-09},
  pages = {936--953},
  author = {Thomson, Alex M. and West, David C. and Wang, Yun and Bannister, A. Peter},
  file = {/home/fh/lib/articles/Thomson2002.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/K7C4JVMR/936.html},
  eprinttype = {pmid},
  eprint = {12183393}
}

@article{McCulloch1943,
  langid = {english},
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  volume = {5},
  issn = {0007-4985, 1522-9602},
  url = {http://link.springer.com/article/10.1007/BF02478259},
  doi = {10.1007/BF02478259},
  abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  number = {4},
  journaltitle = {The bulletin of mathematical biophysics},
  shortjournal = {Bulletin of Mathematical Biophysics},
  urldate = {2016-08-29},
  date = {1943},
  pages = {115--133},
  author = {McCulloch, Warren S. and Pitts, Walter},
  file = {/home/fh/lib/articles/McCulloch1943.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/EANPFSUI/bf02478259.html}
}

@book{Rojas1996,
  langid = {english},
  location = {{Berlin ; New York}},
  title = {Neural {{Networks}}: {{A Systematic Introduction}}},
  edition = {1 edition},
  isbn = {978-3-540-60505-8},
  shorttitle = {Neural {{Networks}}},
  abstract = {Neural networks are a computing paradigm that is finding increasing attention among computer scientists. In this book, theoretical laws and models previously scattered in the literature are brought together into a general theory of artificial neural nets. Always with a view to biology and starting with the simplest nets, it is shown how the properties of models change when more general computing elements and net topologies are introduced. Each chapter contains examples, numerous illustrations, and a bibliography. The book is aimed at readers who seek an overview of the field or who wish to deepen their knowledge. It is suitable as a basis for university courses in neurocomputing.},
  pagetotal = {502},
  publisher = {{Springer}},
  date = {1996-01-01},
  author = {Rojas, Raul},
  file = {/home/fh/lib/books/Rojas1996_Neural-Networks-A-Systematic-Introduction.pdf}
}

@article{Izhikevich2006,
  langid = {english},
  title = {Polychronization: {{Computation}} with {{Spikes}}},
  volume = {18},
  issn = {0899-7667, 1530-888X},
  url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976606775093882},
  doi = {10.1162/089976606775093882},
  shorttitle = {Polychronization},
  number = {2},
  journaltitle = {Neural Computation},
  urldate = {2016-09-02},
  date = {2006-02},
  pages = {245--282},
  author = {Izhikevich, Eugene M.},
  file = {/home/fh/lib/articles/Izhikevich2006.pdf}
}

@article{Goedeke2008,
  langid = {english},
  title = {The Mechanism of Synchronization in Feed-Forward Neuronal Networks},
  volume = {10},
  issn = {1367-2630},
  url = {http://stacks.iop.org/1367-2630/10/i=1/a=015007},
  doi = {10.1088/1367-2630/10/1/015007},
  abstract = {Synchronization in feed-forward subnetworks of the brain has been proposed to explain the precisely timed spike patterns observed in experiments. While the attractor dynamics of these networks is now well understood, the underlying single neuron mechanisms remain unexplained. Previous attempts have captured the effects of the highly fluctuating membrane potential by relating spike intensity f ( U ) to the instantaneous voltage U generated by the input. This article shows that f is high during the rise and low during the decay of U ( t ), demonstrating that the \#\#IMG\#\# [http://ej.iop.org/images/1367-2630/10/1/015007/nj256198ieqn1.gif] $\backslash$dotU -dependence of f , not refractoriness, is essential for synchronization. Moreover, the bifurcation scenario is quantitatively described by a simple \#\#IMG\#\# [http://ej.iop.org/images/1367-2630/10/1/015007/nj256198ieqn2.gif] f(U,$\backslash$dotU) relationship. These findings suggest \#\#IMG\#\# [http://ej.iop.org/images/1367-2630/10/1/015007/nj256198ieqn3.gif] f(U,$\backslash$dotU) as the relevant model class for the investigation of neural synchronization phenomena in a noisy environment.},
  number = {1},
  journaltitle = {New Journal of Physics},
  shortjournal = {New J. Phys.},
  urldate = {2016-09-02},
  date = {2008},
  pages = {015007},
  author = {Goedeke, S. and Diesmann, M.},
  file = {/home/fh/lib/articles/Goedeke2008.pdf}
}

@article{Stepanyants2004,
  langid = {english},
  title = {Class-{{Specific Features}} of {{Neuronal Wiring}}},
  volume = {43},
  issn = {0896-6273},
  url = {/neuron/abstract/S0896-6273(04)00362-9},
  doi = {10.1016/j.neuron.2004.06.013},
  abstract = {Brain function relies on specificity of synaptic connectivity patterns among different classes of neurons. Yet, the substrates of specificity in complex neuropil remain largely unknown. We search for imprints of specificity in the layout of axonal and dendritic arbors from the rat neocortex. An analysis of 3D reconstructions of pairs consisting of pyramidal cells (PCs) and GABAergic interneurons (GIs) revealed that the layout of GI axons is specific. This specificity is manifested in a relatively high tortuosity, small branch length of these axons, and correlations of their trajectories with the positions of postsynaptic neuron dendrites. Axons of PCs show no such specificity, usually taking a relatively straight course through neuropil. However, wiring patterns among PCs hold a large potential for circuit remodeling and specificity through growth and retraction of dendritic spines. Our results define distinct class-specific rules in establishing synaptic connectivity, which could be crucial in formulating a canonical cortical circuit.},
  number = {2},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2016-09-05},
  date = {2004-07-22},
  pages = {251--259},
  author = {Stepanyants, Armen and Tam{\'a}s, G{\'a}bor and Chklovskii, Dmitri B.},
  file = {/home/fh/lib/articles/Stepanyants2004.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/XIDSEI9T/S0896-6273(04)00362-9.html},
  eprinttype = {pmid},
  eprint = {15260960,\%002015260960}
}

@article{Diesmann1999,
  langid = {english},
  title = {Stable Propagation of Synchronous Spiking in Cortical Neural Networks},
  volume = {402},
  issn = {0028-0836},
  url = {http://www.nature.com/nature/journal/v402/n6761/full/402529a0.html},
  doi = {10.1038/990101},
  abstract = {The classical view of neural coding has emphasized the importance of information carried by the rate at which neurons discharge action potentials. More recent proposals that information may be carried by precise spike timing have been challenged by the assumption that these neurons operate in a noisy fashion\textemdash{}presumably reflecting fluctuations in synaptic input\textemdash{}and, thus, incapable of transmitting signals with millisecond fidelity. Here we show that precisely synchronized action potentials can propagate within a model of cortical network activity that recapitulates many of the features of biological systems. An attractor, yielding a stable spiking precision in the (sub)millisecond range, governs the dynamics of synchronization. Our results indicate that a combinatorial neural code, based on rapid associations of groups of neurons co-ordinating their activity at the single spike level, is possible within a cortical-like network.},
  number = {6761},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2016-09-14},
  date = {1999-12-02},
  pages = {529--533},
  author = {Diesmann, Markus and Gewaltig, Marc-Oliver and Aertsen, Ad},
  file = {/home/fh/lib/articles/Diesmann1999.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/CEHEA4KE/402529a0.html}
}

@book{Raschka2015,
  langid = {english},
  location = {{Birmingham}},
  title = {Python Machine Learning},
  isbn = {978-1-78355-513-0},
  shorttitle = {Python Machine Learning},
  pagetotal = {425},
  series = {Community experience distilled},
  publisher = {{Packt Publishing}},
  date = {2015},
  keywords = {Python,Maschinelles Lernen},
  author = {Raschka, Sebastian},
  file = {/home/fh/lib/books/Raschka2015_Python-machine-learning.pdf},
  note = {OCLC: 927915246}
}

@article{Hoffmann2016,
  title = {Non-Random Network Connectivity Comes in Pairs},
  url = {https://doi.org/10.12751/nncn.bc2016.0079},
  doi = {10.12751/nncn.bc2016.0079},
  urldate = {2016-09-19},
  date = {2016},
  keywords = {poster,me},
  author = {Hoffmann, Felix Z. and Triesch, Jochen},
  file = {/home/fh/lib/articles/Hoffmann_BCCN2016.pdf},
  note = {poster=BCCN}
}

@article{Deger2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.05730},
  primaryClass = {q-bio},
  title = {Multi-Contact Synapses for Stable Networks: A Spike-Timing Dependent Model of Dendritic Spine Plasticity and Turnover},
  url = {http://arxiv.org/abs/1609.05730},
  shorttitle = {Multi-Contact Synapses for Stable Networks},
  abstract = {Excitatory synaptic connections in the adult neocortex consist of multiple synaptic contacts, almost exclusively formed on dendritic spines. Changes of dendritic spine shape and volume, a correlate of synaptic strength, can be tracked in vivo for weeks. Here, we present a combined model of spike-timing dependent dendritic spine plasticity and turnover that explains the steady state multi-contact configuration of synapses in adult neocortical networks. In this model, many presynaptic neurons compete to make strong synaptic connections onto postsynaptic neurons, while the synaptic contacts comprising each connection cooperate via postsynaptic firing. We demonstrate that the model is consistent with experimentally observed long-term dendritic spine dynamics under steady-state and lesion induced conditions, and show that cooperation of multiple synaptic contacts is crucial for stable, long-term synaptic memories. In simulations of a simplified network of barrel cortex, our plasticity rule reproduces whisker-trimming induced rewiring of thalamo-cortical and recurrent synaptic connectivity on realistic time scales.},
  urldate = {2016-09-26},
  date = {2016-09-19},
  keywords = {Quantitative Biology - Neurons and Cognition},
  author = {Deger, Moritz and Seeholzer, Alexander and Gerstner, Wulfram},
  file = {/home/fh/lib/articles/Deger2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/XC5852VA/1609.html}
}

@article{Fauth2016,
  title = {Opposing {{Effects}} of {{Neuronal Activity}} on {{Structural Plasticity}}},
  url = {http://journal.frontiersin.org/article/10.3389/fnana.2016.00075/full},
  doi = {10.3389/fnana.2016.00075},
  abstract = {The connectivity of the brain is continuously adjusted to new environmental influences by several activity-dependent adaptive processes. The most investigated adaptive mechanism is activity-dependent functional or synaptic plasticity regulating the transmission efficacy of existing synapses. Another important but less prominently discussed adaptive process is structural plasticity, which changes the connectivity by the formation and deletion of synapses. In this review, we show, based on experimental evidence, that structural plasticity can be classified similar to synaptic plasticity into two categories: (i) Hebbian structural plasticity, which leads to an increase (decrease) of the number of synapses during phases of high (low) neuronal activity and (ii) homeostatic structural plasticity, which balances these changes by removing and adding synapses. Furthermore, based on experimental and theoretical insights, we argue that each type of structural plasticity fulfills a different function. While Hebbian structural changes enhance memory lifetime, storage capacity, and memory robustness, homeostatic structural plasticity self-organizes the connectivity of the neural network to assure stability. However, the link between functional synaptic and structural plasticity as well as the detailed interactions between Hebbian and homeostatic structural plasticity are more complex. This implies even richer dynamics requiring further experimental and theoretical investigations.},
  journaltitle = {Frontiers in Neuroanatomy},
  shortjournal = {Front. Neuroanat},
  urldate = {2016-09-26},
  date = {2016},
  pages = {75},
  keywords = {synaptic plasticity,structural plasticity,architectural plasticity,timescales,network topology},
  author = {Fauth, Michael and Tetzlaff, Christian},
  file = {/home/fh/lib/articles/Fauth2016.pdf}
}

@article{Tetzlaff2011,
  title = {Synaptic Scaling in Combination with Many Generic Plasticity Mechanisms Stabilizes Circuit Connectivity},
  volume = {5},
  url = {http://journal.frontiersin.org/article/10.3389/fncom.2011.00047/full},
  doi = {10.3389/fncom.2011.00047},
  abstract = {Synaptic scaling is a slow process that modifies synapses, keeping the firing rate of neural circuits in specific regimes. Together with other processes, such as conventional synaptic plasticity in the form of long term depression and potentiation, synaptic scaling changes the synaptic patterns in a network, ensuring diverse, functionally relevant, stable, and input-dependent connectivity. How synaptic patterns are generated and stabilized, however, is largely unknown. Here we formally describe and analyze synaptic scaling based on results from experimental studies and demonstrate that the combination of different conventional plasticity mechanisms and synaptic scaling provides a powerful general framework for regulating network connectivity. In addition, we design several simple models that reproduce experimentally observed synaptic distributions as well as the observed synaptic modifications during sustained activity changes. These models predict that the combination of plasticity with scaling generates globally stable, input-controlled synaptic patterns, also in recurrent networks. Thus, in combination with other forms of plasticity, synaptic scaling can robustly yield neuronal circuits with high synaptic diversity, which potentially enables robust dynamic storage of complex activation patterns. This mechanism is even more pronounced when considering networks with a realistic degree of inhibition. Synaptic scaling combined with plasticity could thus be the basis for learning structured behavior even in initially random networks.},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci.},
  urldate = {2016-09-26},
  date = {2011},
  pages = {47},
  keywords = {plasticity,neural network,homeostasis,synapse},
  author = {Tetzlaff, Christian and Kolodziejski, Christoph and Timme, Marc and W{\"o}rg{\"o}tter, Florentin},
  file = {/home/fh/lib/articles/Tetzlaff2011.pdf}
}

@article{Rumpel2016,
  title = {The Dynamic Connectome: Keeping the Balance},
  date = {2016},
  author = {Rumpel, Simon and Kaschube, Matthias and Triesch, Jochen},
  file = {/home/fh/lib/articles/Rumpel2016.pdf},
  note = {project\_proposal}
}

@article{Kesten1973,
  langid = {english},
  title = {Random Difference Equations and {{Renewal}} Theory for Products of Random Matrices},
  volume = {131},
  issn = {0001-5962, 1871-2509},
  url = {http://link.springer.com/article/10.1007/BF02392040},
  doi = {10.1007/BF02392040},
  number = {1},
  journaltitle = {Acta Mathematica},
  shortjournal = {Acta Math.},
  urldate = {2016-09-30},
  date = {1973},
  pages = {207--248},
  author = {Kesten, Harry},
  file = {/home/fh/lib/articles/Kesten1973.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/2Q2SX8TG/10.html}
}

@book{Sornette2006,
  location = {{Berlin ; New York}},
  title = {Critical Phenomena in Natural Sciences: Chaos, Fractals, Selforganization, and Disorder: Concepts and Tools},
  edition = {2nd ed},
  isbn = {978-3-540-30882-9},
  shorttitle = {Critical Phenomena in Natural Sciences},
  pagetotal = {528},
  series = {Springer series in synergetics},
  publisher = {{Springer}},
  date = {2006},
  keywords = {Critical phenomena (Physics)},
  author = {Sornette, Didier},
  file = {/home/fh/lib/books/Sornette2006_Critical-phenomena-in-natural-sciences-chaos,-fractals,-selforganization,-and-disorder-concepts-and-tools.pdf},
  note = {OCLC: ocm67829893}
}

@article{Goldie1991,
  eprinttype = {jstor},
  eprint = {2959629},
  title = {Implicit {{Renewal Theory}} and {{Tails}} of {{Solutions}} of {{Random Equations}}},
  volume = {1},
  issn = {1050-5164},
  abstract = {For the solutions of certain random equations, or equivalently the stationary solutions of certain random recurrences, the distribution tails are evaluated by renewal-theoretic methods. Six such equations, including one arising in queueing theory, are studied in detail. Implications in extreme-value theory are discussed by way of an illustration from economics.},
  number = {1},
  journaltitle = {The Annals of Applied Probability},
  shortjournal = {The Annals of Applied Probability},
  date = {1991},
  pages = {126--166},
  author = {Goldie, Charles M.},
  file = {/home/fh/lib/articles/Goldie1991.pdf}
}

@article{Statman2014,
  title = {Synaptic {{Size Dynamics}} as an {{Effectively Stochastic Process}}},
  volume = {10},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003846},
  doi = {10.1371/journal.pcbi.1003846},
  abstract = {Author Summary   Synapses are specialized sites of cell\textendash{}cell contact that serve to transmit signals between neurons and their targets, most commonly other neurons. It is widely believed that changes in synaptic properties, driven by prior activity or by other physiological signals, represent a major cellular mechanism by which neuronal networks are modified. Recent experiments show that in addition to directed changes, synaptic sizes also change spontaneously, with dynamics that seem to have strong stochastic components. In spite of these dynamics, however, population distributions of synaptic sizes are remarkably stable, and scale smoothly in response to various perturbations. In this study we show that fundamental aspects of synapse size dynamics are captured remarkably well by a simple statistical model known as the Kesten process: the random-like nature of synaptic size changes; the stability and shape of synaptic size distributions; their scaling following various perturbations; and the kinetics of new synapse formation. These findings indicate that the multiple microscopic processes involved in determining synaptic size combine in such a way that their collective behavior buffers many of the underlying details. The simplicity of the model and its robustness provide a new route for understanding the emergence of invariants at the level of the synaptic population.},
  number = {10},
  journaltitle = {PLOS Comput Biol},
  shortjournal = {PLOS Comput Biol},
  urldate = {2016-10-13},
  date = {2014-10-02},
  pages = {e1003846},
  keywords = {neural networks,Neurons,Synapses,Stochastic processes,Population dynamics,Statistical distributions,Statistical models,Random variables},
  author = {Statman, Adiel and Kaufman, Maya and Minerbi, Amir and Ziv, Noam E. and Brenner, Naama},
  file = {/home/fh/lib/articles/Statman2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/2IRS9226/article.html}
}

@article{Gillespie1996,
  title = {Exact Numerical Simulation of the {{Ornstein}}-{{Uhlenbeck}} Process and Its Integral},
  volume = {54},
  url = {http://link.aps.org/doi/10.1103/PhysRevE.54.2084},
  doi = {10.1103/PhysRevE.54.2084},
  abstract = {A numerical simulation algorithm that is exact for any time step $\Delta$t$>$0 is derived for the Ornstein-Uhlenbeck process X(t) and its time integral Y(t). The algorithm allows one to make efficient, unapproximated simulations of, for instance, the velocity and position components of a particle undergoing Brownian motion, and the electric current and transported charge in a simple R-L circuit, provided appropriate values are assigned to the Ornstein-Uhlenbeck relaxation time $\tau$ and diffusion constant c. A simple Taylor expansion in $\Delta$t of the exact simulation formulas shows how the first-order simulation formulas, which are implicit in the Langevin equation for X(t) and the defining equation for Y(t), are modified in second order. The exact simulation algorithm is used here to illustrate the zero-$\tau$ limit theorem. \textcopyright{} 1996 The American Physical Society.},
  number = {2},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  urldate = {2016-10-13},
  date = {1996-08-01},
  pages = {2084--2091},
  author = {Gillespie, Daniel T.},
  file = {/home/fh/lib/articles/Gillespie1996.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/FBW3A83N/PhysRevE.54.html}
}

@article{Hartmann2016,
  title = {Precise {{Synaptic Efficacy Alignment Suggests Potentiation Dominated Learning}}},
  volume = {9},
  issn = {1662-5110},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4711154/},
  doi = {10.3389/fncir.2015.00090},
  abstract = {Recent evidence suggests that parallel synapses from the same axonal branch onto the same dendritic branch have almost identical strength. It has been proposed that this alignment is only possible through learning rules that integrate activity over long time spans. However, learning mechanisms such as spike-timing-dependent plasticity (STDP) are commonly assumed to be temporally local. Here, we propose that the combination of temporally local STDP and a multiplicative synaptic normalization mechanism is sufficient to explain the alignment of parallel synapses. To address this issue, we introduce three increasingly complex models: First, we model the idealized interaction of STDP and synaptic normalization in a single neuron as a simple stochastic process and derive analytically that the alignment effect can be described by a so-called Kesten process. From this we can derive that synaptic efficacy alignment requires potentiation-dominated learning regimes. We verify these conditions in a single-neuron model with independent spiking activities but more realistic synapses. As expected, we only observe synaptic efficacy alignment for long-term potentiation-biased STDP. Finally, we explore how well the findings transfer to recurrent neural networks where the learning mechanisms interact with the correlated activity of the network. We find that due to the self-reinforcing correlations in recurrent circuits under STDP, alignment occurs for both long-term potentiation- and depression-biased STDP, because the learning will be potentiation dominated in both cases due to the potentiating events induced by correlated activity. This is in line with recent results demonstrating a dominance of potentiation over depression during waking and normalization during sleep. This leads us to predict that individual spine pairs will be more similar after sleep compared to after sleep deprivation. In conclusion, we show that synaptic normalization in conjunction with coordinated potentiation\textemdash{}in this case, from STDP in the presence of correlated pre- and post-synaptic activity\textemdash{}naturally leads to an alignment of parallel synapses.},
  journaltitle = {Frontiers in Neural Circuits},
  shortjournal = {Front Neural Circuits},
  urldate = {2016-10-17},
  date = {2016-01-13},
  author = {Hartmann, Christoph and Miner, Daniel C. and Triesch, Jochen},
  file = {/home/fh/lib/articles/Hartmann2016.pdf}
}

@article{Lazar2009,
  title = {{{SORN}}: A Self-Organizing Recurrent Neural Network},
  volume = {3},
  url = {http://journal.frontiersin.org/article/10.3389/neuro.10.023.2009/full},
  doi = {10.3389/neuro.10.023.2009},
  shorttitle = {{{SORN}}},
  abstract = {Understanding the dynamics of recurrent neural networks is crucial for explaining how the brain processes information. In the neocortex, a range of different plasticity mechanisms are shaping recurrent networks into effective information processing circuits that learn appropriate representations for time-varying sensory stimuli. However, it has been difficult to mimic these abilities in artificial neural network models. Here we introduce SORN, a self-organizing recurrent network. It combines three distinct forms of local plasticity to learn spatio-temporal patterns in its input while maintaining its dynamics in a healthy regime suitable for learning. The SORN learns to encode information in the form of trajectories through its high-dimensional state space reminiscent of recent biological findings on cortical coding. All three forms of plasticity are shown to be essential for the network's success.},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci.},
  urldate = {2016-10-20},
  date = {2009},
  pages = {23},
  keywords = {synaptic plasticity,intrinsic plasticity,recurrent neural networks,reservoir computing,time series prediction},
  author = {Lazar, Andreea and Pipa, Gordon and Triesch, Jochen and Lazar, Andreea and Pipa, Gordon and Triesch, Jochen},
  file = {/home/fh/lib/articles/Lazar2009.pdf}
}

@article{Abbott2000,
  langid = {english},
  title = {Synaptic Plasticity: Taming the Beast},
  volume = {3},
  url = {http://www.nature.com/neuro/journal/v3/n11s/full/nn1100_1178.html},
  doi = {10.1038/81453},
  shorttitle = {Synaptic Plasticity},
  abstract = {Synaptic plasticity provides the basis for most models of learning, memory and development in neural circuits. To generate realistic results, synapse-specific Hebbian forms of plasticity, such as long-term potentiation and depression, must be augmented by global processes that regulate overall levels of neuronal and network activity. Regulatory processes are often as important as the more intensively studied Hebbian processes in determining the consequences of synaptic plasticity for network function. Recent experimental results suggest several novel mechanisms for regulating levels of activity in conjunction with Hebbian synaptic modification. We review three of them\textemdash{}synaptic scaling, spike-timing dependent plasticity and synaptic redistribution\textemdash{}and discuss their functional implications.},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2016-10-24},
  date = {2000-11-01},
  pages = {1178--1183},
  author = {Abbott, L. F. and Nelson, Sacha B.},
  file = {/home/fh/lib/articles/Abbott2000.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/XCP54DRA/nn1100_1178.html}
}

@article{Desai1999,
  langid = {english},
  title = {Plasticity in the Intrinsic Excitability of Cortical Pyramidal Neurons},
  volume = {2},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v2/n6/full/nn0699_515.html},
  doi = {10.1038/9165},
  abstract = {During learning and development, the level of synaptic input received by cortical neurons may change dramatically. Given a limited range of possible firing rates, how do neurons maintain responsiveness to both small and large synaptic inputs? We demonstrate that in response to changes in activity, cultured cortical pyramidal neurons regulate intrinsic excitability to promote stability in firing. Depriving pyramidal neurons of activity for two days increased sensitivity to current injection by selectively regulating voltage-dependent conductances. This suggests that one mechanism by which neurons maintain sensitivity to different levels of synaptic input is by altering the function relating current to firing rate.},
  number = {6},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2016-10-24},
  date = {1999-06},
  pages = {515--520},
  author = {Desai, Niraj S. and Rutherford, Lana C. and Turrigiano, Gina G.},
  file = {/home/fh/lib/articles/Desai1999.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/W3TT2CIC/nn0699_515.html}
}

@article{Yasumatsu2008,
  langid = {english},
  title = {Principles of {{Long}}-{{Term Dynamics}} of {{Dendritic Spines}}},
  volume = {28},
  issn = {0270-6474, 1529-2401},
  url = {http://jneurosci.org/content/28/50/13592},
  doi = {10.1523/JNEUROSCI.0603-08.2008},
  abstract = {Long-term potentiation of synapse strength requires enlargement of dendritic spines on cerebral pyramidal neurons. Long-term depression is linked to spine shrinkage. Indeed, spines are dynamic structures: they form, change their shapes and volumes, or can disappear in the space of hours. Do all such changes result from synaptic activity, or do some changes result from intrinsic processes? How do enlargement and shrinkage of spines relate to elimination and generation of spines, and how do these processes contribute to the stationary distribution of spine volumes? To answer these questions, we recorded the volumes of many individual spines daily for several days using two-photon imaging of CA1 pyramidal neurons in cultured slices of rat hippocampus between postnatal days 17 and 23. With normal synaptic transmission, spines often changed volume or were created or eliminated, thereby showing activity-dependent plasticity. However, we found that spines changed volume even after we blocked synaptic activity, reflecting a native instability of these small structures over the long term. Such ``intrinsic fluctuations'' showed unique dependence on spine volume. A mathematical model constructed from these data and the theory of random fluctuations explains population behaviors of spines, such as rates of elimination and generation, stationary distribution of volumes, and the long-term persistence of large spines. Our study finds that generation and elimination of spines are more prevalent than previously believed, and spine volume shows significant correlation with its age and life expectancy. The population dynamics of spines also predict key psychological features of memory.},
  number = {50},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2016-10-25},
  date = {2008-12-10},
  pages = {13592--13608},
  keywords = {synaptic plasticity,Hippocampus,Memory,dendritic spine,slice culture,NMDA receptors},
  author = {Yasumatsu, Nobuaki and Matsuzaki, Masanori and Miyazaki, Takashi and Noguchi, Jun and Kasai, Haruo},
  file = {/home/fh/lib/articles/Yasumatsu2008.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/VSF3NUEB/13592.html},
  eprinttype = {pmid},
  eprint = {19074033}
}

@article{Haas2006,
  langid = {english},
  title = {Spike-{{Timing}}-{{Dependent Plasticity}} of {{Inhibitory Synapses}} in the {{Entorhinal Cortex}}},
  volume = {96},
  issn = {0022-3077, 1522-1598},
  url = {http://jn.physiology.org/content/96/6/3305},
  doi = {10.1152/jn.00551.2006},
  abstract = {Actions of inhibitory interneurons organize and modulate many neuronal processes, yet the mechanisms and consequences of plasticity of inhibitory synapses remain poorly understood. We report on spike-timing-dependent plasticity of inhibitory synapses in the entorhinal cortex. After pairing presynaptic stimulations at time tpre with evoked postsynaptic spikes at time tpost under pharmacological blockade of excitation we found, via whole cell recordings, an asymmetrical timing rule for plasticity of the remaining inhibitory responses. Strength of response varied as a function of the time interval $\Delta$t = tpost - tpre: for $\Delta$t $>$ 0 inhibitory responses potentiated, peaking at a delay of 10 ms. For $\Delta$t $<$ 0, the synaptic coupling depressed, again with a maximal effect near 10 ms of delay. We also show that changes in synaptic strength depend on changes in intracellular calcium concentrations and demonstrate that the calcium enters the postsynaptic cell through voltage-gated channels. Using network models, we demonstrate how this novel form of plasticity can sculpt network behavior efficiently and with remarkable flexibility.},
  number = {6},
  journaltitle = {Journal of Neurophysiology},
  urldate = {2016-10-25},
  date = {2006-12-01},
  pages = {3305--3313},
  author = {Haas, Julie S. and Nowotny, Thomas and Abarbanel, H. D. I.},
  file = {/home/fh/lib/articles/Haas2006.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/77X7WVSU/3305.html},
  eprinttype = {pmid},
  eprint = {16928795}
}

@article{Loewenstein2015,
  langid = {english},
  title = {Predicting the {{Dynamics}} of {{Network Connectivity}} in the {{Neocortex}}},
  volume = {35},
  issn = {0270-6474, 1529-2401},
  url = {http://jneurosci.org/content/35/36/12535},
  doi = {10.1523/JNEUROSCI.2917-14.2015},
  abstract = {Dynamic remodeling of connectivity is a fundamental feature of neocortical circuits. Unraveling the principles underlying these dynamics is essential for the understanding of how neuronal circuits give rise to computations. Moreover, as complete descriptions of the wiring diagram in cortical tissues are becoming available, deciphering the dynamic elements in these diagrams is crucial for relating them to cortical function. Here, we used chronic in vivo two-photon imaging to longitudinally follow a few thousand dendritic spines in the mouse auditory cortex to study the determinants of these spines' lifetimes. We applied nonlinear regression to quantify the independent contribution of spine age and several morphological parameters to the prediction of the future survival of a spine. We show that spine age, size, and geometry are parameters that can provide independent contributions to the prediction of the longevity of a synaptic connection. In addition, we use this framework to emulate a serial sectioning electron microscopy experiment and demonstrate how incorporation of morphological information of dendritic spines from a single time-point allows estimation of future connectivity states. The distinction between predictable and nonpredictable connectivity changes may be used in the future to identify the specific adaptations of neuronal circuits to environmental changes. The full dataset is publicly available for further analysis.
SIGNIFICANCE STATEMENT The neural architecture in the neocortex exhibits constant remodeling. The functional consequences of these modifications are poorly understood, in particular because the determinants of these changes are largely unknown. Here, we aimed to identify those modifications that are predictable from current network state. To that goal, we repeatedly imaged thousands of dendritic spines in the auditory cortex of mice to assess the morphology and lifetimes of synaptic connections. We developed models based on morphological features of dendritic spines that allow predicting future turnover of synaptic connections. The dynamic models presented in this paper provide a quantitative framework for adding putative temporal dynamics to the static description of a neuronal circuit from single time-point connectomics experiments.},
  number = {36},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2016-10-25},
  date = {2015-09-09},
  pages = {12535--12544},
  keywords = {Connectome,dendritic spines,synaptic dynamics},
  author = {Loewenstein, Yonatan and Yanover, Uri and Rumpel, Simon},
  file = {/home/fh/lib/articles/Loewenstein2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/C35U9SIA/12535.html},
  eprinttype = {pmid},
  eprint = {26354919}
}

@article{Hoffmann2016a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.04245},
  primaryClass = {q-bio},
  title = {Non-Random Network Connectivity Comes in Pairs},
  url = {http://arxiv.org/abs/1609.04245},
  abstract = {Overrepresentation of bidirectional connections in local cortical networks has been repeatedly reported and is in the focus of the ongoing discussion of non-random connectivity. Here we show in a brief mathematical analysis that in a network in which connection probabilities are symmetric in pairs, \$P\_\{ij\} = P\_\{ji\}\$, the occurrence of bidirectional connections and non-random structures are inherently linked; an overabundance of reciprocally connected pairs emerges necessarily when the network structure deviates from a random network in any form.},
  urldate = {2016-11-03},
  date = {2016-09-14},
  keywords = {Quantitative Biology - Neurons and Cognition},
  author = {Hoffmann, Felix Z. and Triesch, Jochen},
  file = {/home/fh/lib/articles/Hoffmann2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/V5F7EF2K/1609.html}
}

@article{Kurth-Nelson2016,
  title = {Fast {{Sequences}} of {{Non}}-Spatial {{State Representations}} in {{Humans}}},
  volume = {91},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627316302070},
  doi = {10.1016/j.neuron.2016.05.028},
  abstract = {Summary
Fast internally generated sequences of neural representations are suggested to support learning and online planning. However, these sequences have only been studied in the context of spatial tasks and never in humans. Here, we recorded magnetoencephalography (MEG) while human subjects performed a novel non-spatial reasoning task. The task required selecting paths through a set of six visual objects. We trained pattern classifiers on the MEG activity elicited by direct presentation of the visual objects alone and tested these classifiers on activity recorded during periods when no object was presented. During these object-free periods, the brain spontaneously visited representations of approximately four objects in fast sequences lasting on the order of 120~ms. These sequences followed backward trajectories along the permissible paths in the task. Thus, spontaneous fast sequential representation of states can be measured non-invasively in humans, and these sequences may be a fundamental feature of neural computation across tasks.},
  number = {1},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2016-11-07},
  date = {2016-07-06},
  pages = {194--204},
  author = {Kurth-Nelson, Zeb and Economides, Marcos and Dolan, Raymond J. and Dayan, Peter},
  file = {/home/fh/lib/articles/Kurth-Nelson2016.pdf}
}

@inproceedings{Ceolini2016,
  title = {Temporal Sequence Recognition in a Self-Organizing Recurrent Network},
  doi = {10.1109/EBCCSP.2016.7605258},
  abstract = {A big challenge of reservoir-based Recurrent Neural Networks (RNNs) is the optimization of the connection weights within the network so that the network performance is optimal for the intended task of temporal sequence recognition. One particular RNN called the Self-Organizing Recurrent Network (SORN) avoids the mathematical normalization required after each initialization. Instead, three types of cortical plasticity mechanisms optimize the weights within the network during the initial part of the training. The success of this unsupervised training method was demonstrated on temporal sequences that use input symbols with a binary encoding and that activate only one input pool in each time step. This work extends the analysis towards different types of symbol encoding ranging from encoding methods that activate multiple input pools and that use encoding levels that are not strictly binary but analog in nature. Preliminary results show that the SORN model is able to classify well temporal sequences with symbols using these encoding methods and the advantages of this network over a static network in a classification task is still retained.},
  eventtitle = {2016 {{Second International Conference}} on {{Event}}-Based {{Control}}, {{Communication}}, and {{Signal Processing}} ({{EBCCSP}})},
  booktitle = {2016 {{Second International Conference}} on {{Event}}-Based {{Control}}, {{Communication}}, and {{Signal Processing}} ({{EBCCSP}})},
  date = {2016-06},
  pages = {1--4},
  keywords = {Neurons,Statistics,Brain modeling,Encoding,Sociology,Sparse matrices,Training},
  author = {Ceolini, E. and Neil, D. and Delbruck, T. and Liu, S. C.},
  file = {/home/fh/lib/conferences/Ceolini2016_Temporal-sequence-recognition-in-a-self-organizing-recurrent-network.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/9MXR65T5/7605258.html}
}

@article{Fauth2015,
  langid = {english},
  title = {Formation and {{Maintenance}} of {{Robust Long}}-{{Term Information Storage}} in the {{Presence}} of {{Synaptic Turnover}}},
  volume = {11},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1004684},
  doi = {10.1371/journal.pcbi.1004684},
  number = {12},
  journaltitle = {PLOS Computational Biology},
  urldate = {2016-11-08},
  date = {2015-12-29},
  pages = {e1004684},
  author = {Fauth, Michael and W{\"o}rg{\"o}tter, Florentin and Tetzlaff, Christian},
  editor = {Beck, Jeff},
  file = {/home/fh/lib/articles/Fauth2015_2.pdf}
}

@article{Dvorkin2016,
  langid = {english},
  title = {Relative {{Contributions}} of {{Specific Activity Histories}} and {{Spontaneous Processes}} to {{Size Remodeling}} of {{Glutamatergic Synapses}}},
  volume = {14},
  issn = {1545-7885},
  url = {http://dx.plos.org/10.1371/journal.pbio.1002572},
  doi = {10.1371/journal.pbio.1002572},
  number = {10},
  journaltitle = {PLOS Biology},
  urldate = {2016-11-08},
  date = {2016-10-24},
  pages = {e1002572},
  author = {Dvorkin, Roman and Ziv, Noam E.},
  editor = {Schuman, Erin M},
  file = {/home/fh/lib/articles/Dvorkin2016.PDF}
}

@book{Nelsen2006,
  location = {{New York}},
  title = {An Introduction to Copulas},
  edition = {2nd ed},
  isbn = {978-0-387-28659-4},
  pagetotal = {269},
  series = {Springer series in statistics},
  publisher = {{Springer}},
  date = {2006},
  keywords = {Copulas (Mathematical statistics)},
  author = {Nelsen, Roger B.},
  file = {/home/fh/lib/books/Nelsen2006_An-introduction-to-copulas.pdf}
}

@article{Howe2016,
  title = {Rapid Signalling in Distinct Dopaminergic Axons during Locomotion and Reward},
  volume = {535},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/nature18942},
  doi = {10.1038/nature18942},
  number = {7613},
  journaltitle = {Nature},
  urldate = {2016-11-21},
  date = {2016-07-11},
  pages = {505--510},
  author = {Howe, M. W. and Dombeck, D. A.},
  file = {/home/fh/lib/articles/Howe2016.pdf}
}

@article{Hofer2009,
  langid = {english},
  title = {Experience Leaves a Lasting Structural Trace in Cortical Circuits},
  volume = {457},
  issn = {0028-0836},
  url = {http://www.nature.com/nature/journal/v457/n7227/full/nature07487.html},
  doi = {10.1038/nature07487},
  abstract = {Sensory experiences exert a powerful influence on the function and future performance of neuronal circuits in the mammalian neocortex. Restructuring of synaptic connections is believed to be one mechanism by which cortical circuits store information about the sensory world. Excitatory synaptic structures, such as dendritic spines, are dynamic entities that remain sensitive to alteration of sensory input throughout life. It remains unclear, however, whether structural changes at the level of dendritic spines can outlast the original experience and thereby provide a morphological basis for long-term information storage. Here we follow spine dynamics on apical dendrites of pyramidal neurons in functionally defined regions of adult mouse visual cortex during plasticity of eye-specific responses induced by repeated closure of one eye (monocular deprivation). The first monocular deprivation episode doubled the rate of spine formation, thereby increasing spine density. This effect was specific to layer-5 cells located in binocular cortex, where most neurons increase their responsiveness to the non-deprived eye. Restoring binocular vision returned spine dynamics to baseline levels, but absolute spine density remained elevated and many monocular deprivation-induced spines persisted during this period of functional recovery. However, spine addition did not increase again when the same eye was closed for a second time. This absence of structural plasticity stands out against the robust changes of eye-specific responses that occur even faster after repeated deprivation. Thus, spines added during the first monocular deprivation experience may provide a structural basis for subsequent functional shifts. These results provide a strong link between functional plasticity and specific synaptic rearrangements, revealing a mechanism of how prior experiences could be stored in cortical circuits.},
  number = {7227},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2016-12-14},
  date = {2009-01-15},
  pages = {313--317},
  author = {Hofer, Sonja B. and Mrsic-Flogel, Thomas D. and Bonhoeffer, Tobias and H{\"u}bener, Mark},
  file = {/home/fh/lib/articles/Hofer2009.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/ASXF9CQ6/nature07487.html}
}

@unpublished{Hoffmann2009d,
  title = {Linear {{Algebra}} - {{Basis}} of a Vector Space},
  date = {2009},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2009_Linear-Algebra---Basis-of-a-vector-space.pdf}
}

@unpublished{Hoffmann2009e,
  title = {Analysis - {{Zusammenfassung}}},
  date = {2009},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2009_Analysis---Zusammenfassung.pdf}
}

@unpublished{Hoffmann2009f,
  title = {Analysis - {{Mastercards}}},
  date = {2009},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2009_Analysis---Mastercards.pdf}
}

@unpublished{Hoffmann2011,
  title = {Linear {{Algebra}} - Script},
  date = {2011},
  author = {Hoffmann, Felix},
  file = {/home/fh/lib/manuscripts/Hoffmann2011_Linear-Algebra---script.pdf}
}

@article{Mandali2016,
  title = {Electrode {{Position}} and {{Current Amplitude Modulate Impulsivity}} after {{Subthalamic Stimulation}} in {{Parkinsons Disease}}\textemdash{}{{A Computational Study}}},
  volume = {7},
  issn = {1664-042X},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5126055/},
  doi = {10.3389/fphys.2016.00585},
  abstract = {Background: Subthalamic Nucleus Deep Brain Stimulation (STN-DBS) is highly effective in alleviating motor symptoms of Parkinson's disease (PD) which are not optimally controlled by dopamine replacement therapy. Clinical studies and reports suggest that STN-DBS may result in increased impulsivity and de novo impulse control disorders (ICD)., Objective/Hypothesis: We aimed to compare performance on a decision making task, the Iowa Gambling Task (IGT), in healthy conditions (HC), untreated and medically-treated PD conditions with and without STN stimulation. We hypothesized that the position of electrode and stimulation current modulate impulsivity after STN-DBS., Methods: We built a computational spiking network model of basal ganglia (BG) and compared the model's STN output with STN activity in PD. Reinforcement learning methodology was applied to simulate IGT performance under various conditions of dopaminergic and STN stimulation where IGT total and bin scores were compared among various conditions., Results: The computational model reproduced neural activity observed in normal and PD conditions. Untreated and medically-treated PD conditions had lower total IGT scores (higher impulsivity) compared to HC (P $<$ 0.0001). The electrode position that happens to selectively stimulate the part of the STN corresponding to an advantageous panel on IGT resulted in de-selection of that panel and worsening of performance (P $<$ 0.0001). Supratherapeutic stimulation amplitudes also worsened IGT performance (P $<$ 0.001)., Conclusion(s): In our computational model, STN stimulation led to impulsive decision making in IGT in PD condition. Electrode position and stimulation current influenced impulsivity which may explain the variable effects of STN-DBS reported in patients.},
  journaltitle = {Frontiers in Physiology},
  shortjournal = {Front Physiol},
  urldate = {2016-12-20},
  date = {2016-11-29},
  author = {Mandali, Alekhya and Chakravarthy, V. Srinivasa and Rajan, Roopa and Sarma, Sankara and Kishore, Asha},
  file = {/home/fh/lib/articles/Mandali2016.pdf}
}

@article{Tsuchiya2016,
  title = {Using Category Theory to Assess the Relationship between Consciousness and Integrated Information Theory},
  volume = {107},
  issn = {0168-0102},
  url = {http://www.sciencedirect.com/science/article/pii/S0168010215002989},
  doi = {10.1016/j.neures.2015.12.007},
  abstract = {One of the most mysterious phenomena in science is the nature of conscious experience. Due to its subjective nature, a reductionist approach is having a hard time in addressing some fundamental questions about consciousness. These questions are squarely and quantitatively tackled by a recently developed theoretical framework, called integrated information theory (IIT) of consciousness. In particular, IIT proposes that a maximally irreducible conceptual structure (MICS) is identical to conscious experience. However, there has been no principled way to assess the claimed identity. Here, we propose to apply a mathematical formalism, category theory, to assess the proposed identity and suggest that it is important to consider if there exists a proper translation between the domain of conscious experience and that of the MICS. If such translation exists, we postulate that questions in one domain can be answered in the other domain; very difficult questions in the domain of consciousness can be resolved in the domain of mathematics. We claim that it is possible to empirically test if such a functor exists, by using a combination of neuroscientific and computational approaches. Our general, principled and empirical framework allows us to assess the relationship between the domain of consciousness and the domain of mathematical structures, including those suggested by IIT.},
  journaltitle = {Neuroscience Research},
  shortjournal = {Neuroscience Research},
  urldate = {2016-12-22},
  date = {2016-06},
  pages = {1--7},
  keywords = {Consciousness,Qualia,Category theory,Integrated information theory,Phenomenology,Equivalence},
  author = {Tsuchiya, Naotsugu and Taguchi, Shigeru and Saigo, Hayato},
  file = {/home/fh/lib/articles/Tsuchiya2016.pdf}
}

@book{Murphy2012,
  location = {{Cambridge, MA}},
  title = {Machine Learning: A Probabilistic Perspective},
  isbn = {978-0-262-01802-9},
  shorttitle = {Machine Learning},
  pagetotal = {1067},
  series = {Adaptive computation and machine learning series},
  publisher = {{MIT Press}},
  date = {2012},
  keywords = {Machine learning,Probabilities},
  author = {Murphy, Kevin P.},
  file = {/home/fh/lib/books/Murphy2012_Machine-learning-a-probabilistic-perspective.pdf}
}

@collection{Emoto2016,
  langid = {english},
  location = {{Tokyo}},
  title = {Dendrites},
  isbn = {978-4-431-56048-7 978-4-431-56050-0},
  url = {http://link.springer.com/10.1007/978-4-431-56050-0},
  publisher = {{Springer Japan}},
  urldate = {2016-12-26},
  date = {2016},
  editor = {Emoto, Kazuo and Wong, Rachel and Huang, Eric and Hoogenraad, Casper},
  file = {/home/fh/lib/books/Emoto2016_Dendrites.pdf}
}

@book{Pearl2000,
  location = {{Cambridge, U.K. ; New York}},
  title = {Causality: Models, Reasoning, and Inference},
  isbn = {978-0-521-89560-6 978-0-521-77362-1},
  shorttitle = {Causality},
  pagetotal = {384},
  publisher = {{Cambridge University Press}},
  date = {2000},
  keywords = {Probabilities,Causation},
  author = {Pearl, Judea},
  file = {/home/fh/lib/books/Pearl2000_Causality-models,-reasoning,-and-inference.epub;/home/fh/lib/books/Pearl2000_Causality-models,-reasoning,-and-inference.pdf}
}

@article{Rosenbaum2016,
  title = {The Spatial Structure of Correlated Neuronal Variability},
  volume = {20},
  issn = {1097-6256, 1546-1726},
  url = {http://www.nature.com/doifinder/10.1038/nn.4433},
  doi = {10.1038/nn.4433},
  number = {1},
  journaltitle = {Nature Neuroscience},
  urldate = {2017-01-02},
  date = {2016-10-31},
  pages = {107--114},
  author = {Rosenbaum, Robert and Smith, Matthew A and Kohn, Adam and Rubin, Jonathan E and Doiron, Brent},
  file = {/home/fh/lib/articles/Rosenbaum2016_2.pdf;/home/fh/lib/articles/Rosenbaum2016.pdf}
}

@article{Rolls2013,
  title = {The Mechanisms for Pattern Completion and Pattern Separation in the Hippocampus},
  volume = {7},
  issn = {1662-5137},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3812781/},
  doi = {10.3389/fnsys.2013.00074},
  abstract = {The mechanisms for pattern completion and pattern separation are described in the context of a theory of hippocampal function in which the hippocampal CA3 system operates as a single attractor or autoassociation network to enable rapid, one-trial, associations between any spatial location (place in rodents, or spatial view in primates) and an object or reward, and to provide for completion of the whole memory during recall from any part. The factors important in the pattern completion in CA3 together with a large number of independent memories stored in CA3 include a sparse distributed representation which is enhanced by the graded firing rates of CA3 neurons, representations that are independent due to the randomizing effect of the mossy fibers, heterosynaptic long-term depression as well as long-term potentiation in the recurrent collateral synapses, and diluted connectivity to minimize the number of multiple synapses between any pair of CA3 neurons which otherwise distort the basins of attraction. Recall of information from CA3 is implemented by the entorhinal cortex perforant path synapses to CA3 cells, which in acting as a pattern associator allow some pattern generalization. Pattern separation is performed in the dentate granule cells using competitive learning to convert grid-like entorhinal cortex firing to place-like fields. Pattern separation in CA3, which is important for completion of any one of the stored patterns from a fragment, is provided for by the randomizing effect of the mossy fiber synapses to which neurogenesis may contribute, by the large number of dentate granule cells each with a sparse representation, and by the sparse independent representations in CA3. Recall to the neocortex is achieved by a reverse hierarchical series of pattern association networks implemented by the hippocampo-cortical backprojections, each one of which performs some pattern generalization, to retrieve a complete pattern of cortical firing in higher-order cortical areas.},
  journaltitle = {Frontiers in Systems Neuroscience},
  shortjournal = {Front Syst Neurosci},
  urldate = {2017-01-03},
  date = {2013-10-30},
  author = {Rolls, Edmund T.},
  file = {/home/fh/lib/articles/Rolls2013.pdf}
}

@article{Hindy2016,
  langid = {english},
  title = {Linking Pattern Completion in the Hippocampus to Predictive Coding in Visual Cortex},
  volume = {19},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v19/n5/full/nn.4284.html},
  doi = {10.1038/nn.4284},
  abstract = {Models of predictive coding frame perception as a generative process in which expectations constrain sensory representations. These models account for expectations about how a stimulus will move or change from moment to moment, but do not address expectations about what other, distinct stimuli are likely to appear based on prior experience. We show that such memory-based expectations in human visual cortex are related to the hippocampal mechanism of pattern completion.},
  number = {5},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2017-01-04},
  date = {2016-05},
  pages = {665--667},
  keywords = {Learning and memory,Visual system,Psychology},
  author = {Hindy, Nicholas C. and Ng, Felicia Y. and Turk-Browne, Nicholas B.},
  file = {/home/fh/lib/articles/Hindy2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/MS58RMK8/nn.4284.html}
}

@article{Zenke2015,
  langid = {english},
  title = {Diverse Synaptic Plasticity Mechanisms Orchestrated to Form and Retrieve Memories in Spiking Neural Networks},
  volume = {6},
  issn = {2041-1723},
  url = {http://www.nature.com/ncomms/2015/150421/ncomms7922/full/ncomms7922.html},
  doi = {10.1038/ncomms7922},
  abstract = {The brain exhibits a diversity of plasticity mechanisms across different timecales that constitute the putative basis for learning and memory. Here, the authors demonstrate how these different plasticity mechanisms are orchestrated to support the formation of robust and stable neural cell assemblies.},
  journaltitle = {Nature Communications},
  urldate = {2017-01-04},
  date = {2015-04-21},
  pages = {6922},
  author = {Zenke, Friedemann and Agnes, Everton J. and Gerstner, Wulfram},
  file = {/home/fh/lib/articles/Zenke2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/H97DK2DW/ncomms7922.html}
}

@article{Abbott2016,
  langid = {english},
  title = {Building Functional Networks of Spiking Model Neurons},
  volume = {19},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v19/n3/abs/nn.4241.html},
  doi = {10.1038/nn.4241},
  abstract = {Most of the networks used by computer scientists and many of those studied by modelers in neuroscience represent unit activities as continuous variables. Neurons, however, communicate primarily through discontinuous spiking. We review methods for transferring our ability to construct interesting networks that perform relevant tasks from the artificial continuous domain to more realistic spiking network models. These methods raise a number of issues that warrant further theoretical and experimental study.
View full text},
  number = {3},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2017-01-04},
  date = {2016-03},
  pages = {350--355},
  keywords = {Network models,Neural encoding},
  author = {Abbott, L. F. and DePasquale, Brian and Memmesheimer, Raoul-Martin},
  file = {/home/fh/lib/articles/Abbott2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/2VHXPHD8/nn.4241.html}
}

@article{Tetzlaff2015,
  langid = {english},
  title = {The {{Use}} of {{Hebbian Cell Assemblies}} for {{Nonlinear Computation}}},
  volume = {5},
  issn = {2045-2322},
  url = {http://www.nature.com/srep/2015/150807/srep12866/full/srep12866.html},
  doi = {10.1038/srep12866},
  abstract = {When learning a complex task our nervous system self-organizes large groups of neurons into coherent dynamic activity patterns. During this, a network with multiple, simultaneously active, and computationally powerful cell assemblies is created.},
  journaltitle = {Scientific Reports},
  urldate = {2017-01-05},
  date = {2015-08-07},
  pages = {12866},
  author = {Tetzlaff, Christian and Dasgupta, Sakyasingha and Kulvicius, Tomas and W{\"o}rg{\"o}tter, Florentin},
  file = {/home/fh/lib/articles/Tetzlaff2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/Z4EI6QIQ/srep12866.html}
}

@article{Litwin-Kumar2014,
  langid = {english},
  title = {Formation and Maintenance of Neuronal Assemblies through Synaptic Plasticity},
  volume = {5},
  issn = {2041-1723},
  url = {http://www.nature.com/ncomms/2014/141114/ncomms6319/full/ncomms6319.html},
  doi = {10.1038/ncomms6319},
  abstract = {Connectivity patterns between neurons in the brain store recent sensory experiences, but how these patterns form is unclear. Here, the authors provide a model describing the process through which synaptic plasticity combined with homeostatic mechanisms allow stable neuronal assemblies to form.},
  journaltitle = {Nature Communications},
  urldate = {2017-01-11},
  date = {2014-11-14},
  pages = {5319},
  author = {Litwin-Kumar, Ashok and Doiron, Brent},
  file = {/home/fh/lib/articles/Litwin-Kumar2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/ZDEEGBCE/ncomms6319.html}
}

@article{Suvrathan2016,
  langid = {english},
  title = {Timing {{Rules}} for {{Synaptic Plasticity Matched}} to {{Behavioral Function}}},
  volume = {92},
  issn = {08966273},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627316307231},
  doi = {10.1016/j.neuron.2016.10.022},
  number = {5},
  journaltitle = {Neuron},
  urldate = {2017-01-16},
  date = {2016-12},
  pages = {959--967},
  author = {Suvrathan, Aparna and Payne, Hannah L. and Raymond, Jennifer L.},
  file = {/home/fh/lib/articles/Suvrathan2016.pdf}
}

@article{Landau2016,
  langid = {english},
  title = {The {{Impact}} of {{Structural Heterogeneity}} on {{Excitation}}-{{Inhibition Balance}} in {{Cortical Networks}}},
  volume = {92},
  issn = {08966273},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627316307772},
  doi = {10.1016/j.neuron.2016.10.027},
  number = {5},
  journaltitle = {Neuron},
  urldate = {2017-01-16},
  date = {2016-12},
  pages = {1106--1121},
  author = {Landau, Itamar D. and Egger, Robert and Dercksen, Vincent J. and Oberlaender, Marcel and Sompolinsky, Haim},
  file = {/home/fh/lib/articles/Landau2016.pdf}
}

@article{Rose2016,
  langid = {english},
  title = {Reactivation of Latent Working Memories with Transcranial Magnetic Stimulation},
  volume = {354},
  issn = {0036-8075, 1095-9203},
  url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aah7011},
  doi = {10.1126/science.aah7011},
  number = {6316},
  journaltitle = {Science},
  urldate = {2017-01-23},
  date = {2016-12-02},
  pages = {1136--1139},
  author = {Rose, N. S. and LaRocque, J. J. and Riggall, A. C. and Gosseries, O. and Starrett, M. J. and Meyering, E. E. and Postle, B. R.},
  file = {/home/fh/lib/articles/Rose2016.pdf}
}

@article{Deneve2016,
  langid = {english},
  title = {Efficient Codes and Balanced Networks},
  volume = {19},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v19/n3/abs/nn.4243.html},
  doi = {10.1038/nn.4243},
  abstract = {Recent years have seen a growing interest in inhibitory interneurons and their circuits. A striking property of cortical inhibition is how tightly it balances excitation. Inhibitory currents not only match excitatory currents on average, but track them on a millisecond time scale, whether they are caused by external stimuli or spontaneous fluctuations. We review, together with experimental evidence, recent theoretical approaches that investigate the advantages of such tight balance for coding and computation. These studies suggest a possible revision of the dominant view that neurons represent information with firing rates corrupted by Poisson noise. Instead, tight excitatory/inhibitory balance may be a signature of a highly cooperative code, orders of magnitude more precise than a Poisson rate code. Moreover, tight balance may provide a template that allows cortical neurons to construct high-dimensional population codes and learn complex functions of their inputs.
View full text},
  number = {3},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2017-01-24},
  date = {2016-03},
  pages = {375--382},
  keywords = {Network models,Neural circuits,Neural encoding},
  author = {Den{\`e}ve, Sophie and Machens, Christian K.},
  file = {/home/fh/lib/articles/Denève2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/U4D63CNU/nn.4243.html}
}

@article{Vreeswijk1998,
  title = {Chaotic {{Balanced State}} in a {{Model}} of {{Cortical Circuits}}},
  volume = {10},
  issn = {0899-7667},
  url = {http://dx.doi.org/10.1162/089976698300017214},
  doi = {10.1162/089976698300017214},
  abstract = {The nature and origin of the temporal irregularity in the electrical activity of cortical neurons in vivo are not well understood. We consider the hypothesis that this irregularity is due to a balance of excitatory and inhibitory currents into the cortical cells. We study a network model with excitatory and inhibitory populations of simple binary units. The internal feedback is mediated by relatively large synaptic strengths, so that the magnitude of the total excitatory and inhibitory feedback is much larger than the neuronal threshold. The connectivity is random and sparse. The mean number of connections per unit is large, though small compared to the total number of cells in the network. The network also receives a large, temporally regular input from external sources. We present an analytical solution of the mean-field theory of this model, which is exact in the limit of large network size. This theory reveals a new cooperative stationary state of large networks, which we term a balanced state. In this state, a balance between the excitatory and inhibitory inputs emerges dynamically for a wide range of parameters, resulting in a net input whose temporal fluctuations are of the same order as its mean. The internal synaptic inputs act as a strong negative feedback, which linearizes the population responses to the external drive despite the strong nonlinearity of the individual cells. This feedback also greatly stabilizes the system's state and enables it to track a time-dependent input on time scales much shorter than the time constant of a single cell. The spatiotemporal statistics of the balanced state are calculated. It is shown that the autocorrelations decay on a short time scale, yielding an approximate Poissonian temporal statistics. The activity levels of single cells are broadly distributed, and their distribution exhibits a skewed shape with a long power-law tail. The chaotic nature of the balanced state is revealed by showing that the evolution of the microscopic state of the network is extremely sensitive to small deviations in its initial conditions. The balanced state generated by the sparse, strong connections is an asynchronous chaotic state. It is accompanied by weak spatial cross-correlations, the strength of which vanishes in the limit of large network size. This is in contrast to the synchronized chaotic states exhibited by more conventional network models with high connectivity of weak synapses.},
  number = {6},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2017-01-26},
  date = {1998-08-01},
  pages = {1321--1371},
  author = {van Vreeswijk, C. and Sompolinsky, H.},
  file = {/home/fh/lib/articles/Vreeswijk1998.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/VA2M5KKW/089976698300017214.html}
}

@article{Rosenbaum2014a,
  title = {Balanced {{Networks}} of {{Spiking Neurons}} with {{Spatially Dependent Recurrent Connections}}},
  volume = {4},
  url = {http://link.aps.org/doi/10.1103/PhysRevX.4.021039},
  doi = {10.1103/PhysRevX.4.021039},
  abstract = {Networks of model neurons with balanced recurrent excitation and inhibition capture the irregular and asynchronous spiking activity reported in cortex. While mean-field theories of spatially homogeneous balanced networks are well understood, a mean-field analysis of spatially heterogeneous balanced networks has not been fully developed. We extend the analysis of balanced networks to include a connection probability that depends on the spatial separation between neurons. In the continuum limit, we derive that stable, balanced firing rate solutions require that the spatial spread of external inputs be broader than that of recurrent excitation, which in turn must be broader than or equal to that of recurrent inhibition. Notably, this implies that network models with broad recurrent inhibition are inconsistent with the balanced state. For finite size networks, we investigate the pattern-forming dynamics arising when balanced conditions are not satisfied. Our study highlights the new challenges that balanced networks pose for the spatiotemporal dynamics of complex systems.},
  number = {2},
  journaltitle = {Physical Review X},
  shortjournal = {Phys. Rev. X},
  urldate = {2017-01-29},
  date = {2014-05-28},
  pages = {021039},
  author = {Rosenbaum, Robert and Doiron, Brent},
  file = {/home/fh/lib/articles/Rosenbaum2014_2.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/SZKP5DBK/PhysRevX.4.html}
}

@article{Palatella2011a,
  title = {Distribution of First-Return Times in Correlated Stationary Signals},
  volume = {83},
  url = {http://link.aps.org/doi/10.1103/PhysRevE.83.041102},
  doi = {10.1103/PhysRevE.83.041102},
  abstract = {We present an analytical expression for the first return time (FRT) probability density function of a stationary correlated signal. Precisely, we start by considering a stationary discrete-time Ornstein-Uhlenbeck (OU) process with exponential decaying correlation function. The first return time distribution for this process is derived by adopting a well-known formalism typically used in the study of the FRT statistics for nonstationary diffusive processes. Then, by a subordination approach, we treat the case of a stationary process with power-law tail correlation function and diverging correlation time. We numerically test our findings, obtaining in both cases a good agreement with the analytical results. We notice that neither in the standard OU nor in the subordinated case a simple form of waiting time statistics, like stretched-exponential or similar, can be obtained while it is apparent that long time transient may shadow the final asymptotic behavior.},
  number = {4},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  urldate = {2017-02-02},
  date = {2011-04-01},
  pages = {041102},
  author = {Palatella, Luigi and Pennetta, Cecilia},
  file = {/home/fh/lib/articles/Palatella2011.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/BPZ7QAXG/PhysRevE.83.html}
}

@article{Touboul2010,
  title = {Can {{Power}}-{{Law Scaling}} and {{Neuronal Avalanches Arise}} from {{Stochastic Dynamics}}?},
  volume = {5},
  issn = {1932-6203},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0008982},
  doi = {10.1371/journal.pone.0008982},
  abstract = {The presence of self-organized criticality in biology is often evidenced by a power-law scaling of event size distributions, which can be measured by linear regression on logarithmic axes. We show here that such a procedure does not necessarily mean that the system exhibits self-organized criticality. We first provide an analysis of multisite local field potential (LFP) recordings of brain activity and show that event size distributions defined as negative LFP peaks can be close to power-law distributions. However, this result is not robust to change in detection threshold, or when tested using more rigorous statistical analyses such as the Kolmogorov\textendash{}Smirnov test. Similar power-law scaling is observed for surrogate signals, suggesting that power-law scaling may be a generic property of thresholded stochastic processes. We next investigate this problem analytically, and show that, indeed, stochastic processes can produce spurious power-law scaling without the presence of underlying self-organized criticality. However, this power-law is only apparent in logarithmic representations, and does not survive more rigorous analysis such as the Kolmogorov\textendash{}Smirnov test. The same analysis was also performed on an artificial network known to display self-organized criticality. In this case, both the graphical representations and the rigorous statistical analysis reveal with no ambiguity that the avalanche size is distributed as a power-law. We conclude that logarithmic representations can lead to spurious power-law scaling induced by the stochastic nature of the phenomenon. This apparent power-law scaling does not constitute a proof of self-organized criticality, which should be demonstrated by more stringent statistical tests.},
  number = {2},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2017-02-02},
  date = {2010-02-11},
  pages = {e8982},
  keywords = {neural networks,Action potentials,Stochastic processes,Probability distribution,Statistical distributions,Statistical data,Cats,Signal filtering},
  author = {Touboul, Jonathan and Destexhe, Alain},
  file = {/home/fh/lib/articles/Touboul2010.pdf}
}

@book{Gillespie1992,
  location = {{Boston}},
  title = {Markov {{Processes}}: {{An Introduction}} for {{Physical Scientists}}},
  isbn = {978-0-12-283955-9},
  shorttitle = {Markov Processes},
  pagetotal = {565},
  publisher = {{Academic Press}},
  date = {1992},
  keywords = {Markov processes},
  author = {Gillespie, Daniel T.},
  file = {/home/fh/lib/books/Gillespie1992_Markov-Processes-An-Introduction-for-Physical-Scientists.pdf}
}

@article{Koren2007,
  title = {Leapover {{Lengths}} and {{First Passage Time Statistics}} for {{L}}$\backslash$'evy {{Flights}}},
  volume = {99},
  url = {http://link.aps.org/doi/10.1103/PhysRevLett.99.160602},
  doi = {10.1103/PhysRevLett.99.160602},
  abstract = {Exact results for the first passage time and leapover statistics of symmetric and one-sided L{\'e}vy flights (LFs) are derived. LFs with a stable index $\alpha$ are shown to have leapover lengths that are asymptotically power law distributed with an index $\alpha$ for one-sided LFs and, surprisingly, with an index $\alpha$/2 for symmetric LFs. The first passage time distribution scales like a power law with an index 1/2 as required by the Sparre-Andersen theorem for symmetric LFs, whereas one-sided LFs have a narrow distribution of first passage times. The exact analytic results are confirmed by extensive simulations.},
  number = {16},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  urldate = {2017-02-07},
  date = {2007-10-19},
  pages = {160602},
  author = {Koren, Tal and Lomholt, Michael A. and Chechkin, Aleksei V. and Klafter, Joseph and Metzler, Ralf},
  file = {/home/fh/lib/articles/Koren2007.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/9GV59NQX/PhysRevLett.99.html}
}

@article{Nyberg2016,
  langid = {english},
  title = {A Simple Method to Calculate First-Passage Time Densities with Arbitrary Initial Conditions},
  volume = {18},
  issn = {1367-2630},
  url = {http://stacks.iop.org/1367-2630/18/i=6/a=063019},
  doi = {10.1088/1367-2630/18/6/063019},
  abstract = {Numerous applications all the way from biology and physics to economics depend on the density of first crossings over a boundary. Motivated by the lack of general purpose analytical tools for computing first-passage time densities (FPTDs) for complex problems, we propose a new simple method based on the independent interval approximation (IIA). We generalise previous formulations of the IIA to include arbitrary initial conditions as well as to deal with discrete time and non-smooth continuous time processes. We derive a closed form expression for the FPTD in z and Laplace-transform space to a boundary in one dimension. Two classes of problems are analysed in detail: discrete time symmetric random walks (Markovian) and continuous time Gaussian stationary processes (Markovian and non-Markovian). Our results are in good agreement with Langevin dynamics simulations.},
  number = {6},
  journaltitle = {New Journal of Physics},
  shortjournal = {New J. Phys.},
  urldate = {2017-02-08},
  date = {2016},
  pages = {063019},
  author = {Nyberg, Markus and Ambj{\"o}rnsson, Tobias and Lizana, Ludvig},
  file = {/home/fh/lib/articles/Nyberg2016.pdf}
}

@article{Domine1996,
  eprinttype = {jstor},
  eprint = {3215274},
  title = {First {{Passage Time Distribution}} of a {{Wiener Process}} with {{Drift}} Concerning {{Two Elastic Barriers}}},
  volume = {33},
  issn = {0021-9002},
  doi = {10.2307/3215274},
  abstract = {We solve the Fokker-Planck equation for the Wiener process with drift in the presence of elastic boundaries and a fixed start point. An explicit expression is obtained for the first passage density. The cases with pure absorbing and/or reflecting barriers arise for a special choice of a parameter constellation. These special cases are compared with results in Darling and Siegert [5] and Sweet and Hardin [15].},
  number = {1},
  journaltitle = {Journal of Applied Probability},
  date = {1996},
  pages = {164--175},
  author = {Domin{\'e}, Marco},
  file = {/home/fh/lib/articles/Dominé1996.pdf}
}

@article{Molini2011,
  title = {First Passage Time Statistics of {{Brownian}} Motion with Purely Time Dependent Drift and Diffusion},
  volume = {390},
  issn = {0378-4371},
  url = {http://www.sciencedirect.com/science/article/pii/S0378437111000884},
  doi = {10.1016/j.physa.2011.01.024},
  abstract = {Systems where resource availability approaches a critical threshold are common to many engineering and scientific applications and often necessitate the estimation of first passage time statistics of a Brownian motion (Bm) driven by time-dependent drift and diffusion coefficients. Modeling such systems requires solving the associated Fokker-Planck equation subject to an absorbing barrier. Transitional probabilities are derived via the method of images, whose applicability to time dependent problems is shown to be limited to state-independent drift and diffusion coefficients that only depend on time and are proportional to each other. First passage time statistics, such as the survival probabilities and first passage time densities are obtained analytically. The analysis includes the study of different functional forms of the time dependent drift and diffusion, including power-law time dependence and different periodic drivers. As a case study of these theoretical results, a stochastic model of water resources availability in snowmelt dominated regions is presented, where both temperature effects and snow-precipitation input are incorporated.},
  number = {11},
  journaltitle = {Physica A: Statistical Mechanics and its Applications},
  shortjournal = {Physica A: Statistical Mechanics and its Applications},
  urldate = {2017-02-16},
  date = {2011-06-01},
  pages = {1841--1852},
  keywords = {Brownian motion,Time-dependent drift and diffusion,Absorbing barrier,Snowmelt},
  author = {Molini, A. and Talkner, P. and Katul, G. G. and Porporato, A.},
  file = {/home/fh/lib/articles/Molini2011.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/GAT8UZ48/S0378437111000884.html}
}

@article{deVivo2017,
  langid = {english},
  title = {Ultrastructural Evidence for Synaptic Scaling across the Wake/Sleep Cycle},
  volume = {355},
  issn = {0036-8075, 1095-9203},
  url = {http://www.sciencemag.org/lookup/doi/10.1126/science.aah5982},
  doi = {10.1126/science.aah5982},
  number = {6324},
  journaltitle = {Science},
  urldate = {2017-02-17},
  date = {2017-02-03},
  pages = {507--510},
  author = {de Vivo, Luisa and Bellesi, Michele and Marshall, William and Bushong, Eric A. and Ellisman, Mark H. and Tononi, Giulio and Cirelli, Chiara},
  options = {useprefix=true},
  file = {/home/fh/lib/articles/de Vivo2017.pdf}
}

@article{Fujimoto2010,
  title = {A Multiplicative Stochastic Process Deriving the Probability Distribution in Exact Form},
  volume = {221},
  issn = {1742-6596},
  url = {http://stacks.iop.org/1742-6596/221/i=1/a=012008?key=crossref.b5388c396489b87d660fd77f77996a15},
  doi = {10.1088/1742-6596/221/1/012008},
  journaltitle = {Journal of Physics: Conference Series},
  urldate = {2017-02-21},
  date = {2010-04-01},
  pages = {012008},
  author = {Fujimoto, Shouji and Ishikawa, Atushi and Tomoyose, Masashi},
  file = {/home/fh/lib/articles/Fujimoto2010.pdf}
}

@article{Ibata2008,
  title = {Rapid {{Synaptic Scaling Induced}} by {{Changes}} in {{Postsynaptic Firing}}},
  volume = {57},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627308002134},
  doi = {10.1016/j.neuron.2008.02.031},
  abstract = {Summary
Homeostatic synaptic scaling adjusts a neuron's excitatory synaptic strengths up or down to compensate for perturbations in activity. Little is known about the molecular pathway(s) involved, nor is it clear which aspect of ``activity''\textemdash{}local synaptic signaling, postsynaptic firing, or large-scale changes in network activity\textemdash{}is required to induce synaptic scaling. Here, we selectively block either postsynaptic firing in individual neurons or a fraction of presynaptic inputs, while optically monitoring changes in synaptic strength. We find that synaptic scaling is rapidly induced by block of postsynaptic firing, but not by local synaptic blockade, and is mediated through a drop in somatic calcium influx, reduced activation of CaMKIV, and an increase in transcription. Cortical neurons thus homeostatically adjust synaptic strengths in response to changes in their own firing rate, a mechanism with the computational advantage of efficiently normalizing synaptic strengths without interfering with synapse-specific mechanisms of information storage.},
  number = {6},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2017-03-09},
  date = {2008-03-27},
  pages = {819--826},
  keywords = {SIGNALING,CELLBIO,MOLENURO},
  author = {Ibata, Keiji and Sun, Qian and Turrigiano, Gina G.},
  file = {/home/fh/lib/articles/Ibata2008.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/2QXHE9DR/S0896627308002134.html}
}

@article{Clauset2009,
  title = {Power-{{Law Distributions}} in {{Empirical Data}}},
  volume = {51},
  issn = {0036-1445},
  url = {http://epubs.siam.org/doi/abs/10.1137/070710111},
  doi = {10.1137/070710111},
  abstract = {Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution\textemdash{}the part of the distribution representing large but rare events\textemdash{}and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov\textendash{}Smirnov (KS) statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data, while in others the power law is ruled out.},
  number = {4},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  urldate = {2017-03-22},
  date = {2009-11-04},
  pages = {661--703},
  author = {Clauset, A. and Shalizi, C. and Newman, M.},
  file = {/home/fh/lib/articles/Clauset2009.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/NETDNTV4/070710111.html}
}

@article{Hoffmann2017,
  title = {Nonrandom Network Connectivity Comes in Pairs},
  volume = {1},
  url = {http://dx.doi.org/10.1162/NETN_a_00004},
  doi = {10.1162/NETN_a_00004},
  abstract = {Overrepresentation of bidirectional connections in local cortical networks has been repeatedly reported and is a focus of the ongoing discussion of nonrandom connectivity. Here we show in a brief mathematical analysis that in a network in which connection probabilities are symmetric in pairs, Pij = Pji, the occurrences of bidirectional connections and nonrandom structures are inherently linked; an overabundance of reciprocally connected pairs emerges necessarily when some pairs of neurons are more likely to be connected than others. Our numerical results imply that such overrepresentation can also be sustained when connection probabilities are only approximately symmetric.},
  number = {1},
  journaltitle = {Network Neuroscience},
  shortjournal = {Network Neuroscience},
  urldate = {2017-04-04},
  date = {2017-01-06},
  pages = {31--41},
  author = {Hoffmann, Felix Z. and Triesch, Jochen},
  file = {/home/fh/lib/articles/Hoffmann2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/XHXVZN82/NETN_a_00004.html}
}

@article{Radford2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.01444},
  primaryClass = {cs},
  title = {Learning to {{Generate Reviews}} and {{Discovering Sentiment}}},
  url = {http://arxiv.org/abs/1704.01444},
  abstract = {We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.},
  urldate = {2017-04-08},
  date = {2017-04-05},
  keywords = {Computer Science - Learning,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  author = {Radford, Alec and Jozefowicz, Rafal and Sutskever, Ilya},
  file = {/home/fh/lib/articles/Radford2017.pdf}
}

@article{Weber2017,
  langid = {english},
  title = {Learning Place Cells, Grid Cells and Invariances: {{A}} Unifying Model},
  url = {http://biorxiv.org/content/early/2017/02/24/102525},
  doi = {10.1101/102525},
  shorttitle = {Learning Place Cells, Grid Cells and Invariances},
  abstract = {Neurons in the hippocampus and adjacent brain areas show a large diversity in their tuning to location and head direction. The underlying circuit mechanisms are not fully resolved. In particular, it is unclear why certain cell types are selective to one spatial variable, but invariant to another. For example, a place cell is highly selective to location, but typically invariant to head direction. Here, we propose that all observed spatial tuning patterns -- in both their selectivity and their invariance -- are a consequence of the same mechanism: Excitatory and inhibitory synaptic plasticity that is driven by the spatial tuning statistics of synaptic inputs. Using simulations and a mathematical analysis, we show that combined excitatory and inhibitory plasticity can lead to localized, grid-like or invariant activity. Combinations of different input statistics along different spatial dimensions reproduce all major spatial tuning patterns observed in rodents. The model is robust to changes in parameters, develops patterns on behavioral time scales and makes distinctive experimental predictions. Our results suggest that the interaction of excitatory and inhibitory plasticity is a general principle for the formation of neural representations.},
  journaltitle = {bioRxiv},
  urldate = {2017-04-18},
  date = {2017-02-24},
  pages = {102525},
  author = {Weber, Simon N. and Sprekeler, Henning},
  file = {/home/fh/lib/articles/Weber2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/KTZSTZ9B/102525.html}
}

@article{Lim2013,
  langid = {english},
  title = {Balanced Cortical Microcircuitry for Maintaining Information in Working Memory},
  volume = {16},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v16/n9/abs/nn.3492.html},
  doi = {10.1038/nn.3492},
  abstract = {Persistent neural activity in the absence of a stimulus has been identified as a neural correlate of working memory, but how such activity is maintained by neocortical circuits remains unknown. We used a computational approach to show that the inhibitory and excitatory microcircuitry of neocortical memory-storing regions is sufficient to implement a corrective feedback mechanism that enables persistent activity to be maintained stably for prolonged durations. When recurrent excitatory and inhibitory inputs to memory neurons were balanced in strength and offset in time, drifts in activity triggered a corrective signal that counteracted memory decay. Circuits containing this mechanism temporally integrated their inputs, generated the irregular neural firing observed during persistent activity and were robust against common perturbations that severely disrupted previous models of short-term memory storage. These results reveal a mechanism for the accumulation and storage of memories in neocortical circuits based on principles of corrective negative feedback that are widely used in engineering applications.
View full text},
  number = {9},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2017-04-20},
  date = {2013-09},
  pages = {1306--1314},
  keywords = {Network models},
  author = {Lim, Sukbin and Goldman, Mark S.},
  file = {/home/fh/lib/articles/Lim2013_2.pdf;/home/fh/lib/articles/Lim2013.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/V3NGX7GD/nn.3492.html}
}

@article{Sadeh2017,
  langid = {english},
  title = {Assessing the Role of Inhibition in Stabilizing Neocortical Networks Requires Large-Scale Perturbation of the Inhibitory Population},
  url = {http://biorxiv.org/content/early/2017/04/12/123950},
  doi = {10.1101/123950},
  abstract = {Neurons within cortical microcircuits are densely interconnected by recurrent excitatory synaptic connections that are thought to amplify signals (Douglas and Martin, 2007), form selective subnetworks (Ko et al., 2011) and aid feature discrimination. Strong inhibition (Haider et al., 2013) counterbalances excitation, enabling sensory features to be sharpened and represented by sparse codes (Willmore et al., 2011). This "balance" of excitation and inhibition has made it difficult to assess the strength, or gain, of the recurrent excitatory connections within cortical networks, which is key to understanding their operational regime and the computations they can perform. Networks of neurons that combine an unstable high-gain excitatory population with stabilising inhibitory feedback are known as inhibition-stabilized networks (ISNs; Tsodyks et al. 1997). Classical theoretical studies using reduced network models predict that ISNs produce paradoxical responses to perturbation, but in vivo optogenetic perturbations have failed to find evidence for ISNs in cortex (Atallah et al., 2012). We re-examined this question by investigating how rate-based and spiking cortical network models consisting of many neurons behave following perturbations, and found that results obtained from reduced network models fail to predict the effects of perturbations in more realistic networks. Our cortical network models predict that a large proportion of the inhibitory network must be perturbed in order to robustly detect an ISN regime in cortex. We propose that optogenetic suppression of inhibition under a promoter targeting all inhibitory neurons, coupled with wide-field illumination, may provide a sufficient perturbation to reveal the operating regime of cortex. Our results suggest that detailed computational models of optogenetic perturbations are necessary to interpret the results of experimental paradigms.},
  journaltitle = {bioRxiv},
  urldate = {2017-04-20},
  date = {2017-04-12},
  pages = {123950},
  author = {Sadeh, Sadra and Silver, Robin Angus and Mrsic-Flogel, Thomas and Muir, Dylan},
  file = {/home/fh/lib/articles/Sadeh2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/6A2S8FVW/123950.html}
}

@article{Nykamp2017,
  langid = {english},
  title = {Mean-{{Field Equations For Neuronal Networks With Arbitrary Degree Distributions}}},
  url = {http://biorxiv.org/content/early/2017/03/20/118463},
  doi = {10.1101/118463},
  abstract = {The emergent dynamics in networks of recurrently coupled spiking neurons depends on the interplay between single-cell dynamics and network topology. Most theoretical studies on network dynamics have assumed simple topologies, such as connections which are made randomly and independently with a fixed probability (Erdos-Renyi network) (ER), or all-to-all connected networks. However, recent findings from slice experiments suggest that the actual patterns of connectivity between cortical neurons are more structured than in the ER random network. Here we explore how introducing additional higher-order statistical structure into the connectivity can affect the dynamics in neuronal networks. Specifically, we consider networks in which the number of pre-synaptic and post-synaptic contacts for each neuron, the degrees, are drawn from a joint degree distribution. We derive mean-field equations for a single population of homogeneous neurons and for a network of excitatory and inhibitory neurons, where the neurons can have arbitrary degree distributions. Through analysis of the mean-field equations and simulation of networks of integrate-and-fire neurons, we show that such networks have potentially much richer dynamics than an equivalent ER network. Finally, we relate the degree distributions to so-called cortical motifs.},
  journaltitle = {bioRxiv},
  urldate = {2017-04-20},
  date = {2017-03-20},
  pages = {118463},
  author = {Nykamp, Duane and Friedman, Daniel and Shaker, Sammy and Shinn, Maxwell and Vella, Michael and Compte, Albert and Roxin, Alex},
  file = {/home/fh/lib/articles/Nykamp2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/KWHF85C6/118463.html}
}

@article{Ocker2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.03132},
  primaryClass = {q-bio},
  title = {From the Statistics of Connectivity to the Statistics of Spike Times in Neuronal Networks},
  url = {http://arxiv.org/abs/1703.03132},
  abstract = {An essential step toward understanding neural circuits is linking their structure and their dynamics. In general, this relationship can be almost arbitrarily complex. Recent theoretical work has, however, begun to identify some broad principles underlying collective spiking activity in neural circuits. The first is that local features of network connectivity can be surprisingly effective in predicting global statistics of activity across a network. The second is that, for the important case of large networks with excitatory-inhibitory balance, correlated spiking persists or vanishes depending on the spatial scales of recurrent and feedforward connectivity. We close by showing how these ideas, together with plasticity rules, can help to close the loop between network structure and activity statistics.},
  urldate = {2017-04-20},
  date = {2017-03-08},
  keywords = {Quantitative Biology - Neurons and Cognition},
  author = {Ocker, Gabriel Koch and Hu, Yu and Buice, Michael A. and Doiron, Brent and Josi{\'c}, Kre{\v s}imir and Rosenbaum, Robert and Shea-Brown, Eric},
  file = {/home/fh/lib/articles/Ocker2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/DH3VCRC7/1703.html}
}

@article{Litwin-Kumar2017,
  title = {Optimal {{Degrees}} of {{Synaptic Connectivity}}},
  volume = {93},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627317300545},
  doi = {10.1016/j.neuron.2017.01.030},
  abstract = {Summary
Synaptic connectivity varies widely across neuronal types. Cerebellar granule cells receive five orders of magnitude fewer inputs than the Purkinje cells they innervate, and cerebellum-like circuits, including the insect mushroom body, also exhibit large divergences in connectivity. In contrast, the number of inputs per neuron in cerebral cortex is more uniform and large. We investigate how the dimension of a representation formed by a population of neurons depends on how many inputs each neuron receives and what this implies for learning associations. Our theory predicts that the dimensions of the cerebellar granule-cell and Drosophila Kenyon-cell representations are maximized at degrees of synaptic connectivity that match those observed anatomically, showing that sparse connectivity is sometimes superior to dense connectivity. When input synapses are subject to supervised plasticity, however, dense wiring becomes advantageous, suggesting that the type of plasticity exhibited by a set of synapses is a major determinant of connection density.},
  number = {5},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2017-04-20},
  date = {2017-03-08},
  pages = {1153--1164.e7},
  author = {Litwin-Kumar, Ashok and Harris, Kameron Decker and Axel, Richard and Sompolinsky, Haim and Abbott, L. F.},
  file = {/home/fh/lib/articles/Litwin-Kumar2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/CSQSB94F/S0896627317300545.html}
}

@article{Keck2017,
  langid = {english},
  title = {Integrating {{Hebbian}} and Homeostatic Plasticity: The Current State of the Field and Future Research Directions},
  volume = {372},
  issn = {0962-8436, 1471-2970},
  url = {http://rstb.royalsocietypublishing.org/lookup/doi/10.1098/rstb.2016.0158},
  doi = {10.1098/rstb.2016.0158},
  shorttitle = {Integrating {{Hebbian}} and Homeostatic Plasticity},
  number = {1715},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  urldate = {2017-04-24},
  date = {2017-03-05},
  pages = {20160158},
  author = {Keck, Tara and Toyoizumi, Taro and Chen, Lu and Doiron, Brent and Feldman, Daniel E. and Fox, Kevin and Gerstner, Wulfram and Haydon, Philip G. and H{\"u}bener, Mark and Lee, Hey-Kyoung and Lisman, John E. and Rose, Tobias and Sengpiel, Frank and Stellwagen, David and Stryker, Michael P. and Turrigiano, Gina G. and van Rossum, Mark C.},
  options = {useprefix=true},
  file = {/home/fh/lib/articles/Keck2017.pdf}
}

@article{Koch2016,
  langid = {english},
  title = {Neural Correlates of Consciousness: Progress and Problems},
  volume = {17},
  issn = {1471-003X},
  url = {http://www.nature.com/nrn/journal/v17/n5/abs/nrn.2016.22.html},
  doi = {10.1038/nrn.2016.22},
  shorttitle = {Neural Correlates of Consciousness},
  abstract = {There have been a number of advances in the search for the neural correlates of consciousness \textemdash{} the minimum neural mechanisms sufficient for any one specific conscious percept. In this Review, we describe recent findings showing that the anatomical neural correlates of consciousness are primarily localized to a posterior cortical hot zone that includes sensory areas, rather than to a fronto-parietal network involved in task monitoring and reporting. We also discuss some candidate neurophysiological markers of consciousness that have proved illusory, and measures of differentiation and integration of neural activity that offer more promising quantitative indices of consciousness.
View full text},
  number = {5},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  urldate = {2017-04-26},
  date = {2016-05},
  pages = {307--321},
  keywords = {Consciousness,Disorders of consciousness},
  author = {Koch, Christof and Massimini, Marcello and Boly, Melanie and Tononi, Giulio},
  file = {/home/fh/lib/articles/Koch2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/AQV52NAF/nrn.2016.22.html}
}

@misc{Miner2017,
  title = {[{{Draft}}] {{The Slope}} of {{Distributions}} of {{Synaptic Lifetimes}} Can {{Serve}} as a {{Proxy Measure}} for the {{Balance Between Potentiation}} and {{Depression}} in a {{Cortical Networks}}},
  date = {2017},
  author = {Miner, Daniel},
  file = {/home/fh/lib/documents/Miner2017.pdf}
}

@article{Bengio2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.04156},
  primaryClass = {cs},
  title = {Towards {{Biologically Plausible Deep Learning}}},
  url = {http://arxiv.org/abs/1502.04156},
  abstract = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
  urldate = {2017-04-26},
  date = {2015-02-13},
  keywords = {Computer Science - Learning},
  author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
  file = {/home/fh/lib/articles/Bengio2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/FXQH9T62/1502.html}
}

@article{Lee2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.7525},
  primaryClass = {cs},
  title = {Difference {{Target Propagation}}},
  url = {http://arxiv.org/abs/1412.7525},
  abstract = {Back-propagation has been the workhorse of recent successes of deep learning but it relies on infinitesimal effects (partial derivatives) in order to perform credit assignment. This could become a serious issue as one considers deeper and more non-linear functions, e.g., consider the extreme case of nonlinearity where the relation between parameters and cost is actually discrete. Inspired by the biological implausibility of back-propagation, a few approaches have been proposed in the past that could play a similar credit assignment role. In this spirit, we explore a novel approach to credit assignment in deep networks that we call target propagation. The main idea is to compute targets rather than gradients, at each layer. Like gradients, they are propagated backwards. In a way that is related but different from previously proposed proxies for back-propagation which rely on a backwards network with symmetric weights, target propagation relies on auto-encoders at each layer. Unlike back-propagation, it can be applied even when units exchange stochastic bits rather than real numbers. We show that a linear correction for the imperfectness of the auto-encoders, called difference target propagation, is very effective to make target propagation actually work, leading to results comparable to back-propagation for deep networks with discrete and continuous units and denoising auto-encoders and achieving state of the art for stochastic networks.},
  urldate = {2017-04-26},
  date = {2014-12-23},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Lee, Dong-Hyun and Zhang, Saizheng and Fischer, Asja and Bengio, Yoshua},
  file = {/home/fh/lib/articles/Lee2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/F7DTGFH4/1412.html}
}

@article{Toyoizumi2014a,
  title = {Modeling the {{Dynamic Interaction}} of {{Hebbian}} and {{Homeostatic Plasticity}}},
  volume = {84},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627314008940},
  doi = {10.1016/j.neuron.2014.09.036},
  abstract = {Summary
Hebbian and homeostatic plasticity together refine neural circuitry, but their interactions are unclear. In most existing models, each form of plasticity directly modifies synaptic strength. Equilibrium is reached when the two are inducing equal and opposite changes. We show that such models cannot reproduce ocular dominance plasticity (ODP) because negative feedback from the slow homeostatic plasticity observed in ODP cannot stabilize the positive feedback of fast Hebbian plasticity. We propose a model in which synaptic strength is the product of a synapse-specific Hebbian factor and a postsynaptic-cell-specific homeostatic factor, with each factor separately arriving at a stable inactive state. This model captures ODP dynamics and has plausible biophysical substrates. We confirm model predictions experimentally that plasticity is inactive at stable states and that synaptic strength overshoots during recovery from visual deprivation. These results highlight the importance of multiple regulatory pathways for interactions of plasticity mechanisms operating over separate timescales.},
  number = {2},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2017-04-27},
  date = {2014-10-22},
  pages = {497--510},
  author = {Toyoizumi, Taro and Kaneko, Megumi and Stryker, Michael P. and Miller, Kenneth D.},
  file = {/home/fh/lib/articles/Toyoizumi2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/QWEG8U5C/S0896627314008940.html}
}

@article{Rumpel2016a,
  langid = {english},
  title = {The Dynamic Connectome},
  volume = {7},
  issn = {1868-856X},
  url = {https://link.springer.com/article/10.1007/s13295-016-0026-2},
  doi = {10.1007/s13295-016-0026-2},
  abstract = {When trying to gain intuitions about the computations implemented in neural circuits, we often use comparisons with electronic circuits. However, one fundamental difference to hard-wired electronic circuits is that the structure of neural circuits undergoes constant remodeling. Here, we discuss recent findings highlighting the dynamic nature of neural circuits and the underlying mechanisms. The dynamics of neural circuits follows rules that explain steady state statistics of synaptic properties observed at a single time point. Interestingly, these rules allow the prediction of future network states and extend the insights gained from serial sectioning electron microscopy of brain samples, which inherently provides information from only a single time point. We argue how the connectome's dynamic nature can be reconciled with stable functioning and long-term memory storage and how it may even benefit learning.},
  number = {3},
  journaltitle = {e-Neuroforum},
  shortjournal = {e-Neuroforum},
  urldate = {2017-04-27},
  date = {2016-09-01},
  pages = {48--53},
  author = {Rumpel, Simon and Triesch, Jochen},
  file = {/home/fh/lib/articles/Rumpel2016_2.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/89DVA4W6/s13295-016-0026-2.html}
}

@article{Wang2017,
  langid = {english},
  title = {Structure and Dynamics of Self-Organized Neuronal Network with an Improved {{STDP}} Rule},
  issn = {0924-090X, 1573-269X},
  url = {https://link.springer.com/article/10.1007/s11071-017-3348-x},
  doi = {10.1007/s11071-017-3348-x},
  abstract = {The chemical synapses in a neural network are known to be modulated by the neuronal firing activities through the spike-timing-dependent plasticity (STDP) rule. In this paper, we improve the multiplicative STDP rule by adding a momentum item with the aim of overcoming the low rate with which the neuronal network self-organizes into a stable complex structure. We find that the improved STDP rule with suitable momentum factors significantly speeds up the evolutionary process of the self-organized neuronal network. In addition, we explore the topological structure of self-organized neuronal network using complex network method. We show that the improved STDP rule generally results in a smaller node degree, clustering coefficient and modularity of self-organized neuronal network. Furthermore, we investigate the dynamical behaviors of self-organized neuronal network. We observe that depending on the momentum factor, the improved STDP rule has different effects on the network synchronization, neural information transmission, modularity and network complexity. Remarkably, for a specific momentum factor, the self-organized neuronal network shows the highest global efficiency of information transmission and the best combination between functional segregation and integration, which reflects the optimal dynamics as well as the topological structure. Our results provide a reasonable and efficient modulating rule of chemical synapse underlying the neuronal firing activities.},
  journaltitle = {Nonlinear Dynamics},
  shortjournal = {Nonlinear Dyn},
  urldate = {2017-04-27},
  date = {2017-02-10},
  pages = {1--14},
  author = {Wang, Rong and Wu, Ying and Wang, Li and Du, Mengmeng and Li, Jiajia},
  file = {/home/fh/lib/articles/Wang2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/MWCD6IPP/10.html}
}

@article{Alstott2014,
  title = {Powerlaw: {{A Python Package}} for {{Analysis}} of {{Heavy}}-{{Tailed Distributions}}},
  volume = {9},
  issn = {1932-6203},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0085777},
  doi = {10.1371/journal.pone.0085777},
  shorttitle = {Powerlaw},
  abstract = {Power laws are theoretically interesting probability distributions that are also frequently used to describe empirical data. In recent years, effective statistical methods for fitting power laws have been developed, but appropriate use of these techniques requires significant programming and statistical insight. In order to greatly decrease the barriers to using good statistical methods for fitting power law distributions, we developed the powerlaw Python package. This software package provides easy commands for basic fitting and statistical analysis of distributions. Notably, it also seeks to support a variety of user needs by being exhaustive in the options available to the user. The source code is publicly available and easily extensible.},
  number = {1},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2017-04-28},
  date = {2014-01-29},
  pages = {e85777},
  keywords = {Simulation and Modeling,Neurons,Probability distribution,Statistical distributions,Data visualization,Source code,Probability density,Statistical methods},
  author = {Alstott, Jeff and Bullmore, Ed and Plenz, Dietmar},
  file = {/home/fh/lib/articles/Alstott2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/IU3P2VTP/article.html}
}

@article{Gilson2011,
  langid = {english},
  title = {Stability versus {{Neuronal Specialization}} for {{STDP}}: {{Long}}-{{Tail Weight Distributions Solve}} the {{Dilemma}}},
  volume = {6},
  issn = {1932-6203},
  url = {http://dx.plos.org/10.1371/journal.pone.0025339},
  doi = {10.1371/journal.pone.0025339},
  shorttitle = {Stability versus {{Neuronal Specialization}} for {{STDP}}},
  number = {10},
  journaltitle = {PLoS ONE},
  urldate = {2017-05-05},
  date = {2011-10-07},
  pages = {e25339},
  author = {Gilson, Matthieu and Fukai, Tomoki},
  editor = {Mansvelder, Huibert D.},
  file = {/home/fh/lib/articles/Gilson2011.PDF}
}

@article{Burkitt2004,
  title = {Spike-{{Timing}}-{{Dependent Plasticity}}: {{The Relationship}} to {{Rate}}-{{Based Learning}} for {{Models}} with {{Weight Dynamics Determined}} by a {{Stable Fixed Point}}},
  volume = {16},
  issn = {0899-7667},
  url = {http://dx.doi.org/10.1162/089976604773135041},
  doi = {10.1162/089976604773135041},
  shorttitle = {Spike-{{Timing}}-{{Dependent Plasticity}}},
  number = {5},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2017-05-05},
  date = {2004-05-01},
  pages = {885--940},
  author = {Burkitt, Anthony N. and Meffin, Hamish and Grayden, David. B.},
  file = {/home/fh/lib/articles/Burkitt2004.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/CG9Q4WI9/089976604773135041.html}
}

@article{Costa2017,
  langid = {english},
  title = {Functional Consequences of Pre- and Postsynaptic Expression of Synaptic Plasticity},
  volume = {372},
  issn = {0962-8436, 1471-2970},
  url = {http://rstb.royalsocietypublishing.org/content/372/1715/20160153},
  doi = {10.1098/rstb.2016.0153},
  abstract = {Growing experimental evidence shows that both homeostatic and Hebbian synaptic plasticity can be expressed presynaptically as well as postsynaptically. In this review, we start by discussing this evidence and methods used to determine expression loci. Next, we discuss the functional consequences of this diversity in pre- and postsynaptic expression of both homeostatic and Hebbian synaptic plasticity. In particular, we explore the functional consequences of a biologically tuned model of pre- and postsynaptically expressed spike-timing-dependent plasticity complemented with postsynaptic homeostatic control. The pre- and postsynaptic expression in this model predicts (i) more reliable receptive fields and sensory perception, (ii) rapid recovery of forgotten information (memory savings), and (iii) reduced response latencies, compared with a model with postsynaptic expression only. Finally, we discuss open questions that will require a considerable research effort to better elucidate how the specific locus of expression of homeostatic and Hebbian plasticity alters synaptic and network computations.
This article is part of the themed issue `Integrating Hebbian and homeostatic plasticity'.},
  number = {1715},
  journaltitle = {Phil. Trans. R. Soc. B},
  shortjournal = {Phil. Trans. R. Soc. B},
  urldate = {2017-05-05},
  date = {2017-03-05},
  pages = {20160153},
  author = {Costa, Rui Ponte and Mizusaki, Beatriz E. P. and Sj{\"o}str{\"o}m, P. Jesper and van Rossum, Mark C. W.},
  file = {/home/fh/lib/articles/Costa2017.pdf},
  eprinttype = {pmid},
  eprint = {28093547}
}

@article{Rossum2000,
  langid = {english},
  title = {Stable {{Hebbian Learning}} from {{Spike Timing}}-{{Dependent Plasticity}}},
  volume = {20},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/20/23/8812},
  abstract = {We explore a synaptic plasticity model that incorporates recent findings that potentiation and depression can be induced by precisely timed pairs of synaptic events and postsynaptic spikes. In addition we include the observation that strong synapses undergo relatively less potentiation than weak synapses, whereas depression is independent of synaptic strength. After random stimulation, the synaptic weights reach an equilibrium distribution which is stable, unimodal, and has positive skew. This weight distribution compares favorably to the distributions of quantal amplitudes and of receptor number observed experimentally in central neurons and contrasts to the distribution found in plasticity models without size-dependent potentiation. Also in contrast to those models, which show strong competition between the synapses, stable plasticity is achieved with little competition. Instead, competition can be introduced by including a separate mechanism that scales synaptic strengths multiplicatively as a function of postsynaptic activity. In this model, synaptic weights change in proportion to how correlated they are with other inputs onto the same postsynaptic neuron. These results indicate that stable correlation-based plasticity can be achieved without introducing competition, suggesting that plasticity and competition need not coexist in all circuits or at all developmental stages.},
  number = {23},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2017-05-05},
  date = {2000-12-01},
  pages = {8812--8821},
  keywords = {Hebbian plasticity,synaptic weights,synaptic competition,activity-dependent scaling,temporal learning,stochastic approaches},
  author = {van Rossum, M. C. W. and Bi, G. Q. and Turrigiano, G. G.},
  file = {/home/fh/lib/articles/Rossum2000.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/65VHRF9H/8812.html},
  eprinttype = {pmid},
  eprint = {11102489}
}

@article{Miller1994,
  title = {The {{Role}} of {{Constraints}} in {{Hebbian Learning}}},
  volume = {6},
  issn = {0899-7667},
  url = {http://dx.doi.org/10.1162/neco.1994.6.1.100},
  doi = {10.1162/neco.1994.6.1.100},
  number = {1},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2017-05-05},
  date = {1994-01-01},
  pages = {100--126},
  author = {Miller, Kenneth D. and MacKay, David J. C.},
  file = {/home/fh/lib/articles/Miller1994.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/WMRME25H/neco.1994.6.1.html}
}

@article{Weigand2017,
  langid = {english},
  title = {Universal Transition from Unstructured to Structured Neural Maps},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1616163114},
  doi = {10.1073/pnas.1616163114},
  journaltitle = {Proceedings of the National Academy of Sciences},
  urldate = {2017-05-06},
  date = {2017-05-03},
  pages = {201616163},
  author = {Weigand, Marvin and Sartori, Fabio and Cuntz, Hermann},
  file = {/home/fh/lib/articles/Weigand2017.pdf}
}

@article{Fasoli2016,
  title = {The {{Complexity}} of {{Dynamics}} in {{Small Neural Circuits}}},
  volume = {12},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004992},
  doi = {10.1371/journal.pcbi.1004992},
  abstract = {Author Summary The mesoscopic level of brain organization, describing the organization and dynamics of small circuits of neurons including from few tens to few thousands, has recently received considerable experimental attention. It is useful for describing small neural systems of invertebrates, and in mammalian neural systems it is often seen as a middle ground that is fundamental to link single neuron activity to complex functions and behavior. However, and somewhat counter-intuitively, the behavior of neural networks of small and intermediate size can be much more difficult to study mathematically than that of large networks, and appropriate mathematical methods to study the dynamics of such networks have not been developed yet. Here we consider a model of a network of firing-rate neurons with arbitrary finite size, and we study its local bifurcations using an analytical approach. This analysis, complemented by numerical studies for both the local and global bifurcations, shows the emergence of strong and previously unexplored finite-size effects that are particularly hard to detect in large networks. This study advances the tools available for the comprehension of finite-size neural circuits, going beyond the insights provided by the mean-field approximation and the current techniques for the quantification of finite-size effects.},
  number = {8},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2017-05-08},
  date = {2016-08-05},
  pages = {e1004992},
  keywords = {neural networks,Neurons,Neural pathways,Membrane potential,Network analysis,Bifurcation theory,Eigenvalues,Mesoscopic physics},
  author = {Fasoli, Diego and Cattani, Anna and Panzeri, Stefano},
  file = {/home/fh/lib/articles/Fasoli2016.pdf}
}

@article{Tsodyks1997,
  langid = {english},
  title = {Paradoxical {{Effects}} of {{External Modulation}} of {{Inhibitory Interneurons}}},
  volume = {17},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/17/11/4382},
  abstract = {The neocortex, hippocampus, and several other brain regions contain populations of excitatory principal cells with recurrent connections and strong interactions with local inhibitory interneurons. To improve our understanding of the interactions among these cell types, we modeled the dynamic behavior of this type of network, including external inputs. A surprising finding was that increasing the direct external inhibitory input to the inhibitory interneurons, without directly affecting any other part of the network, can, in some circumstances, cause the interneurons to increase their firing rates. The main prerequisite for this paradoxical response to external input is that the recurrent connections among the excitatory cells are strong enough to make the excitatory network unstable when feedback inhibition is removed. Because this requirement is met in the neocortex and several regions of the hippocampus, these observations have important implications for understanding the responses of interneurons to a variety of pharmacological and electrical manipulations. The analysis can be extended to a scenario with periodically varying external input, where it predicts a systematic relationship between the phase shift and depth of modulation for each interneuron. This prediction was tested by recording from interneurons in the CA1 region of the rat hippocampusin vivo, and the results broadly confirmed the model. These findings have further implications for the function of inhibitory and neuromodulatory circuits, which can be tested experimentally.},
  number = {11},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2017-05-09},
  date = {1997-06-01},
  pages = {4382--4388},
  keywords = {Hippocampus,inhibition,network model,oscillation,theta rhythm,interneurons},
  author = {Tsodyks, Misha V. and Skaggs, William E. and Sejnowski, Terrence J. and McNaughton, Bruce L.},
  file = {/home/fh/lib/articles/Tsodyks1997.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/3IXCHA8F/4382.html},
  eprinttype = {pmid},
  eprint = {9151754}
}

@article{Giraud2012,
  langid = {english},
  title = {Cortical Oscillations and Speech Processing: Emerging Computational Principles and Operations},
  volume = {15},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v15/n4/full/nn.3063.html},
  doi = {10.1038/nn.3063},
  shorttitle = {Cortical Oscillations and Speech Processing},
  abstract = {Neuronal oscillations are ubiquitous in the brain and may contribute to cognition in several ways: for example, by segregating information and organizing spike timing. Recent data show that delta, theta and gamma oscillations are specifically engaged by the multi-timescale, quasi-rhythmic properties of speech and can track its dynamics. We argue that they are foundational in speech and language processing, 'packaging' incoming information into units of the appropriate temporal granularity. Such stimulus-brain alignment arguably results from auditory and motor tuning throughout the evolution of speech and language and constitutes a natural model system allowing auditory research to make a unique contribution to the issue of how neural oscillatory activity affects human cognition.},
  number = {4},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2017-05-09},
  date = {2012-04},
  pages = {511--517},
  keywords = {Cognitive neuroscience,computational neuroscience,Neurophysiology},
  author = {Giraud, Anne-Lise and Poeppel, David},
  file = {/home/fh/lib/articles/Giraud2012.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/DRE3M7MV/nn.3063.html}
}

@article{Hickok2007,
  langid = {english},
  title = {The Cortical Organization of Speech Processing},
  volume = {8},
  issn = {1471-003X},
  url = {http://www.nature.com/nrn/journal/v8/n5/full/nrn2113.html},
  doi = {10.1038/nrn2113},
  abstract = {Despite decades of research, the functional neuroanatomy of speech processing has been difficult to characterize. A major impediment to progress may have been the failure to consider task effects when mapping speech-related processing systems. We outline a dual-stream model of speech processing that remedies this situation. In this model, a ventral stream processes speech signals for comprehension, and a dorsal stream maps acoustic speech signals to frontal lobe articulatory networks. The model assumes that the ventral stream is largely bilaterally organized \textemdash{} although there are important computational differences between the left- and right-hemisphere systems \textemdash{} and that the dorsal stream is strongly left-hemisphere dominant.},
  number = {5},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  urldate = {2017-05-09},
  date = {2007-05},
  pages = {393--402},
  author = {Hickok, Gregory and Poeppel, David},
  file = {/home/fh/lib/articles/Hickok2007.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/TX78MHQZ/nrn2113.html}
}

@article{Overath2015,
  langid = {english},
  title = {The Cortical Analysis of Speech-Specific Temporal Structure Revealed by Responses to Sound Quilts},
  volume = {18},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v18/n6/full/nn.4021.html},
  doi = {10.1038/nn.4021},
  abstract = {Speech contains temporal structure that the brain must analyze to enable linguistic processing. To investigate the neural basis of this analysis, we used sound quilts, stimuli constructed by shuffling segments of a natural sound, approximately preserving its properties on short timescales while disrupting them on longer scales. We generated quilts from foreign speech to eliminate language cues and manipulated the extent of natural acoustic structure by varying the segment length. Using functional magnetic resonance imaging, we identified bilateral regions of the superior temporal sulcus (STS) whose responses varied with segment length. This effect was absent in primary auditory cortex and did not occur for quilts made from other natural sounds or acoustically matched synthetic sounds, suggesting tuning to speech-specific spectrotemporal structure. When examined parametrically, the STS response increased with segment length up to \textasciitilde{}500 ms. Our results identify a locus of speech analysis in human auditory cortex that is distinct from lexical, semantic or syntactic processes.},
  number = {6},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2017-05-09},
  date = {2015-06},
  pages = {903--911},
  keywords = {Psychology,language,Cortex,Perception},
  author = {Overath, Tobias and McDermott, Josh H. and Zarate, Jean Mary and Poeppel, David},
  file = {/home/fh/lib/articles/Overath2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/4GENSKCI/nn.4021.html}
}

@article{Ding2016,
  langid = {english},
  title = {Cortical Tracking of Hierarchical Linguistic Structures in Connected Speech},
  volume = {19},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v19/n1/abs/nn.4186.html},
  doi = {10.1038/nn.4186},
  abstract = {The most critical attribute of human language is its unbounded combinatorial nature: smaller elements can be combined into larger structures on the basis of a grammatical system, resulting in a hierarchy of linguistic units, such as words, phrases and sentences. Mentally parsing and representing such structures, however, poses challenges for speech comprehension. In speech, hierarchical linguistic structures do not have boundaries that are clearly defined by acoustic cues and must therefore be internally and incrementally constructed during comprehension. We found that, during listening to connected speech, cortical activity of different timescales concurrently tracked the time course of abstract linguistic structures at different hierarchical levels, such as words, phrases and sentences. Notably, the neural tracking of hierarchical linguistic structures was dissociated from the encoding of acoustic cues and from the predictability of incoming words. Our results indicate that a hierarchy of neural processing timescales underlies grammar-based internal construction of hierarchical linguistic structure.
View full text},
  number = {1},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2017-05-09},
  date = {2016-01},
  pages = {158--164},
  keywords = {Psychology,language,Sensory processing},
  author = {Ding, Nai and Melloni, Lucia and Zhang, Hang and Tian, Xing and Poeppel, David},
  file = {/home/fh/lib/articles/Ding2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/7BJZCX2N/nn.4186.html}
}

@article{Wilson1973,
  langid = {english},
  title = {A Mathematical Theory of the Functional Dynamics of Cortical and Thalamic Nervous Tissue},
  volume = {13},
  issn = {0023-5946, 1432-0770},
  url = {https://link.springer.com/article/10.1007/BF00288786},
  doi = {10.1007/BF00288786},
  abstract = {It is proposed that distinct anatomical regions of cerebral cortex and of thalamic nuclei are functionally two-dimensional. On this view, the third (radial) dimension of cortical and thalamic structures is associated with a redundancy of circuits and functions so that reliable signal processing obtains in the presence of noisy or ambiguous stimuli.A mathematical model of simple cortical and thalamic nervous tissue is consequently developed, comprising two types of neurons (excitatory and inhibitory), homogeneously distributed in planar sheets, and interacting by way of recurrent lateral connexions. Following a discussion of certain anatomical and physiological restrictions on such interactions, numerical solutions of the relevant non-linear integro-differential equations are obtained. The results fall conveniently into three categories, each of which is postulated to correspond to a distinct type of tissue: sensory neo-cortex, archior prefrontal cortex, and thalamus.The different categories of solution are referred to as dynamical modes. The mode appropriate to thalamus involves a variety of non-linear oscillatory phenomena. That appropriate to archior prefrontal cortex is defined by the existence of spatially inhomogeneous stable steady states which retain contour information about prior stimuli. Finally, the mode appropriate to sensory neo-cortex involves active transient responses. It is shown that this particular mode reproduces some of the phenomenology of visual psychophysics, including spatial modulation transfer function determinations, certain metacontrast effects, and the spatial hysteresis phenomenon found in stereopsis.},
  number = {2},
  journaltitle = {Kybernetik},
  shortjournal = {Kybernetik},
  urldate = {2017-05-12},
  date = {1973-09-01},
  pages = {55--80},
  author = {Wilson, H. R. and Cowan, J. D.},
  file = {/home/fh/lib/articles/Wilson1973.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/P3I7Q8KC/BF00288786.html}
}

@article{Kamyshanska2015,
  title = {The {{Potential Energy}} of an {{Autoencoder}}},
  volume = {37},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2014.2362140},
  abstract = {Autoencoders are popular feature learning models, that are conceptually simple, easy to train and allow for efficient inference. Recent work has shown how certain autoencoders can be associated with an energy landscape, akin to negative log-probability in a probabilistic model, which measures how well the autoencoder can represent regions in the input space. The energy landscape has been commonly inferred heuristically, by using a training criterion that relates the autoencoder to a probabilistic model such as a Restricted Boltzmann Machine (RBM). In this paper we show how most common autoencoders are naturally associated with an energy function, independent of the training procedure, and that the energy landscape can be inferred analytically by integrating the reconstruction function of the autoencoder. For autoencoders with sigmoid hidden units, the energy function is identical to the free energy of an RBM, which helps shed light onto the relationship between these two types of model. We also show that the autoencoder energy function allows us to explain common regularization procedures, such as contractive training, from the perspective of dynamical systems. As a practical application of the energy function, a generative classifier based on class-specific autoencoders is presented.},
  number = {6},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  date = {2015-06},
  pages = {1261--1273},
  keywords = {Probability,Training,Boltzmann machines,learning (artificial intelligence),pattern classification,RBM,class-specific autoencoders,contractive training,energy function,energy landscape,feature learning models,generative classifier,negative log-probability,potential energy,probabilistic model,reconstruction function,regularization procedures,restricted Boltzmann machine,sigmoid hidden units,training criterion,Analytical models,Data models,Principal component analysis,Probabilistic logic,Vectors,Autoencoders,generative classification,representation learning,unsupervised learning},
  author = {Kamyshanska, H. and Memisevic, R.},
  file = {/home/fh/lib/articles/Kamyshanska2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/7EMTPMBK/6918504.html}
}

@article{Hahnloser1998,
  title = {On the Piecewise Analysis of Networks of Linear Threshold Neurons},
  volume = {11},
  issn = {0893-6080},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608098000124},
  doi = {10.1016/S0893-6080(98)00012-4},
  abstract = {The computational abilities of recurrent networks of neurons with a linear activation function above threshold are analyzed. These networks selectively realise a linear mapping of their input. Using this property, the dynamics as well as the number and the stability of stationary states can be investigated. The important property of the boundedness of neural activities can be guaranteed by global inhibition. If used together with self-excitation, the global inhibition gives rise to a multi stable winner-take-all (WTA) mechanism. A condition for a neuron to be a potential winner of the competing dynamics is derived. The network becomes a largest input selector when the self-excitation is marginal.
Slowing down the global inhibition produces oscillations. The study of oscillations of random networks suggests that all cyclic trajectories of linear threshold networks are a result of the existence of partitions with undamped linear oscillations. Chaotic dynamics were never encountered in computer simulations and perhaps do not exist at all in small networks.},
  number = {4},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  urldate = {2017-05-15},
  date = {1998-06},
  pages = {691--697},
  keywords = {Piecewise Linearity,Network Dynamics,Monostable and multistable winner-take-all,Chaos},
  author = {Hahnloser, R. L. T.},
  file = {/home/fh/lib/articles/Hahnloser1998.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/4TJ2CVM4/S0893608098000124.html}
}

@article{Rutishauser2008,
  title = {State-{{Dependent Computation Using Coupled Recurrent Networks}}},
  volume = {21},
  issn = {0899-7667},
  url = {http://dx.doi.org/10.1162/neco.2008.03-08-734},
  doi = {10.1162/neco.2008.03-08-734},
  number = {2},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2017-05-15},
  date = {2008-09-11},
  pages = {478--509},
  author = {Rutishauser, Ueli and Douglas, Rodney J.},
  file = {/home/fh/lib/articles/Rutishauser2008.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/JZRCPPGT/neco.2008.html}
}

@article{Litwin-Kumar2016,
  langid = {english},
  title = {Inhibitory Stabilization and Visual Coding in Cortical Circuits with Multiple Interneuron Subtypes},
  volume = {115},
  issn = {0022-3077, 1522-1598},
  url = {http://jn.physiology.org/content/115/3/1399},
  doi = {10.1152/jn.00732.2015},
  abstract = {Recent anatomical and functional characterization of cortical inhibitory interneurons has highlighted the diverse computations supported by different subtypes of interneurons. However, most theoretical models of cortex do not feature multiple classes of interneurons and rather assume a single homogeneous population. We study the dynamics of recurrent excitatory-inhibitory model cortical networks with parvalbumin (PV)-, somatostatin (SOM)-, and vasointestinal peptide-expressing (VIP) interneurons, with connectivity properties motivated by experimental recordings from mouse primary visual cortex. Our theory describes conditions under which the activity of such networks is stable and how perturbations of distinct neuronal subtypes recruit changes in activity through recurrent synaptic projections. We apply these conclusions to study the roles of each interneuron subtype in disinhibition, surround suppression, and subtractive or divisive modulation of orientation tuning curves. Our calculations and simulations determine the architectural and stimulus tuning conditions under which cortical activity consistent with experiment is possible. They also lead to novel predictions concerning connectivity and network dynamics that can be tested via optogenetic manipulations. Our work demonstrates that recurrent inhibitory dynamics must be taken into account to fully understand many properties of cortical dynamics observed in experiments.},
  number = {3},
  journaltitle = {Journal of Neurophysiology},
  urldate = {2017-05-15},
  date = {2016-03-01},
  pages = {1399--1409},
  author = {Litwin-Kumar, Ashok and Rosenbaum, Robert and Doiron, Brent},
  file = {/home/fh/lib/articles/Litwin-Kumar2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/JWQ8QEC4/1399.html},
  eprinttype = {pmid},
  eprint = {26740531}
}

@article{Binzegger2004,
  langid = {english},
  title = {A {{Quantitative Map}} of the {{Circuit}} of {{Cat Primary Visual Cortex}}},
  volume = {24},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/24/39/8441},
  doi = {10.1523/JNEUROSCI.1400-04.2004},
  abstract = {We developed a quantitative description of the circuits formed in cat area 17 by estimating the ``weight'' of the projections between different neuronal types. To achieve this, we made three-dimensional reconstructions of 39 single neurons and thalamic afferents labeled with horseradish peroxidase during intracellular recordings in vivo. These neurons served as representatives of the different types and provided the morphometrical data about the laminar distribution of the dendritic trees and synaptic boutons and the number of synapses formed by a given type of neuron. Extensive searches of the literature provided the estimates of numbers of the different neuronal types and their distribution across the cortical layers. Applying the simplification that synapses between different cell types are made in proportion to the boutons and dendrites that those cell types contribute to the neuropil in a given layer, we were able to estimate the probable source and number of synapses made between neurons in the six layers. The predicted synaptic maps were quantitatively close to the estimates derived from the experimental electron microscopic studies for the case of the main sources of excitatory and inhibitory input to the spiny stellate cells, which form a major target of layer 4 afferents. The map of the whole cortical circuit shows that there are very few ``strong'' but many ``weak'' excitatory projections, each of which may involve only a few percentage of the total complement of excitatory synapses of a single neuron.},
  number = {39},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2017-05-15},
  date = {2004-09-29},
  pages = {8441--8453},
  keywords = {interlaminar connectivity,synaptic weights,cell types,cell reconstruction,connectivity rule,laminar distribution},
  author = {Binzegger, Tom and Douglas, Rodney J. and Martin, Kevan A. C.},
  file = {/home/fh/lib/articles/Binzegger2004.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/4GWHDTH4/8441.html},
  eprinttype = {pmid},
  eprint = {15456817}
}

@article{Muir2015,
  title = {Eigenspectrum Bounds for Semirandom Matrices with Modular and Spatial Structure for Neural Networks},
  volume = {91},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.91.042808},
  doi = {10.1103/PhysRevE.91.042808},
  abstract = {The eigenvalue spectrum of the matrix of directed weights defining a neural network model is informative of several stability and dynamical properties of network activity. Existing results for eigenspectra of sparse asymmetric random matrices neglect spatial or other constraints in determining entries in these matrices, and so are of partial applicability to cortical-like architectures. Here we examine a parameterized class of networks that are defined by sparse connectivity, with connection weighting modulated by physical proximity (i.e., asymmetric Euclidean random matrices), modular network partitioning, and functional specificity within the excitatory population. We present a set of analytical constraints that apply to the eigenvalue spectra of associated weight matrices, highlighting the relationship between connectivity rules and classes of network dynamics.},
  number = {4},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  urldate = {2017-05-15},
  date = {2015-04-24},
  pages = {042808},
  author = {Muir, Dylan R. and Mrsic-Flogel, Thomas},
  file = {/home/fh/lib/articles/Muir2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/8PTR6BCK/PhysRevE.91.html}
}

@article{Douglas2007,
  title = {Recurrent Neuronal Circuits in the Neocortex},
  volume = {17},
  issn = {0960-9822},
  url = {http://www.sciencedirect.com/science/article/pii/S0960982207012651},
  doi = {10.1016/j.cub.2007.04.024},
  number = {13},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  urldate = {2017-05-16},
  date = {2007-07-03},
  pages = {R496--R500},
  author = {Douglas, Rodney J. and Martin, Kevan A. C.},
  file = {/home/fh/lib/articles/Douglas2007.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/AQJDAK5K/S0960982207012651.html}
}

@article{Neftci2013,
  langid = {english},
  title = {Synthesizing Cognition in Neuromorphic Electronic Systems},
  volume = {110},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/110/37/E3468},
  doi = {10.1073/pnas.1212083110},
  abstract = {The quest to implement intelligent processing in electronic neuromorphic systems lacks methods for achieving reliable behavioral dynamics on substrates of inherently imprecise and noisy neurons. Here we report a solution to this problem that involves first mapping an unreliable hardware layer of spiking silicon neurons into an abstract computational layer composed of generic reliable subnetworks of model neurons and then composing the target behavioral dynamics as a ``soft state machine'' running on these reliable subnets. In the first step, the neural networks of the abstract layer are realized on the hardware substrate by mapping the neuron circuit bias voltages to the model parameters. This mapping is obtained by an automatic method in which the electronic circuit biases are calibrated against the model parameters by a series of population activity measurements. The abstract computational layer is formed by configuring neural networks as generic soft winner-take-all subnetworks that provide reliable processing by virtue of their active gain, signal restoration, and multistability. The necessary states and transitions of the desired high-level behavior are then easily embedded in the computational layer by introducing only sparse connections between some neurons of the various subnets. We demonstrate this synthesis method for a neuromorphic sensory agent that performs real-time context-dependent classification of motion patterns observed by a silicon retina.},
  number = {37},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  urldate = {2017-05-16},
  date = {2013-10-09},
  pages = {E3468--E3476},
  keywords = {decision making,sensorimotor,working memory,analog very large-scale integration,artificial neural systems},
  author = {Neftci, Emre and Binas, Jonathan and Rutishauser, Ueli and Chicca, Elisabetta and Indiveri, Giacomo and Douglas, Rodney J.},
  file = {/home/fh/lib/articles/Neftci2013.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/RKXATFSF/E3468.html},
  eprinttype = {pmid},
  eprint = {23878215}
}

@article{Muir2014,
  title = {Anatomical {{Constraints}} on {{Lateral Competition}} in {{Columnar Cortical Architectures}}},
  volume = {26},
  issn = {0899-7667},
  url = {http://dx.doi.org/10.1162/NECO_a_00613},
  doi = {10.1162/NECO_a_00613},
  number = {8},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2017-05-16},
  date = {2014-05-30},
  pages = {1624--1666},
  author = {Muir, Dylan R. and Cook, Matthew},
  file = {/home/fh/lib/articles/Muir2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/G6BQZED5/NECO_a_00613.html}
}

@article{Atallah2012,
  title = {Parvalbumin-{{Expressing Interneurons Linearly Transform Cortical Responses}} to {{Visual Stimuli}}},
  volume = {73},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627311010944},
  doi = {10.1016/j.neuron.2011.12.013},
  abstract = {Summary
The response of cortical neurons to a sensory stimulus is shaped by the network in which they are embedded. Here we establish a role of parvalbumin (PV)-expressing cells, a large class of inhibitory neurons that target the soma and perisomatic compartments of pyramidal cells, in controlling cortical responses. By bidirectionally manipulating PV cell activity in visual cortex we show that these neurons strongly modulate layer 2/3 pyramidal cell spiking responses to visual stimuli while only modestly affecting their tuning properties. PV cells' impact on pyramidal cells is captured by a linear transformation, both additive and multiplicative, with a threshold. These results indicate that PV cells are ideally suited to modulate cortical gain and establish a causal relationship between a select neuron type and specific computations performed by the cortex during sensory processing.},
  number = {1},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2017-05-16},
  date = {2012-01-12},
  pages = {159--170},
  author = {Atallah, Bassam V. and Bruns, William and Carandini, Matteo and Scanziani, Massimo},
  file = {/home/fh/lib/articles/Atallah2012.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/G2AASBC4/S0896627311010944.html}
}

@article{Bourne2011,
  langid = {english},
  title = {Coordination of Size and Number of Excitatory and Inhibitory Synapses Results in a Balanced Structural Plasticity along Mature Hippocampal {{CA1}} Dendrites during {{LTP}}},
  volume = {21},
  issn = {1098-1063},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/hipo.20768/abstract},
  doi = {10.1002/hipo.20768},
  abstract = {Enlargement of dendritic spines and synapses correlates with enhanced synaptic strength during long-term potentiation (LTP), especially in immature hippocampal neurons. Less clear is the nature of this structural synaptic plasticity on mature hippocampal neurons, and nothing is known about the structural plasticity of inhibitory synapses during LTP. Here the timing and extent of structural synaptic plasticity and changes in local protein synthesis evidenced by polyribosomes were systematically evaluated at both excitatory and inhibitory synapses on CA1 dendrites from mature rats following induction of LTP with theta-burst stimulation (TBS). Recent work suggests dendritic segments can act as functional units of plasticity. To test whether structural synaptic plasticity is similarly coordinated, we reconstructed from serial section transmission electron microscopy all of the spines and synapses along representative dendritic segments receiving control stimulation or TBS-LTP. At 5 min after TBS, polyribosomes were elevated in large spines suggesting an initial burst of local protein synthesis, and by 2 h only those spines with further enlarged synapses contained polyribosomes. Rapid induction of synaptogenesis was evidenced by an elevation in asymmetric shaft synapses and stubby spines at 5 min and more nonsynaptic filopodia at 30 min. By 2 h, the smallest synaptic spines were markedly reduced in number. This synapse loss was perfectly counterbalanced by enlargement of the remaining excitatory synapses such that the summed synaptic surface area per length of dendritic segment was constant across time and conditions. Remarkably, the inhibitory synapses showed a parallel synaptic plasticity, also demonstrating a decrease in number perfectly counterbalanced by an increase in synaptic surface area. Thus, TBS-LTP triggered spinogenesis followed by loss of small excitatory and inhibitory synapses and a subsequent enlargement of the remaining synapses by 2 h. These data suggest that dendritic segments coordinate structural plasticity across multiple synapses and maintain a homeostatic balance of excitatory and inhibitory inputs through local protein-synthesis and selective capture or redistribution of dendritic resources. \textcopyright{}2010 Wiley-Liss, Inc.},
  number = {4},
  journaltitle = {Hippocampus},
  shortjournal = {Hippocampus},
  urldate = {2017-05-17},
  date = {2011-04-01},
  pages = {354--373},
  keywords = {dendritic spine,serial section transmission electron microscopy,3D reconstruction,polyribosomes,hippocampal slice,adult,ultrastructure},
  author = {Bourne, Jennifer N. and Harris, Kristen M.},
  file = {/home/fh/lib/articles/Bourne2011.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/XZ93NQF7/abstract.html}
}

@article{Sharpee2016,
  title = {25th {{Annual Computational Neuroscience Meeting}}: {{CNS}}-2016},
  volume = {17},
  issn = {1471-2202},
  url = {http://dx.doi.org/10.1186/s12868-016-0283-6},
  doi = {10.1186/s12868-016-0283-6},
  shorttitle = {25th {{Annual Computational Neuroscience Meeting}}},
  abstract = {A1 Functional advantages of cell-type heterogeneity in neural circuits},
  number = {1},
  journaltitle = {BMC Neuroscience},
  shortjournal = {BMC Neuroscience},
  urldate = {2017-05-17},
  date = {2016},
  pages = {54},
  author = {Sharpee, Tatyana O. and Destexhe, Alain and Kawato, Mitsuo and Sekuli{\'c}, Vladislav and Skinner, Frances K. and W{\'o}jcik, Daniel K. and Chintaluri, Chaitanya and Cserp{\'a}n, Dorottya and Somogyv{\'a}ri, Zolt{\'a}n and Kim, Jae Kyoung and Kilpatrick, Zachary P. and Bennett, Matthew R. and Josi{\'c}, Kresimir and Elices, Irene and Arroyo, David and Levi, Rafael and Rodriguez, Francisco B. and Varona, Pablo and Hwang, Eunjin and Kim, Bowon and Han, Hio-Been and Kim, Tae and McKenna, James T. and Brown, Ritchie E. and McCarley, Robert W. and Choi, Jee Hyun and Rankin, James and Popp, Pamela Osborn and Rinzel, John and Tabas, Alejandro and Rupp, Andr{\'e} and Balaguer-Ballester, Emili and Maturana, Matias I. and Grayden, David B. and Cloherty, Shaun L. and Kameneva, Tatiana and Ibbotson, Michael R. and Meffin, Hamish and Koren, Veronika and Lochmann, Timm and Dragoi, Valentin and Obermayer, Klaus and Psarrou, Maria and Schilstra, Maria and Davey, Neil and Torben-Nielsen, Benjamin and Steuber, Volker and Ju, Huiwen and Yu, Jiao and Hines, Michael L. and Chen, Liang and Yu, Yuguo and Kim, Jimin and Leahy, Will and Shlizerman, Eli and Birgiolas, Justas and Gerkin, Richard C. and Crook, Sharon M. and Viriyopase, Atthaphon and Memmesheimer, Raoul-Martin and Gielen, Stan and Dabaghian, Yuri and DeVito, Justin and Perotti, Luca and Kim, Anmo J. and Fenk, Lisa M. and Cheng, Cheng and Maimon, Gaby and Zhao, Chang and Widmer, Yves and Sprecher, Simon and Senn, Walter and Halnes, Geir and M{\"a}ki-Marttunen, Tuomo and Keller, Daniel and Pettersen, Klas H. and Andreassen, Ole A. and Einevoll, Gaute T. and Yamada, Yasunori and Steyn-Ross, Moira L. and Alistair Steyn-Ross, D. and Mejias, Jorge F. and Murray, John D. and Kennedy, Henry and Wang, Xiao-Jing and Kruscha, Alexandra and Grewe, Jan and Benda, Jan and Lindner, Benjamin and Badel, Laurent and Ohta, Kazumi and Tsuchimoto, Yoshiko and Kazama, Hokto and Kahng, B. and Tam, Nicoladie D. and Pollonini, Luca and Zouridakis, George and Soh, Jaehyun and Kim, DaeEun and Yoo, Minsu and Palmer, S. E. and Culmone, Viviana and Bojak, Ingo and Ferrario, Andrea and Merrison-Hort, Robert and Borisyuk, Roman and Kim, Chang Sub and Tezuka, Taro and Joo, Pangyu and Rho, Young-Ah and Burton, Shawn D. and Bard Ermentrout, G. and Jeong, Jaeseung and Urban, Nathaniel N. and Marsalek, Petr and Kim, Hoon-Hee and Moon, Seok-hyun and Lee, Do-won and Lee, Sung-beom and Lee, Ji-yong and Molkov, Yaroslav I. and Hamade, Khaldoun and Teka, Wondimu and Barnett, William H. and Kim, Taegyo and Markin, Sergey and Rybak, Ilya A. and Forro, Csaba and Dermutz, Harald and Demk{\'o}, L{\'a}szl{\'o} and V{\"o}r{\"o}s, J{\'a}nos and Babichev, Andrey and Huang, Haiping and Verduzco-Flores, Sergio and Dos Santos, Filipa and Andras, Peter and Metzner, Christoph and Schweikard, Achim and Zurowski, Bartosz and Roach, James P. and Sander, Leonard M. and Zochowski, Michal R. and Skilling, Quinton M. and Ognjanovski, Nicolette and Aton, Sara J. and Zochowski, Michal and Wang, Sheng-Jun and Ouyang, Guang and Guang, Jing and Zhang, Mingsha and Michael Wong, K. Y. and Zhou, Changsong and Robinson, Peter A. and Sanz-Leon, Paula and Drysdale, Peter M. and Fung, Felix and Abeysuriya, Romesh G. and Rennie, Chris J. and Zhao, Xuelong and Choe, Yoonsuck and Yang, Huei-Fang and Mi, Yuanyuan and Lin, Xiaohan and Wu, Si and Liedtke, Joscha and Schottdorf, Manuel and Wolf, Fred and Yamamura, Yoriko and Wickens, Jeffery R. and Rumbell, Timothy and Ramsey, Julia and Reyes, Amy and Dragulji{\'c}, Danel and Hof, Patrick R. and Luebke, Jennifer and Weaver, Christina M. and He, Hu and Yang, Xu and Ma, Hailin and Xu, Zhiheng and Wang, Yuzhe and Baek, Kwangyeol and Morris, Laurel S. and Kundu, Prantik and Voon, Valerie and Agnes, Everton J. and Vogels, Tim P. and Podlaski, William F. and Giese, Martin and Kuravi, Pradeep and Vogels, Rufin and Seeholzer, Alexander and Podlaski, William and Ranjan, Rajnish and Vogels, Tim and Torres, Joaquin J. and Baroni, Fabiano and Latorre, Roberto and Gips, Bart and Lowet, Eric and Roberts, Mark J. and de Weerd, Peter and Jensen, Ole and van der Eerden, Jan and Goodarzinick, Abdorreza and Niry, Mohammad D. and Valizadeh, Alireza and Pariz, Aref and Parsi, Shervin S. and Warburton, Julia M. and Marucci, Lucia and Tamagnini, Francesco and Brown, Jon and Tsaneva-Atanasova, Krasimira and Kleberg, Florence I. and Triesch, Jochen and Moezzi, Bahar and Iannella, Nicolangelo and Schaworonkow, Natalie and Plogmacher, Lukas and Goldsworthy, Mitchell R. and Hordacre, Brenton and McDonnell, Mark D. and Ridding, Michael C. and Zapotocky, Martin and Smit, Daniel and Fouquet, Coralie and Trembleau, Alain and Dasgupta, Sakyasingha and Nishikawa, Isao and Aihara, Kazuyuki and Toyoizumi, Taro and Robb, Daniel T. and Mellen, Nick and Toporikova, Natalia and Tang, Rongxiang and Tang, Yi-Yuan and Liang, Guangsheng and Kiser, Seth A. and Howard, James H. and Goncharenko, Julia and Voronenko, Sergej O. and Ahamed, Tosif and Stephens, Greg and Yger, Pierre and Lefebvre, Baptiste and Spampinato, Giulia Lia Beatrice and Esposito, Elric and et Olivier Marre, Marcel Stimberg and Choi, Hansol and Song, Min-Ho and Chung, SueYeon and Lee, Dan D. and Sompolinsky, Haim and Phillips, Ryan S. and Smith, Jeffrey and Chatzikalymniou, Alexandra Pierri and Ferguson, Katie and Alex Cayco Gajic, N. and Clopath, Claudia and Angus Silver, R. and Gleeson, Padraig and Marin, Boris and Sadeh, Sadra and Quintana, Adrian and Cantarelli, Matteo and Dura-Bernal, Salvador and Lytton, William W. and Davison, Andrew and Li, Luozheng and Zhang, Wenhao and Wang, Dahui and Song, Youngjo and Park, Sol and Choi, Ilhwan and Shin, Hee-sup and Choi, Hannah and Pasupathy, Anitha and Shea-Brown, Eric and Huh, Dongsung and Sejnowski, Terrence J. and Vogt, Simon M. and Kumar, Arvind and Schmidt, Robert and Van Wert, Stephen and Schiff, Steven J. and Veale, Richard and Scheutz, Matthias and Lee, Sang Wan and Gallinaro, J{\'u}lia and Rotter, Stefan and Rubchinsky, Leonid L. and Cheung, Chung Ching and Ratnadurai-Giridharan, Shivakeshavan and Shomali, Safura Rashid and Ahmadabadi, Majid Nili and Shimazaki, Hideaki and Nader Rasuli, S. and Zhao, Xiaochen and Rasch, Malte J. and {etal}},
  options = {useprefix=true},
  file = {/home/fh/lib/articles/Sharpee2016.pdf}
}

@inproceedings{2017,
  title = {Cosyne},
  date = {2017},
  file = {/home/fh/lib/conferences/2017_Cosyne.pdf}
}

@inproceedings{2016,
  title = {Cosyne},
  date = {2016},
  file = {/home/fh/lib/conferences/2016_Cosyne.pdf}
}

@inproceedings{2015,
  title = {Cosyne},
  date = {2015},
  file = {/home/fh/lib/conferences/2015_Cosyne.pdf}
}

@article{Timme2016,
  title = {High-{{Degree Neurons Feed Cortical Computations}}},
  volume = {12},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004858},
  doi = {10.1371/journal.pcbi.1004858},
  abstract = {Author Summary We recorded the electrical activity of hundreds of neurons simultaneously in brain tissue from mice and we analyzed these signals using state-of-the-art tools from information theory. These tools allowed us to ascertain which neurons were transmitting information to other neurons and to characterize the computations performed by neurons using the inputs they received from two or more other neurons. We found that computations did not occur equally in all neurons throughout the networks. Surprisingly, neurons that computed large amounts of information tended to be recipients of information from neurons with a large number of outgoing connections. Interestingly, the number of incoming connections to a neuron was not related to the amount of information that neuron computed. To better understand these results, we built a network model to match the data. Unexpectedly, the model also maximized information transfer in the presence of network-wide correlations. This suggested a way that networks of cortical neurons could deal with common random background input. These results are the first to show that the amount of information computed by a neuron depends on where it is located in the surrounding network.},
  number = {5},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2017-05-18},
  date = {2016-05-09},
  pages = {e1004858},
  keywords = {neural networks,Neurons,Action potentials,Signaling networks,Information theory,Network analysis,entropy,Communications},
  author = {Timme, Nicholas M. and Ito, Shinya and Myroshnychenko, Maxym and Nigam, Sunny and Shimono, Masanori and Yeh, Fang-Chin and Hottowy, Pawel and Litke, Alan M. and Beggs, John M.},
  file = {/home/fh/lib/articles/Timme2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/4X6PKKT8/article.html}
}

@article{Dettner2016,
  langid = {english},
  title = {Temporal Pairwise Spike Correlations Fully Capture Single-Neuron Information},
  volume = {7},
  issn = {2041-1723},
  url = {http://www.nature.com/ncomms/2016/161215/ncomms13805/full/ncomms13805.html},
  doi = {10.1038/ncomms13805},
  abstract = {To understand the neural code it is important to determine what spiking features contain the relevant information. Here, the authors use mathematical approaches to show that two pair-wise correlation functions, the autocorrelation function within spike trains and cross-correlation function across stimulus presentations, fully determine the neural information content.},
  journaltitle = {Nature Communications},
  urldate = {2017-05-22},
  date = {2016-12-15},
  pages = {13805},
  author = {Dettner, Amadeus and M{\"u}nzberg, Sabrina and Tchumatchenko, Tatjana},
  file = {/home/fh/lib/articles/Dettner2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/ITHAMRSX/ncomms13805.html}
}

@article{Villa2016,
  langid = {english},
  title = {Inhibitory {{Synapses Are Repeatedly Assembled}} and {{Removed}} at {{Persistent Sites In Vivo}}},
  volume = {89},
  issn = {08966273},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627316000118},
  doi = {10.1016/j.neuron.2016.01.010},
  number = {4},
  journaltitle = {Neuron},
  urldate = {2017-05-22},
  date = {2016-02},
  pages = {756--769},
  author = {Villa, Katherine L. and Berry, Kalen P. and Subramanian, Jaichandar and Cha, Jae Won and Oh, Won Chan and Kwon, Hyung-Bae and Kubota, Yoshiyuki and So, Peter T.C. and Nedivi, Elly},
  file = {/home/fh/lib/articles/Villa2016.pdf}
}

@article{Kanders2017,
  langid = {english},
  title = {Avalanche and Edge-of-Chaos Criticality Do Not Necessarily Co-Occur in Neural Networks},
  volume = {27},
  issn = {1054-1500, 1089-7682},
  url = {http://aip.scitation.org/doi/10.1063/1.4978998},
  doi = {10.1063/1.4978998},
  number = {4},
  journaltitle = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  urldate = {2017-05-22},
  date = {2017-04},
  pages = {047408},
  author = {Kanders, Karlis and Lorimer, Tom and Stoop, Ruedi},
  file = {/home/fh/lib/articles/Kanders2017.pdf}
}

@article{Takahashi2016,
  langid = {english},
  title = {Active Cortical Dendrites Modulate Perception},
  volume = {354},
  issn = {0036-8075, 1095-9203},
  url = {http://www.sciencemag.org/lookup/doi/10.1126/science.aah6066},
  doi = {10.1126/science.aah6066},
  number = {6319},
  journaltitle = {Science},
  urldate = {2017-05-22},
  date = {2016-12-23},
  pages = {1587--1590},
  author = {Takahashi, Naoya and Oertner, Thomas G. and Hegemann, Peter and Larkum, Matthew E.},
  file = {/home/fh/lib/articles/Takahashi2016.pdf}
}

@article{Buzsaki2014,
  langid = {english},
  title = {The Log-Dynamic Brain: How Skewed Distributions Affect Network Operations},
  volume = {15},
  issn = {1471-003X},
  url = {http://www.nature.com/nrn/journal/v15/n4/full/nrn3687.html},
  doi = {10.1038/nrn3687},
  shorttitle = {The Log-Dynamic Brain},
  abstract = {We often assume that the variables of functional and structural brain parameters \textemdash{} such as synaptic weights, the firing rates of individual neurons, the synchronous discharge of neural populations, the number of synaptic contacts between neurons and the size of dendritic boutons \textemdash{} have a bell-shaped distribution. However, at many physiological and anatomical levels in the brain, the distribution of numerous parameters is in fact strongly skewed with a heavy tail, suggesting that skewed (typically lognormal) distributions are fundamental to structural and functional brain organization. This insight not only has implications for how we should collect and analyse data, it may also help us to understand how the different levels of skewed distributions \textemdash{} from synapses to cognition \textemdash{} are related to each other.},
  number = {4},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  urldate = {2017-05-23},
  date = {2014-04},
  pages = {264--278},
  keywords = {Cellular neuroscience,Synaptic Transmission,Spine regulation and structure},
  author = {Buzs{\'a}ki, Gy{\"o}rgy and Mizuseki, Kenji},
  file = {/home/fh/lib/articles/Buzsáki2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/9TCHJH6X/nrn3687.html}
}

@report{zotero-1878,
  title = {Sansmath},
  file = {/home/fh/lib/manuals/latex/sansmath.pdf},
  note = {manuals/latex}
}

@book{Spivak1994,
  langid = {english},
  location = {{Houston, Tex}},
  title = {Calculus},
  edition = {3. ed},
  isbn = {978-0-914098-89-8},
  pagetotal = {670},
  publisher = {{Perish}},
  date = {1994},
  keywords = {Analysis,Infinitesimalrechnung},
  author = {Spivak, Michael},
  file = {/home/fh/lib/books/Spivak1994_Calculus.pdf},
  note = {OCLC: 31441929}
}

@book{Spivak1996,
  langid = {english},
  location = {{Houston, TX}},
  title = {Answer {{Book}} to {{Calculus}}},
  isbn = {978-0-914098-90-4},
  publisher = {{Publish or Perish}},
  date = {1996},
  author = {Spivak, Michael},
  file = {/home/fh/lib/books/Spivak1996_Answer-Book-to-Calculus.pdf},
  note = {OCLC: 263629218}
}

@book{Strang2010,
  langid = {english},
  location = {{Wellesley, Mass}},
  title = {Calculus},
  edition = {2. ed},
  isbn = {978-0-9802327-4-5},
  publisher = {{Wellesley-Cambridge Press}},
  date = {2010},
  keywords = {Lehrbuch,Analysis,Mathematik},
  author = {Strang, Gilbert},
  file = {/home/fh/lib/books/Strang2010_Calculus.pdf},
  note = {OCLC: 820373854}
}

@book{Ross2013,
  location = {{New York}},
  title = {Elementary Analysis: The Theory of Calculus},
  isbn = {978-1-4614-6270-5},
  shorttitle = {Elementary Analysis},
  publisher = {{Springer}},
  date = {2013},
  author = {Ross, Kenneth A.},
  file = {/home/fh/lib/books/Ross2013_Elementary-analysis-the-theory-of-calculus.pdf}
}

@book{Rudin1976,
  location = {{New York}},
  title = {Principles of Mathematical Analysis},
  edition = {3d ed},
  isbn = {978-0-07-054235-8},
  pagetotal = {342},
  series = {International series in pure and applied mathematics},
  publisher = {{McGraw-Hill}},
  date = {1976},
  keywords = {Mathematical analysis},
  author = {Rudin, Walter},
  file = {/home/fh/lib/books/Rudin1976_Principles-of-mathematical-analysis.pdf}
}

@article{Doya2007,
  title = {Reinforcement Learning: {{Computational}} Theory and Biological Mechanisms},
  volume = {1},
  issn = {1955-2068},
  url = {http://www.tandfonline.com/doi/abs/10.2976/1.2732246/10.2976/1},
  doi = {10.2976/1.2732246/10.2976/1},
  shorttitle = {Reinforcement Learning},
  abstract = {Reinforcement learning is a computational framework for an active agent to learn behaviors on the basis of a scalar reward signal. The agent can be an animal, a human, or an artificial system such as a robot or a computer program. The reward can be food, water, money, or whatever measure of the performance of the agent. The theory of reinforcement learning, which was developed in an artificial intelligence community with intuitions from animal learning theory, is now giving a coherent account on the function of the basal ganglia. It now serves as the ``common language'' in which biologists, engineers, and social scientists can exchange their problems and findings. This article reviews the basic theoretical framework of reinforcement learning and discusses its recent and future contributions toward the understanding of animal behaviors and human decision making.},
  number = {1},
  journaltitle = {HFSP Journal},
  urldate = {2017-05-28},
  date = {2007-05-01},
  pages = {30--40},
  author = {Doya, Kenji},
  file = {/home/fh/lib/articles/Doya2007.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/KMV67ENT/1.html},
  eprinttype = {pmid},
  eprint = {19404458}
}

@article{Barto1983,
  title = {Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems},
  volume = {SMC-13},
  issn = {0018-9472},
  doi = {10.1109/TSMC.1983.6313077},
  abstract = {It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.},
  number = {5},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics},
  date = {1983-09},
  pages = {834--846},
  keywords = {Neurons,Training,adaptive control,learning systems,neural nets,adaptive critic element,animal learning studies,associative search element,learning control problem,movable cart,neuronlike adaptive elements,Adaptive systems,Biological neural networks,Pattern recognition,Problem-solving,Supervised learning},
  author = {Barto, A. G. and Sutton, R. S. and Anderson, C. W.},
  file = {/home/fh/lib/articles/Barto1983.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/T9DPZP85/6313077.html}
}

@article{Sutton1988,
  langid = {english},
  title = {Learning to Predict by the Methods of Temporal Differences},
  volume = {3},
  issn = {0885-6125, 1573-0565},
  url = {https://link.springer.com/article/10.1007/BF00115009},
  doi = {10.1007/BF00115009},
  abstract = {This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
  number = {1},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  urldate = {2017-05-28},
  date = {1988-08-01},
  pages = {9--44},
  author = {Sutton, Richard S.},
  file = {/home/fh/lib/articles/Sutton1988.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/Q59QT9TI/BF00115009.html}
}

@article{Zenke2017,
  title = {The Temporal Paradox of {{Hebbian}} Learning and Homeostatic Plasticity},
  volume = {43},
  issn = {0959-4388},
  url = {http://www.sciencedirect.com/science/article/pii/S0959438817300910},
  doi = {10.1016/j.conb.2017.03.015},
  abstract = {Hebbian plasticity, a synaptic mechanism which detects and amplifies co-activity between neurons, is considered a key ingredient underlying learning and memory in the brain. However, Hebbian plasticity alone is unstable, leading to runaway neuronal activity, and therefore requires stabilization by additional compensatory processes. Traditionally, a diversity of homeostatic plasticity phenomena found in neural circuits is thought to play this role. However, recent modelling work suggests that the slow evolution of homeostatic plasticity, as observed in experiments, is insufficient to prevent instabilities originating from Hebbian plasticity. To remedy this situation, we suggest that homeostatic plasticity is complemented by additional rapid compensatory processes, which rapidly stabilize neuronal activity on short timescales.},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  series = {Neurobiology of Learning and Plasticity},
  urldate = {2017-05-30},
  date = {2017-04},
  pages = {166--176},
  author = {Zenke, Friedemann and Gerstner, Wulfram and Ganguli, Surya},
  file = {/home/fh/lib/articles/Zenke2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/9QMWM388/S0959438817300910.html}
}

@article{Sprekeler2017,
  title = {Functional Consequences of Inhibitory Plasticity: Homeostasis, the Excitation-Inhibition Balance and Beyond},
  volume = {43},
  issn = {0959-4388},
  url = {http://www.sciencedirect.com/science/article/pii/S0959438817300909},
  doi = {10.1016/j.conb.2017.03.014},
  shorttitle = {Functional Consequences of Inhibitory Plasticity},
  abstract = {Computational neuroscience has a long-standing tradition of investigating the consequences of excitatory synaptic plasticity. In contrast, the functions of inhibitory plasticity are still largely nebulous, particularly given the bewildering diversity of interneurons in the brain. Here, we review recent computational advances that provide first suggestions for the functional roles of inhibitory plasticity, such as a maintenance of the excitation-inhibition balance, a stabilization of recurrent network dynamics and a decorrelation of sensory responses. The field is still in its infancy, but given the existing body of theory for excitatory plasticity, it is likely to mature quickly and deliver important insights into the self-organization of inhibitory circuits in the brain.},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  series = {Neurobiology of Learning and Plasticity},
  urldate = {2017-05-30},
  date = {2017-04},
  pages = {198--203},
  author = {Sprekeler, Henning},
  file = {/home/fh/lib/articles/Sprekeler2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/MR2BDMV5/S0959438817300909.html}
}

@article{Zenke2017a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.04200},
  primaryClass = {cs, q-bio, stat},
  title = {Improved Multitask Learning through Synaptic Intelligence},
  url = {http://arxiv.org/abs/1703.04200},
  abstract = {Deep learning has led to remarkable advances when applied to problems where the data distribution does not change over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, and solve a diversity of tasks simultaneously. Furthermore, synapses in biological neurons are not simply real-valued scalars, but possess complex molecular machinery enabling non-trivial learning dynamics. In this study, we take a first step toward bringing this biological complexity into artificial neural networks. We introduce a model of intelligent synapses that accumulate task relevant information over time, and exploit this information to efficiently consolidate memories of old tasks to protect them from being overwritten as new tasks are learned. We apply our framework to learning sequences of related classification problems, and show that it dramatically reduces catastrophic forgetting while maintaining computational efficiency.},
  urldate = {2017-05-30},
  date = {2017-03-12},
  keywords = {Quantitative Biology - Neurons and Cognition,Computer Science - Learning,Statistics - Machine Learning},
  author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
  file = {/home/fh/lib/articles/Zenke2017_2.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/IJEF4HA2/1703.html}
}

@unpublished{Pfeiffer2006,
  title = {Reinforcement Learning Theory},
  date = {2006},
  author = {Pfeiffer, Michael},
  file = {/home/fh/lib/manuscripts/Pfeiffer2006_Reinforcement-learning-theory.pdf}
}

@book{Goodfellow2016,
  location = {{Cambridge, Massachusetts}},
  title = {Deep Learning},
  isbn = {978-0-262-03561-3},
  pagetotal = {775},
  series = {Adaptive computation and machine learning},
  publisher = {{The MIT Press}},
  date = {2016},
  keywords = {Machine learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  file = {/home/fh/lib/books/Goodfellow2016_Deep-learning.pdf}
}

@book{Mohri2012,
  location = {{Cambridge, MA}},
  title = {Foundations of Machine Learning},
  isbn = {978-0-262-01825-8},
  pagetotal = {414},
  series = {Adaptive computation and machine learning series},
  publisher = {{MIT Press}},
  date = {2012},
  keywords = {Machine learning,Computer algorithms},
  author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  file = {/home/fh/lib/books/Mohri2012_Foundations-of-machine-learning.pdf}
}

@book{Sutton2016,
  title = {Reinforcement {{Learning}}: {{An Introduction}} - {{Sep2016Draft}}},
  date = {2016},
  author = {Sutton},
  file = {/home/fh/lib/books/Sutton2016_Reinforcement-Learning-An-Introduction---Sep2016Draft.pdf}
}

@article{Doiron2016,
  langid = {english},
  title = {The Mechanics of State-Dependent Neural Correlations},
  volume = {19},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v19/n3/abs/nn.4242.html},
  doi = {10.1038/nn.4242},
  abstract = {Simultaneous recordings from large neural populations are becoming increasingly common. An important feature of population activity is the trial-to-trial correlated fluctuation of spike train outputs from recorded neuron pairs. Similar to the firing rate of single neurons, correlated activity can be modulated by a number of factors, from changes in arousal and attentional state to learning and task engagement. However, the physiological mechanisms that underlie these changes are not fully understood. We review recent theoretical results that identify three separate mechanisms that modulate spike train correlations: changes in input correlations, internal fluctuations and the transfer function of single neurons. We first examine these mechanisms in feedforward pathways and then show how the same approach can explain the modulation of correlations in recurrent networks. Such mechanistic constraints on the modulation of population activity will be important in statistical analyses of high-dimensional neural data.
View full text},
  number = {3},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2017-06-15},
  date = {2016-03},
  pages = {383--393},
  keywords = {computational neuroscience,Neural circuits,Biophysical models},
  author = {Doiron, Brent and Litwin-Kumar, Ashok and Rosenbaum, Robert and Ocker, Gabriel K. and Josi{\'c}, Kre{\v s}imir},
  file = {/home/fh/lib/articles/Doiron2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/GMP4I8W5/nn.4242.html}
}

@report{zotero-null-1942,
  title = {Booktabs},
  file = {/home/fh/lib/manuals/latex/booktabs.pdf},
  note = {manuals/latex}
}

@article{Hennequin2016,
  langid = {english},
  title = {Inhibitory {{Plasticity}}: {{Balance}}, {{Control}}, and {{Codependence}}},
  url = {http://www.annualreviews.org/doi/10.1146/annurev-neuro-072116-031005},
  shorttitle = {Inhibitory {{Plasticity}}},
  abstract = {Inhibitory neurons, although relatively few in number, exert powerful control over brain circuits. They stabilize network activity in the face of strong feedback excitation and actively engage in computations. Recent studies reveal the importance of a precise balance of excitation and inhibition in neural circuits, which often requires exquisite fine-tuning of inhibitory connections. Wereview inhibitory synaptic plasticity and its roles in shaping both feedforward and feedback control. We discuss the necessity of complex, codependent plasticity mechanisms to build nontrivial, functioning networks, and we end by summarizing experimental evidence of such interactions. Expected final online publication date for the Annual Review of Neuroscience Volume 40 is July 8, 2017. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
  journaltitle = {https://doi.org/10.1146/annurev-neuro-072116-031005},
  urldate = {2017-06-10},
  date = {2016-08-22},
  author = {Hennequin, Guillaume and Agnes, Everton J. and Vogels, Tim P.},
  file = {/home/fh/lib/articles/Hennequin2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/A7HUC75Z/annurev-neuro-072116-031005.html}
}

@article{Froemke2015,
  title = {Plasticity of {{Cortical Excitatory}}-{{Inhibitory Balance}}},
  volume = {38},
  url = {https://doi.org/10.1146/annurev-neuro-071714-034002},
  doi = {10.1146/annurev-neuro-071714-034002},
  abstract = {Synapses are highly plastic and are modified by changes in patterns of neural activity or sensory experience. Plasticity of cortical excitatory synapses is thought to be important for learning and memory, leading to alterations in sensory representations and cognitive maps. However, these changes must be coordinated across other synapses within local circuits to preserve neural coding schemes and the organization of excitatory and inhibitory inputs, i.e., excitatory-inhibitory balance. Recent studies indicate that inhibitory synapses are also plastic and are controlled directly by a large number of neuromodulators, particularly during episodes of learning. Many modulators transiently alter excitatory-inhibitory balance by decreasing inhibition, and thus disinhibition has emerged as a major mechanism by which neuromodulation might enable long-term synaptic modifications naturally. This review examines the relationships between neuromodulation and synaptic plasticity, focusing on the induction of long-term changes that collectively enhance cortical excitatory-inhibitory balance for improving perception and behavior.},
  number = {1},
  journaltitle = {Annual Review of Neuroscience},
  urldate = {2017-06-12},
  date = {2015},
  pages = {195--219},
  author = {Froemke, Robert C.},
  file = {/home/fh/lib/articles/Froemke2015.pdf},
  eprinttype = {pmid},
  eprint = {25897875}
}

@report{zotero-null-1947,
  title = {Siunitx},
  file = {/home/fh/lib/manuals/latex/siunitx.pdf},
  note = {manuals/latex}
}

@article{Lucke2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.04812},
  primaryClass = {stat},
  title = {K-{{Means}} Is a {{Variational EM Approximation}} of {{Gaussian Mixture Models}}},
  url = {http://arxiv.org/abs/1704.04812},
  abstract = {We show that k-means (Lloyd's algorithm) is equivalent to a variational EM approximation of a Gaussian Mixture Model (GMM) with isotropic Gaussians. The k-means algorithm is obtained if truncated posteriors are used as variational distributions. In contrast to the standard way to relate k-means and GMMs, we show that it is not required to consider the limit case of Gaussians with zero variance. There are a number of consequences following from our observation: (A) k-means can be shown to monotonously increase the free-energy associated with truncated distributions; (B) Using the free-energy, we can derive an explicit and compact formula of a lower GMM likelihood bound which uses the k-means objective as argument; (C) We can generalize k-means using truncated variational EM, and relate such generalizations to other k-means-like algorithms. In general, truncated variational EM provides a natural and quantitative link between k-means-like clustering and GMM clustering algorithms which may be very relevant for future theoretical as well as empirical studies.},
  urldate = {2017-06-13},
  date = {2017-04-16},
  keywords = {Statistics - Machine Learning,62H30},
  author = {L{\"u}cke, J{\"o}rg and Forster, Dennis},
  file = {/home/fh/lib/articles/Lücke2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/7VMDFTZK/1704.html}
}

@article{Rubinski2015,
  title = {Remodeling and {{Tenacity}} of {{Inhibitory Synapses}}: {{Relationships}} with {{Network Activity}} and {{Neighboring Excitatory Synapses}}},
  volume = {11},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004632},
  doi = {10.1371/journal.pcbi.1004632},
  shorttitle = {Remodeling and {{Tenacity}} of {{Inhibitory Synapses}}},
  abstract = {Author Summary Synaptic plasticity is widely believed to constitute a fundamental mechanism for altering network function. An (implicit) extension of this belief is an assumption that spontaneous changes in synaptic function should not occur to any significant degree. Where excitatory synapses are concerned, recent studies have questioned the validity of this assumption. Where inhibitory synapses are concerned, however, much less is known. Here we followed the spontaneous remodeling dynamics of inhibitory synapses for days, and analyzed these dynamics within a statistical framework previously developed for glutamatergic synapses. Like their excitatory counterparts, sizes of individual synapses fluctuated considerably. Similarly, these spontaneous fluctuations were governed by a well-defined statistical process which assures that synaptic size distributions remain constant. Contrary to the aforementioned assumption, these spontaneous fluctuations drove changes in synaptic size configurations; interestingly, however, change rates were slower for inhibitory synapses. Unlike excitatory synapses, suppressing network activity barely affected inhibitory synapse remodeling dynamics, synaptic configuration change rates or synaptic size distributions. Our findings thus point to quantitative differences in spontaneous remodeling dynamics of inhibitory and excitatory synapses, but also indicate that the processes that control their sizes and govern their remodeling dynamics are fundamentally similar.},
  number = {11},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2017-06-12},
  date = {2015-11-24},
  pages = {e1004632},
  keywords = {neural networks,Neurons,Synapses,Neuronal dendrites,Statistical distributions,Fluorescence imaging,Fluorescence microscopy,Linear regression analysis},
  author = {Rubinski, Anna and Ziv, Noam E.},
  file = {/home/fh/lib/articles/Rubinski2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/ECDZS98V/article.html}
}

@book{Puterman2005,
  langid = {english},
  location = {{Hoboken, NJ}},
  title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  isbn = {978-0-471-72782-8},
  shorttitle = {Markov Decision Processes},
  pagetotal = {649},
  series = {Wiley series in probability and statistics},
  publisher = {{Wiley-Interscience}},
  date = {2005},
  keywords = {Markov processes,Diskreter Markov-Prozess,Dynamic programming,Statistical decision},
  author = {Puterman, Martin L.},
  file = {/home/fh/lib/books/Puterman2005_Markov-decision-processes-discrete-stochastic-dynamic-programming.djvu},
  note = {OCLC: 254152847}
}

@article{Chaudhuri2016,
  langid = {english},
  title = {Computational Principles of Memory},
  volume = {19},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v19/n3/abs/nn.4237.html},
  doi = {10.1038/nn.4237},
  abstract = {The ability to store and later use information is essential for a variety of adaptive behaviors, including integration, learning, generalization, prediction and inference. In this Review, we survey theoretical principles that can allow the brain to construct persistent states for memory. We identify requirements that a memory system must satisfy and analyze existing models and hypothesized biological substrates in light of these requirements. We also highlight open questions, theoretical puzzles and problems shared with computer science and information theory.
View full text},
  number = {3},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2017-06-15},
  date = {2016-03},
  pages = {394--403},
  keywords = {Dynamical systems,Learning algorithms,Long-term memory,Short-term memory},
  author = {Chaudhuri, Rishidev and Fiete, Ila},
  file = {/home/fh/lib/articles/Chaudhuri2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/MGHQH6U3/nn.4237.html}
}

@article{Xue2014,
  langid = {english},
  title = {Equalizing Excitation-Inhibition Ratios across Visual Cortical Neurons},
  volume = {511},
  issn = {0028-0836},
  url = {https://www.nature.com/nature/journal/v511/n7511/full/nature13321.html},
  doi = {10.1038/nature13321},
  abstract = {The relationship between synaptic excitation and inhibition (E/I ratio), two opposing forces in the mammalian cerebral cortex, affects many cortical functions such as feature selectivity and gain. Individual pyramidal cells show stable E/I ratios in time despite fluctuating cortical activity levels. This is because when excitation increases, inhibition increases proportionally through the increased recruitment of inhibitory neurons, a phenomenon referred to as excitation-inhibition balance. However, little is known about the distribution of E/I ratios across pyramidal cells. Through their highly divergent axons, inhibitory neurons indiscriminately contact most neighbouring pyramidal cells. Is inhibition homogeneously distributed or is it individually matched to the different amounts of excitation received by distinct pyramidal cells? Here we discover that pyramidal cells in layer 2/3 of mouse primary visual cortex each receive inhibition in a similar proportion to their excitation. As a consequence, E/I ratios are equalized across pyramidal cells. This matched inhibition is mediated by parvalbumin-expressing but not somatostatin-expressing inhibitory cells and results from the independent adjustment of synapses originating from individual parvalbumin-expressing cells targeting different pyramidal cells. Furthermore, this match is activity-dependent as it is disrupted by perturbing pyramidal cell activity. Thus, the equalization of E/I ratios across pyramidal cells reveals an unexpected degree of order in the spatial distribution of synaptic strengths and indicates that the relationship between the cortex/'s two opposing forces is stabilized not only in time but also in space.},
  number = {7511},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2017-06-12},
  date = {2014-07-31},
  pages = {596--600},
  keywords = {Striate cortex,Inhibition-excitation balance},
  author = {Xue, Mingshan and Atallah, Bassam V. and Scanziani, Massimo},
  file = {/home/fh/lib/articles/Xue2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/65CDI5DN/nature13321.html}
}

@article{Barral2016,
  langid = {english},
  title = {Synaptic Scaling Rule Preserves Excitatory-Inhibitory Balance and Salient Neuronal Network Dynamics},
  volume = {19},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v19/n12/full/nn.4415.html},
  doi = {10.1038/nn.4415},
  abstract = {The balance between excitation and inhibition (E\textendash{}I balance) is maintained across brain regions though the network size, strength and number of synaptic connections, and connection architecture may vary substantially. We use a culture preparation to examine the homeostatic synaptic scaling rules that produce E\textendash{}I balance and in vivo-like activity. We show that synaptic strength scales with the number of connections K as \textasciitilde{} , close to the ideal theoretical value. Using optogenetic techniques, we delivered spatiotemporally patterned stimuli to neurons and confirmed key theoretical predictions: E\textendash{}I balance is maintained, active decorrelation occurs and the spiking correlation increases with firing rate. Moreover, the trial-to-trial response variability decreased during stimulation, as observed in vivo. These results\textemdash{}obtained in generic cultures, predicted by theory and observed in the intact brain\textemdash{}suggest that the synaptic scaling rule and resultant dynamics are emergent properties of networks in general.},
  number = {12},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2017-06-12},
  date = {2016-12},
  pages = {1690--1696},
  keywords = {Network models,Dynamical systems,Neural circuits},
  author = {Barral, J{\'e}r{\'e}mie and Reyes, Alex D.},
  file = {/home/fh/lib/articles/Barral2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/3SPJ9CCQ/nn.4415.html}
}

@incollection{Bhalla2014,
  langid = {english},
  title = {Multiscale {{Modeling}} and {{Synaptic Plasticity}}},
  volume = {123},
  isbn = {978-0-12-397897-4},
  url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123978974000127},
  booktitle = {Progress in {{Molecular Biology}} and {{Translational Science}}},
  publisher = {{Elsevier}},
  urldate = {2017-06-20},
  date = {2014},
  pages = {351--386},
  author = {Bhalla, Upinder S.},
  file = {/home/fh/lib/book_sections/Bhalla2014.pdf},
  doi = {10.1016/B978-0-12-397897-4.00012-7}
}

@article{Lim2015,
  title = {Inferring Learning Rules from Distributions of Firing Rates in Cortical Neurons},
  volume = {18},
  issn = {1097-6256, 1546-1726},
  url = {http://www.nature.com/doifinder/10.1038/nn.4158},
  doi = {10.1038/nn.4158},
  number = {12},
  journaltitle = {Nature Neuroscience},
  urldate = {2017-06-20},
  date = {2015-11-02},
  pages = {1804--1810},
  author = {Lim, Sukbin and McKee, Jillian L and Woloszyn, Luke and Amit, Yali and Freedman, David J and Sheinberg, David L and Brunel, Nicolas},
  file = {/home/fh/lib/articles/Lim2015.pdf}
}

@collection{DeSchutter2010,
  location = {{Cambridge, Mass}},
  title = {Computational Modeling Methods for Neuroscientists},
  isbn = {978-0-262-01327-7},
  pagetotal = {419},
  series = {Computational neuroscience},
  publisher = {{MIT Press}},
  date = {2010},
  keywords = {Neurosciences,Neurobiology,Mathematical models,methods,Models; Neurological,computational neuroscience},
  editor = {De Schutter, Erik},
  file = {/home/fh/lib/books/De Schutter2010_Computational-modeling-methods-for-neuroscientists.pdf},
  note = {OCLC: ocn310075964}
}

@article{Burak2009,
  title = {Accurate {{Path Integration}} in {{Continuous Attractor Network Models}} of {{Grid Cells}}},
  volume = {5},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000291},
  doi = {10.1371/journal.pcbi.1000291},
  abstract = {Author Summary Even in the absence of external sensory cues, foraging rodents maintain an estimate of their position, allowing them to return home in a roughly straight line. This computation is known as dead reckoning or path integration. A discovery made three years ago in rats focused attention on the dorsolateral medial entorhinal cortex (dMEC) as a location in the rat's brain where this computation might be performed. In this area, so-called grid cells fire whenever the rat is on any vertex of a triangular grid that tiles the plane. Here we propose a model that could generate grid-cell-like responses in a neural network. The inputs to the model network convey information about the rat's velocity and heading, consistent with known inputs projecting into the dMEC. The network effectively integrates these inputs to produce a response that depends on the rat's absolute position. We show that such a neural network can integrate position accurately and can reproduce grid-cell-like responses similar to those observed experimentally. We then suggest a set of experiments that could help identify whether our suggested mechanism is responsible for the emergence of grid cells and for path integration in the rat's brain.},
  number = {2},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2017-06-22},
  date = {2009-02-20},
  pages = {e1000291},
  keywords = {N,P,a,c,e,f,g,i,k,l,m,o,r,s,t,u,v,w,y},
  author = {Burak, Yoram and Fiete, Ila R.},
  file = {/home/fh/lib/articles/Burak2009.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/QNWXZGQ6/article.html}
}

@article{Gallinaro2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.02912},
  primaryClass = {q-bio},
  title = {Associative Properties of Structural Plasticity Based on Firing Rate Homeostasis in a Balanced Recurrent Network of Spiking Neurons},
  url = {http://arxiv.org/abs/1706.02912},
  abstract = {Hebbian and homeostatic plasticity have been studied extensively in the past, both experimentally and theoretically, but many aspects of their interaction remain to be elucidated. Hebbian plasticity is thought to shape neuronal connectivity during development and learning, whereas homeostatic plasticity would stabilize network activity. Here we investigate another aspect of this interaction, which is whether Hebbian associative properties can also emerge as a network effect from a plasticity rule based on homeostatic principles on the neuronal level. The maturation of cortical networks during sensory experience is an ideal case to explore this question. Excitatory neurons in the visual cortex of rodents have been shown to connect preferentially to neurons that respond to similar visual features. Since this connectivity bias is not existent at the time of eye opening, but only after some weeks of visual experience, it has been suggested that plastic mechanisms are responsible for the changes taking place during sensory stimulation. We consider a structural plasticity rule driven by a homeostasis of firing rate in a recurrent network of leaky integrate-and-fire (LIF) neurons exposed to external input that is modulated by the orientation of a visual stimulus. Our results show that feature specific connectivity, similar to what has been experimentally observed in rodent visual cortex, can emerge out of a random balanced network of LIF neurons with a plasticity rule that is not explicitly dependent on correlations between pre- and postsynaptic neuronal activity. The synaptic association of neurons responding to similar stimulus features occurs as a side-effect of controlling the activity of individual neurons. The experience dependent structural changes that are triggered by simulation are long lasting and decay only slowly when the neurons are exposed again to non modulated external input.},
  urldate = {2017-06-23},
  date = {2017-06-09},
  keywords = {Quantitative Biology - Neurons and Cognition},
  author = {Gallinaro, J{\'u}lia V. and Rotter, Stefan},
  file = {/home/fh/lib/articles/Gallinaro2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/5KTQTZZG/1706.html}
}

@article{Bhalla2017a,
  langid = {english},
  title = {Synaptic Input Sequence Discrimination on Behavioral Timescales Mediated by Reaction-Diffusion Chemistry in Dendrites},
  volume = {6},
  issn = {2050-084X},
  url = {https://elifesciences.org/articles/25827},
  doi = {10.7554/eLife.25827},
  abstract = {The powerful computational operation of sequence recognition on behavioral timescales of approximately 1 s may emerge from synaptic activity-triggered build-up of biochemical waves in short 20 micron zones on dendrites.},
  journaltitle = {eLife},
  shortjournal = {eLife Sciences},
  urldate = {2017-06-27},
  date = {2017-04-19},
  pages = {e25827},
  author = {Bhalla, Upinder Singh},
  file = {/home/fh/lib/articles/Bhalla2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/RVNR74DU/25827.html}
}

@article{Loewenstein2011,
  title = {Multiplicative {{Dynamics Underlie}} the {{Emergence}} of the {{Log}}-{{Normal Distribution}} of {{Spine Sizes}} in the {{Neocortex In Vivo}}},
  volume = {31},
  number = {26},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  date = {2011-06-29},
  pages = {9481--9488},
  author = {Loewenstein, Yonatan and Kuras, Annerose and Rumpel, Simon},
  file = {/home/fh/lib/articles/Loewenstein2011.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/7M8PZBG9/9481.html}
}

@article{Sadacca2016,
  title = {The {{Behavioral Relevance}} of {{Cortical Neural Ensemble Responses Emerges Suddenly}}},
  volume = {36},
  number = {3},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  date = {2016-01-20},
  pages = {655--669},
  author = {Sadacca, Brian F. and Mukherjee, Narendra and Vladusich, Tony and Li, Jennifer X. and Katz, Donald B. and Miller, Paul},
  file = {/home/fh/lib/articles/Sadacca2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/9CKQFGKQ/655.html}
}

@article{Ito2015,
  title = {Distinct {{Neural Representation}} in the {{Dorsolateral}}, {{Dorsomedial}}, and {{Ventral Parts}} of the {{Striatum}} during {{Fixed}}- and {{Free}}-{{Choice Tasks}}},
  volume = {35},
  number = {8},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  date = {2015-02-25},
  pages = {3499--3514},
  author = {Ito, Makoto and Doya, Kenji},
  file = {/home/fh/lib/articles/Ito2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/CDXXH4BD/3499.html}
}

@incollection{Miner2017a,
  title = {Structural {{Plasticity}} and the {{Generation}} of {{Bidirectional Connectivity}}},
  isbn = {978-0-12-803784-3},
  booktitle = {The {{Rewiring Brain}}},
  publisher = {{Elsevier, Acad. Press}},
  date = {2017},
  pages = {247--260},
  author = {Miner, Daniel and Hoffmann, Felix Z. and Kleberg, Florence and Triesch, Jochen},
  file = {/home/fh/lib/book_sections/Miner2017.PDF}
}

@article{DelPapa2017,
  langid = {english},
  title = {Criticality Meets Learning: {{Criticality}} Signatures in a Self-Organizing Recurrent Neural Network},
  volume = {12},
  issn = {1932-6203},
  url = {http://dx.plos.org/10.1371/journal.pone.0178683},
  doi = {10.1371/journal.pone.0178683},
  shorttitle = {Criticality Meets Learning},
  number = {5},
  journaltitle = {PLOS ONE},
  urldate = {2017-07-16},
  date = {2017-05-26},
  pages = {e0178683},
  author = {Del Papa, Bruno and Priesemann, Viola and Triesch, Jochen},
  editor = {Chialvo, Dante R.},
  file = {/home/fh/lib/articles/Del Papa2017.pdf}
}

@article{Hassabis2017,
  langid = {english},
  title = {Neuroscience-{{Inspired Artificial Intelligence}}},
  volume = {95},
  issn = {0896-6273},
  url = {http://www.cell.com/neuron/abstract/S0896-6273(17)30509-3},
  doi = {10.1016/j.neuron.2017.06.011},
  number = {2},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2017-07-19},
  date = {2017-07-19},
  pages = {245--258},
  keywords = {learning,cognition,Brain,neural network,artificial intelligence},
  author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
  file = {/home/fh/lib/articles/Hassabis2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/F8D353H4/S0896-6273(17)30509-3.html}
}

@article{Topalidou2015,
  langid = {english},
  title = {A Long Journey into Reproducible Computational Neuroscience},
  volume = {9},
  issn = {1662-5188},
  url = {http://journal.frontiersin.org/article/10.3389/fncom.2015.00030/full},
  doi = {10.3389/fncom.2015.00030},
  abstract = {A long journey into reproducible computational neuroscience},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci.},
  urldate = {2017-07-25},
  date = {2015},
  keywords = {Computational models,Notebook,Version control,public repository,publication process,python language,reproducible science},
  author = {Topalidou, Meropi and Leblois, Arthur and Boraud, Thomas and Rougier, Nicolas P.},
  file = {/home/fh/lib/articles/Topalidou2015.pdf}
}

@article{Munafo2017,
  langid = {english},
  title = {A Manifesto for Reproducible Science},
  volume = {1},
  issn = {2397-3374},
  url = {https://www.nature.com/articles/s41562-016-0021},
  doi = {10.1038/s41562-016-0021},
  abstract = {Leading voices in the reproducibility landscape call for the adoption of measures to optimize key elements of the scientific process.$<$/p$>$},
  number = {1},
  journaltitle = {Nature Human Behaviour},
  urldate = {2017-07-26},
  date = {2017-01-10},
  pages = {s41562--016--0021--016},
  author = {Munaf{\`o}, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and du Sert, Nathalie Percie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  file = {/home/fh/lib/articles/Munafò2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/SXGIWWPN/s41562-016-0021.html}
}

@article{Wilson2014,
  title = {Best {{Practices}} for {{Scientific Computing}}},
  volume = {12},
  issn = {1545-7885},
  url = {http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001745},
  doi = {10.1371/journal.pbio.1001745},
  abstract = {We describe a set of best practices for scientific software development, based on research and experience, that will improve scientists' productivity and the reliability of their software.},
  number = {1},
  journaltitle = {PLOS Biology},
  shortjournal = {PLOS Biology},
  urldate = {2017-07-25},
  date = {2014-01-07},
  pages = {e1001745},
  keywords = {Computer software,Computers,Open source software,Programming languages,Research validity,Scientists,Software development,Software tools},
  author = {Wilson, Greg and Aruliah, D. A. and Brown, C. Titus and Hong, Neil P. Chue and Davis, Matt and Guy, Richard T. and Haddock, Steven H. D. and Huff, Kathryn D. and Mitchell, Ian M. and Plumbley, Mark D. and Waugh, Ben and White, Ethan P. and Wilson, Paul},
  file = {/home/fh/lib/articles/Wilson2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/AGUNBWRN/article.html}
}

@article{Ludaescher2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.09958},
  primaryClass = {cs},
  title = {Capturing the "{{Whole Tale}}" of {{Computational Research}}: {{Reproducibility}} in {{Computing Environments}}},
  url = {http://arxiv.org/abs/1610.09958},
  shorttitle = {Capturing the "{{Whole Tale}}" of {{Computational Research}}},
  abstract = {We present an overview of the recently funded "Merging Science and Cyberinfrastructure Pathways: The Whole Tale" project (NSF award \#1541450). Our approach has two nested goals: 1) deliver an environment that enables researchers to create a complete narrative of the research process including exposure of the data-to-publication lifecycle, and 2) systematically and persistently link research publications to their associated digital scholarly objects such as the data, code, and workflows. To enable this, Whole Tale will create an environment where researchers can collaborate on data, workspaces, and workflows and then publish them for future adoption or modification. Published data and applications will be consumed either directly by users using the Whole Tale environment or can be integrated into existing or future domain Science Gateways.},
  urldate = {2017-07-26},
  date = {2016-10-28},
  keywords = {Computer Science - Digital Libraries},
  author = {Ludaescher, Bertram and Chard, Kyle and Gaffney, Niall and Jones, Matthew B. and Nabrzyski, Jaroslaw and Stodden, Victoria and Turk, Matthew},
  file = {/home/fh/lib/articles/Ludaescher2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/XFJ52RKN/1610.html}
}

@article{OConnor2017,
  langid = {english},
  title = {The {{Dockstore}}: Enabling Modular, Community-Focused Sharing of {{Docker}}-Based Genomics Tools and Workflows},
  volume = {6},
  issn = {2046-1402},
  url = {https://f1000research.com/articles/6-52/v1},
  doi = {10.12688/f1000research.10137.1},
  shorttitle = {The {{Dockstore}}},
  journaltitle = {F1000Research},
  urldate = {2017-07-26},
  date = {2017-01-18},
  pages = {52},
  author = {O'Connor, Brian D. and Yuen, Denis and Chung, Vincent and Duncan, Andrew G. and Liu, Xiang Kun and Patricia, Janice and Paten, Benedict and Stein, Lincoln and Ferretti, Vincent},
  file = {/home/fh/lib/articles/O'Connor2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/2Q9U9H3N/v1.html}
}

@article{DiTommaso2017,
  langid = {english},
  title = {Nextflow Enables Reproducible Computational Workflows},
  volume = {35},
  issn = {1087-0156},
  url = {http://www.nature.com/nbt/journal/v35/n4/full/nbt.3820.html?foxtrotcallback=true},
  doi = {10.1038/nbt.3820},
  abstract = {To the Editor:
The increasing complexity of readouts for omics analyses goes hand-in-hand with concerns about the reproducibility of experiments that analyze 'big data'. When analyzing very large data sets, the main source of computational irreproducibility arises from a lack of good practice pertaining to software and database usage},
  number = {4},
  journaltitle = {Nature Biotechnology},
  shortjournal = {Nat Biotech},
  urldate = {2017-07-26},
  date = {2017-04},
  pages = {316--319},
  keywords = {Computational biology and bioinformatics,Data publication and archiving},
  author = {Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W. and Barja, Pablo Prieto and Palumbo, Emilio and Notredame, Cedric},
  file = {/home/fh/lib/articles/Di Tommaso2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/7D3NUPIT/nbt.3820.html}
}

@article{Rostami2017,
  title = {[{{Re}}] {{Spike Synchronization}} and {{Rate Modulation Differentially Involved}} in {{Motor Cortical Function}}},
  volume = {3},
  url = {https://zenodo.org/record/583814#.WXuGDCdLfCI},
  doi = {10.5281/zenodo.583814},
  abstract = {A reference implementation of Spike synchronization and rate modulation di erentially involved in motor cortical function. Alexa Riehle, Sonja Gr{\"u}n, Markus Diesmann, and Ad Aertsen (1997) Science 278:1950-1953. DOI:10.1126/science.278.5345.19 50},
  number = {1},
  journaltitle = {ReScience},
  urldate = {2017-07-28},
  date = {2017-05-29},
  keywords = {Spike time synchrony; Unitary Events method ; Python},
  author = {Rostami, Vahid and Ito, Junji and Denker, Michael and Gr{\"u}n, Sonja},
  file = {/home/fh/lib/articles/Rostami2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/C4UKRNNJ/583814.html}
}

@book{Stodden2014,
  langid = {english},
  title = {Implementing Reproducible Research},
  isbn = {978-1-4665-6160-1},
  url = {http://www.crcnetbase.com/isbn/9781466561601},
  urldate = {2017-07-28},
  date = {2014},
  author = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D and {CRC Press}},
  file = {/home/fh/lib/books/Stodden2014_Implementing-reproducible-research.pdf},
  note = {OCLC: 948565410}
}

@article{Sandve2013,
  langid = {english},
  title = {Ten {{Simple Rules}} for {{Reproducible Computational Research}}},
  volume = {9},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1003285},
  doi = {10.1371/journal.pcbi.1003285},
  number = {10},
  journaltitle = {PLoS Computational Biology},
  urldate = {2017-07-28},
  date = {2013-10-24},
  pages = {e1003285},
  author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
  editor = {Bourne, Philip E.},
  file = {/home/fh/lib/articles/Sandve2013.PDF}
}

@article{Elliott2009,
  title = {The {{Modulation Transfer Function}} for {{Speech Intelligibility}}},
  volume = {5},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000302},
  doi = {10.1371/journal.pcbi.1000302},
  abstract = {Author Summary The sound signal of speech is rich in temporal and frequency patterns. These fluctuations of power in time and frequency are called modulations. Despite their acoustic complexity, spoken words remain intelligible after drastic degradations in either time or frequency. To fully understand the perception of speech and to be able to reduce speech to its most essential components, we need to completely characterize how modulations in amplitude and frequency contribute together to the comprehensibility of speech. Hallmark research distorted speech in either time or frequency but described the arbitrary manipulations in terms limited to one domain or the other, without quantifying the remaining and missing portions of the signal. Here, we use a novel sound filtering technique to systematically investigate the joint features in time and frequency that are crucial for understanding speech. Both the modulation-filtering approach and the resulting characterization of speech have the potential to change the way that speech is compressed in audio engineering and how it is processed in medical applications such as cochlear implants.},
  number = {3},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2017-07-30},
  date = {2009-03-06},
  pages = {e1000302},
  keywords = {Signal filtering,Acoustic signals,Audio signal processing,Bandwidth (signal processing),Bioacoustics,Modulation,Speech,Speech signal processing},
  author = {Elliott, Taffeta M. and Theunissen, Fr{\'e}d{\'e}ric E.},
  file = {/home/fh/lib/articles/Elliott2009.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/43VS5X52/article.html}
}

@article{Kim2017,
  langid = {english},
  title = {Experimenting with Reproducibility in Bioinformatics},
  url = {http://www.biorxiv.org/content/early/2017/06/20/143503},
  doi = {10.1101/143503},
  abstract = {Reproducibility or replication has been shown to be limited in many scientific fields. This question is a fundamental tenet of the scientific activity, but the related issues of reusability of scientific data are poorly documented. Here, we present a case study of our attempt to reproduce a bioinformatics method and illustrate the challenges to use a published method for which code and data were available. From this example, we address the difficulties that pave the way towards reproducibility and propose some recommendations to the research community to improve the reusability of the data.},
  journaltitle = {bioRxiv},
  urldate = {2017-07-31},
  date = {2017-06-20},
  pages = {143503},
  author = {Kim, Yang-Min and Poline, Jean-Baptiste and Dumas, Guillaume},
  file = {/home/fh/lib/articles/Kim2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/6NE2QD5B/143503.html}
}

@article{Morrison2008,
  langid = {english},
  title = {Phenomenological Models of Synaptic Plasticity Based on Spike Timing},
  volume = {98},
  issn = {0340-1200, 1432-0770},
  url = {http://link.springer.com/10.1007/s00422-008-0233-1},
  doi = {10.1007/s00422-008-0233-1},
  number = {6},
  journaltitle = {Biological Cybernetics},
  urldate = {2017-08-06},
  date = {2008-06},
  pages = {459--478},
  author = {Morrison, Abigail and Diesmann, Markus and Gerstner, Wulfram},
  file = {/home/fh/lib/articles/Morrison2008.pdf}
}

@article{Jang2017,
  langid = {english},
  title = {Human {{Cortical Neurons}} in the {{Anterior Temporal Lobe Reinstate Spiking Activity}} during {{Verbal Memory Retrieval}}},
  volume = {27},
  issn = {09609822},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S096098221730550X},
  doi = {10.1016/j.cub.2017.05.014},
  number = {11},
  journaltitle = {Current Biology},
  urldate = {2017-08-05},
  date = {2017-06},
  pages = {1700--1705.e5},
  author = {Jang, Anthony I. and Wittig, John H. and Inati, Sara K. and Zaghloul, Kareem A.},
  file = {/home/fh/lib/articles/Jang2017.pdf}
}

@article{Titley2017,
  langid = {english},
  title = {Toward a {{Neurocentric View}} of {{Learning}}},
  volume = {95},
  issn = {08966273},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627317304592},
  doi = {10.1016/j.neuron.2017.05.021},
  number = {1},
  journaltitle = {Neuron},
  urldate = {2017-08-05},
  date = {2017-07},
  pages = {19--32},
  author = {Titley, Heather K. and Brunel, Nicolas and Hansel, Christian},
  file = {/home/fh/lib/articles/Titley2017.pdf}
}

@article{Kleberg2017,
  date = {2017},
  author = {Kleberg, Florence},
  file = {/home/fh/lib/articles/Kleberg2017.pdf}
}

@collection{Doya2007a,
  location = {{Cambridge, Mass}},
  title = {Bayesian Brain: Probabilistic Approaches to Neural Coding},
  isbn = {978-0-262-04238-3},
  shorttitle = {Bayesian Brain},
  pagetotal = {326},
  series = {Computational neuroscience},
  publisher = {{MIT Press}},
  date = {2007},
  keywords = {Neurons,Brain,Bayesian statistical decision theory},
  editor = {Doya, Kenji},
  file = {/home/fh/lib/books/Doya2007_Bayesian-brain-probabilistic-approaches-to-neural-coding.pdf}
}

@article{Mishra2016,
  title = {Symmetric Spike Timing-Dependent Plasticity at {{CA3}}\textendash{}{{CA3}} Synapses Optimizes Storage and Recall in Autoassociative Networks},
  volume = {7},
  issn = {2041-1723},
  url = {http://www.nature.com/doifinder/10.1038/ncomms11552},
  doi = {10.1038/ncomms11552},
  journaltitle = {Nature Communications},
  urldate = {2017-08-05},
  date = {2016-05-13},
  pages = {11552},
  author = {Mishra, Rajiv K. and Kim, Sooyun and Guzman, Segundo J. and Jonas, Peter},
  file = {/home/fh/lib/articles/Mishra2016.pdf}
}

@article{Cocchi2017,
  title = {Criticality in the Brain: {{A}} Synthesis of Neurobiology, Models and Cognition},
  issn = {0301-0082},
  url = {http://www.sciencedirect.com/science/article/pii/S0301008216301630},
  doi = {10.1016/j.pneurobio.2017.07.002},
  shorttitle = {Criticality in the Brain},
  abstract = {Cognitive function requires the coordination of neural activity across many scales, from neurons and circuits to large-scale networks. As such, it is unlikely that an explanatory framework focused upon any single scale will yield a comprehensive theory of brain activity and cognitive function. Modelling and analysis methods for neuroscience should aim to accommodate multiscale phenomena. Emerging research now suggests that multi-scale processes in the brain arise from so-called critical phenomena that occur very broadly in the natural world. Criticality arises in complex systems perched between order and disorder, and is marked by fluctuations that do not have any privileged spatial or temporal scale. We review the core nature of criticality, the evidence supporting its role in neural systems and its explanatory potential in brain health and disease.},
  journaltitle = {Progress in Neurobiology},
  shortjournal = {Progress in Neurobiology},
  urldate = {2017-08-06},
  date = {2017-07-19},
  keywords = {cognition,Bifurcations,dynamics,metastability,multistability,power-law},
  author = {Cocchi, Luca and Gollo, Leonardo L. and Zalesky, Andrew and Breakspear, Michael},
  file = {/home/fh/lib/articles/Cocchi2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/MBAP2MJ4/S0301008216301630.html}
}

@article{Wolf2014,
  langid = {english},
  title = {Dynamical Models of Cortical Circuits},
  volume = {25},
  issn = {09594388},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0959438814000324},
  doi = {10.1016/j.conb.2014.01.017},
  journaltitle = {Current Opinion in Neurobiology},
  urldate = {2017-08-05},
  date = {2014-04},
  pages = {228--236},
  author = {Wolf, Fred and Engelken, Rainer and Puelma-Touzel, Maximilian and Weidinger, Juan Daniel Fl{\'o}rez and Neef, Andreas},
  file = {/home/fh/lib/articles/Wolf2014.pdf}
}

@article{Steyvers2004,
  title = {Word Association Spaces for Predicting Semantic Similarity Effects in Episodic Memory},
  journaltitle = {Experimental cognitive psychology and its applications: Festschrift in honor of Lyle Bourne, Walter Kintsch, and Thomas Landauer},
  date = {2004},
  pages = {237--249},
  author = {Steyvers, Mark and Shiffrin, Richard M and Nelson, Douglas L},
  file = {/home/fh/lib/articles/Steyvers2004.pdf}
}

@article{Chen2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.02583},
  primaryClass = {cs},
  title = {A {{Gentle Tutorial}} of {{Recurrent Neural Network}} with {{Error Backpropagation}}},
  url = {http://arxiv.org/abs/1610.02583},
  abstract = {We describe recurrent neural networks (RNNs), which have attracted great attention on sequential tasks, such as handwriting recognition, speech recognition and image to text. However, compared to general feedforward neural networks, RNNs have feedback loops, which makes it a little hard to understand the backpropagation step. Thus, we focus on basics, especially the error backpropagation to compute gradients with respect to model parameters. Further, we go into detail on how error backpropagation algorithm is applied on long short-term memory (LSTM) by unfolding the memory unit.},
  urldate = {2017-08-05},
  date = {2016-10-08},
  keywords = {Computer Science - Learning},
  author = {Chen, Gang},
  file = {/home/fh/lib/articles/Chen2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/FGNNPQUT/1610.html}
}

@inproceedings{Karklin2011,
  title = {Efficient Coding of Natural Images with a Population of Noisy Linear-Nonlinear Neurons},
  booktitle = {Advances in Neural Information Processing Systems},
  date = {2011},
  pages = {999--1007},
  author = {Karklin, Yan and Simoncelli, Eero P},
  file = {/home/fh/lib/conferences/Karklin2011_Efficient-coding-of-natural-images-with-a-population-of-noisy-linear-nonlinear-neurons.pdf}
}

@article{Rougier2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.04393},
  primaryClass = {cs},
  title = {Sustainable Computational Science: The {{ReScience}} Initiative},
  url = {http://arxiv.org/abs/1707.04393},
  shorttitle = {Sustainable Computational Science},
  abstract = {Computer science offers a large set of tools for prototyping, writing, running, testing, validating, sharing and reproducing results, however computational science lags behind. In the best case, authors may provide their source code as a compressed archive and they may feel confident their research is reproducible. But this is not exactly true. James Buckheit and David Donoho proposed more than two decades ago that an article about computational results is advertising, not scholarship. The actual scholarship is the full software environment, code, and data that produced the result. This implies new workflows, in particular in peer-reviews. Existing journals have been slow to adapt: source codes are rarely requested, hardly ever actually executed to check that they produce the results advertised in the article. ReScience is a peer-reviewed journal that targets computational research and encourages the explicit replication of already published research, promoting new and open-source implementations in order to ensure that the original research can be replicated from its description. To achieve this goal, the whole publishing chain is radically different from other traditional scientific journals. ReScience resides on GitHub where each new implementation of a computational study is made available together with comments, explanations, and software tests.},
  urldate = {2017-08-06},
  date = {2017-07-14},
  keywords = {Computer Science - Digital Libraries},
  author = {Rougier, Nicolas P. and Hinsen, Konrad and Alexandre, Fr{\'e}d{\'e}ric and Arildsen, Thomas and Barba, Lorena and Benureau, Fabien C. Y. and Brown, C. Titus and de Buyl, Pierre and Caglayan, Ozan and Davison, Andrew P. and Delsuc, Marc Andr{\'e} and Detorakis, Georgios and Diem, Alexandra K. and Drix, Damien and Enel, Pierre and Girard, Beno{\^\i}t and Guest, Olivia and Hall, Matt G. and Henriques, Rafael Neto and Hinaut, Xavier and Jaron, Kamil S. and Khamassi, Mehdi and Klein, Almar and Manninen, Tiina and Marchesi, Pietro and McGlinn, Dan and Metzner, Christoph and Petchey, Owen L. and Plesser, Hans Ekkehard and Poisot, Timoth{\'e}e and Ram, Karthik and Ram, Yoav and Roesch, Etienne and Rossant, Cyrille and Rostami, Vahid and Shifman, Aaron and Stachelek, Joseph and Stimberg, Marcel and Stollmeier, Frank and Vaggi, Federico and Viejo, Guillaume and Vitay, Julien and Vostinar, Anya and Yurchak, Roman and Zito, Tiziano},
  options = {useprefix=true},
  file = {/home/fh/lib/articles/Rougier2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/HIGVJJ3K/1707.html}
}

@article{Vegue2017,
  langid = {english},
  title = {On {{The Structure Of Cortical Micro}}-{{Circuits Inferred From Small Sample Sizes}}},
  url = {http://www.biorxiv.org/content/early/2017/04/05/118471},
  doi = {10.1101/118471},
  abstract = {The connectivity of cortical micro-circuits exhibits features which are inconsistent with a simple random network. Here we show that several classes of network models can account for this non-random structure despite qualitative differences in their global properties. This apparent paradox is a consequence of the small numbers of simultaneously recorded neurons in experiment: when inferred via small sample sizes many networks may be indistinguishable, despite being globally distinct. We develop a connectivity measure which successfully classifies networks even when estimated locally, with a few neurons at a time. We show that data from rat cortex is consistent with a network in which the likelihood of a connection between neurons depends on spatial distance and on non-spatial, asymmetric clustering.$<$/p$>$},
  journaltitle = {bioRxiv},
  urldate = {2017-08-05},
  date = {2017-04-05},
  pages = {118471},
  author = {Vegue, Marina and Perin, Rodrigo and Roxin, Alex},
  file = {/home/fh/lib/articles/Vegue2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/E7ID2I87/118471.html}
}

@article{Poirazi2001,
  title = {Impact of {{Active Dendrites}} and {{Structural Plasticity}} on the {{Memory Capacity}} of {{Neural Tissue}}},
  volume = {29},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627301002525},
  doi = {10.1016/S0896-6273(01)00252-5},
  abstract = {We consider the combined effects of active dendrites and structural plasticity on the storage capacity of neural tissue. We compare capacity for two different modes of dendritic integration: (1) linear, where synaptic inputs are summed across the entire dendritic arbor, and (2) nonlinear, where each dendritic compartment functions as a separately thresholded neuron-like summing unit. We calculate much larger storage capacities for cells with nonlinear subunits and show that this capacity is accessible to a structural learning rule that combines random synapse formation with activity-dependent stabilization/elimination. In a departure from the common view that memories are encoded in the overall connection strengths between neurons, our results suggest that long-term information storage in neural tissue could reside primarily in the selective addressing of synaptic contacts onto dendritic subunits.},
  number = {3},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2017-08-05},
  date = {2001-03-01},
  pages = {779--796},
  author = {Poirazi, Panayiota and Mel, Bartlett W.},
  file = {/home/fh/lib/articles/Poirazi2001.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/A3KFKADW/S0896627301002525.html}
}

@article{Iacaruso2017,
  title = {Synaptic Organization of Visual Space in Primary Visual Cortex},
  volume = {547},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/nature23019},
  doi = {10.1038/nature23019},
  number = {7664},
  journaltitle = {Nature},
  urldate = {2017-08-06},
  date = {2017-07-12},
  pages = {449--452},
  author = {Iacaruso, M. Florencia and Gasler, Ioana T. and Hofer, Sonja B.},
  file = {/home/fh/lib/articles/Iacaruso2017.pdf}
}

@article{Gal2017,
  title = {Rich Cell-Type-Specific Network Topology in Neocortical Microcircuitry},
  volume = {20},
  issn = {1097-6256, 1546-1726},
  url = {http://www.nature.com/doifinder/10.1038/nn.4576},
  doi = {10.1038/nn.4576},
  number = {7},
  journaltitle = {Nature Neuroscience},
  urldate = {2017-08-07},
  date = {2017-06-05},
  pages = {1004--1013},
  author = {Gal, Eyal and London, Michael and Globerson, Amir and Ramaswamy, Srikanth and Reimann, Michael W and Muller, Eilif and Markram, Henry and Segev, Idan},
  file = {/home/fh/lib/articles/Gal2017.pdf}
}

@article{Tosi2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.00133},
  primaryClass = {q-bio},
  title = {Cortical {{Circuits}} from {{Scratch}}: {{A Metaplastic Architecture}} for the {{Emergence}} of {{Lognormal Firing Rates}} and {{Realistic Topology}}},
  url = {http://arxiv.org/abs/1706.00133},
  shorttitle = {Cortical {{Circuits}} from {{Scratch}}},
  abstract = {Our current understanding of neuroplasticity paints a picture of a complex interconnected system of dependent processes, which shape cortical structure so as to produce an efficient information processing system. Indeed, the cooperation of these processes is associated with robust, stable, adaptable networks with characteristic features of activity and synaptic topology. However, combining the actions of these mechanisms in models has proven exceptionally difficult and to date no model has been able to do so without significant hand-tuning. Until such a model exists that can successfully combine these mechanisms to form a stable circuits with realistic features, our ability to study neuroplasticity in the context of (more realistic) dynamic networks and potentially reap whatever rewards these features and mechanisms imbue biological networks with is hindered. We introduce a model which combines five known plasticity mechanisms that act on the network as well as a unique metaplastic mechanism which acts on other plasticity mechanisms, to produce a neural circuit model which is both stable and capable of broadly reproducing many characteristic features of cortical networks. The MANA (metaplastic artificial neural architecture) represents the first model of its kind in that it is able to self-organize realistic, nonrandom features of cortical networks, from a null initial state (no synaptic connectivity or neuronal differentiation) with no hand-tuning of relevant variables. In the same vein as models like the SORN (self-organizing recurrent network) MANA represents further progress toward the reverse engineering of the brain at the network level.},
  urldate = {2017-08-07},
  date = {2017-05-31},
  keywords = {Quantitative Biology - Neurons and Cognition},
  author = {Tosi, Zach and Beggs, John},
  file = {/home/fh/lib/articles/Tosi2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/4DKCD5UM/1706.html}
}

@article{Sweeney2017,
  langid = {english},
  title = {Emergent Spatial Synaptic Structure from Diffusive Plasticity},
  volume = {45},
  issn = {0953816X},
  url = {http://doi.wiley.com/10.1111/ejn.13279},
  doi = {10.1111/ejn.13279},
  number = {8},
  journaltitle = {European Journal of Neuroscience},
  urldate = {2017-08-08},
  date = {2017-04},
  pages = {1057--1067},
  author = {Sweeney, Yann and Clopath, Claudia},
  editor = {Poirazi, Panayiota},
  file = {/home/fh/lib/articles/Sweeney2017.pdf}
}

@book{Arnold1992,
  langid = {english},
  location = {{Berlin}},
  title = {Ordinary Differential Equations},
  edition = {3rd ed.},
  isbn = {978-3-540-54813-3 978-0-387-54813-5},
  pagetotal = {334},
  publisher = {{New York : Springer-Verlag}},
  date = {1992},
  keywords = {Differential equations},
  author = {Arnolʹd, V. I.},
  file = {/home/fh/lib/books/Arnolʹd1992_Ordinary-differential-equations.djvu}
}

@book{Hirsch1974,
  location = {{New York}},
  title = {Differential Equations, Dynamical Systems, and Linear Algebra},
  isbn = {978-0-12-349550-1},
  pagetotal = {358},
  number = {v. 60},
  series = {Pure and applied mathematics; a series of monographs and textbooks},
  publisher = {{Academic Press}},
  date = {1974},
  keywords = {Differential equations,Algebras; Linear},
  author = {Hirsch, Morris W. and Smale, Stephen},
  file = {/home/fh/lib/books/Hirsch1974_Differential-equations,-dynamical-systems,-and-linear-algebra.pdf}
}

@article{Marr1976,
  langid = {american},
  title = {From {{Understanding Computation}} to {{Understanding Neural Circuitry}}},
  url = {http://dspace.mit.edu/handle/1721.1/5782},
  abstract = {The CNS needs to be understood at four nearly independent levels of description: (1) that at which the nature of computation is expressed; (2) that at which the algorithms that implement a computation are characterized; (3) that at which an algorithm is committed to particular mechanisms; and (4) that at which the mechanisms are realized in hardware. In general, the nature of a computation is determined by the problem to be solved, the mechanisms that are used depend upon the available hardware, and the particular algorithms chosen depend on the problem and on the available mechanisms. Examples are given of theories at each level.},
  urldate = {2017-08-15},
  date = {1976-05-01},
  author = {Marr, D. and Poggio, T.},
  file = {/home/fh/lib/articles/Marr1976.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/T9K5393F/5782.html}
}

@article{Yim2014,
  langid = {english},
  title = {Impact of Correlated Inputs to Neurons: Modeling Observations from in Vivo Intracellular Recordings},
  volume = {37},
  issn = {0929-5313, 1573-6873},
  url = {http://link.springer.com/10.1007/s10827-014-0502-z},
  doi = {10.1007/s10827-014-0502-z},
  shorttitle = {Impact of Correlated Inputs to Neurons},
  number = {2},
  journaltitle = {Journal of Computational Neuroscience},
  urldate = {2017-08-30},
  date = {2014-10},
  pages = {293--304},
  author = {Yim, Man Yi and Kumar, Arvind and Aertsen, Ad and Rotter, Stefan},
  file = {/home/fh/lib/articles/Yim2014.pdf}
}

@book{Marr2010,
  location = {{Cambridge, Mass}},
  title = {Vision: A Computational Investigation into the Human Representation and Processing of Visual Information},
  isbn = {978-0-262-51462-0},
  shorttitle = {Vision},
  pagetotal = {403},
  publisher = {{MIT Press}},
  date = {2010},
  keywords = {Mathematical models,Data processing,Human information processing,Vision},
  author = {Marr, David},
  file = {/home/fh/lib/books/Marr2010_Vision-a-computational-investigation-into-the-human-representation-and-processing-of-visual-information.pdf},
  note = {OCLC: ocn472791457}
}

@book{Stein2003,
  location = {{Princeton}},
  title = {Fourier Analysis: An Introduction},
  isbn = {978-0-691-11384-5},
  shorttitle = {Fourier Analysis},
  pagetotal = {311},
  number = {1},
  series = {Princeton lectures in analysis},
  publisher = {{Princeton University Press}},
  date = {2003},
  keywords = {Fourier analysis},
  author = {Stein, Elias M. and Shakarchi, Rami},
  file = {/home/fh/lib/books/Stein2003_Fourier-analysis-an-introduction.pdf}
}

@article{Kuang2016,
  title = {Planning {{Movements}} in {{Visual}} and {{Physical Space}} in {{Monkey Posterior Parietal Cortex}}},
  volume = {26},
  issn = {1047-3211},
  url = {https://academic.oup.com/cercor/article/26/2/731/2367044/Planning-Movements-in-Visual-and-Physical-Space-in},
  doi = {10.1093/cercor/bhu312},
  abstract = {Neurons in the posterior parietal cortex respond selectively for spatial parameters of planned goal-directed movements. Yet, it is still unclear which aspects of the movement the neurons encode: the spatial parameters of the upcoming physical movement (physical goal), or the upcoming visual limb movement (visual goal). To test this, we recorded neuronal activity from the parietal reach region while monkeys planned reaches under either normal or prism-reversed viewing conditions. We found predominant encoding of physical goals while fewer neurons were selective for visual goals during planning. In contrast, local field potentials recorded in the same brain region exhibited predominant visual goal encoding, similar to previous imaging data from humans. The visual goal encoding in individual neurons was neither related to immediate visual input nor to visual memory, but to the future visual movement. Our finding suggests that action planning in parietal cortex is not exclusively a precursor of impending physical movements, as reflected by the predominant physical goal encoding, but also contains spatial kinematic parameters of upcoming visual movement, as reflected by co-existing visual goal encoding in neuronal spiking. The co-existence of visual and physical goals adds a complementary perspective to the current understanding of parietal spatial computations in primates.},
  number = {2},
  journaltitle = {Cerebral Cortex},
  shortjournal = {Cereb Cortex},
  urldate = {2017-09-04},
  date = {2016-02-01},
  pages = {731--747},
  author = {Kuang, Shenbing and Morel, Pierre and Gail, Alexander},
  file = {/home/fh/lib/articles/Kuang2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/YK8ICVI3/Planning-Movements-in-Visual-and-Physical-Space-in.html}
}

@article{Bertschinger2004,
  title = {Real-{{Time Computation}} at the {{Edge}} of {{Chaos}} in {{Recurrent Neural Networks}}},
  volume = {16},
  issn = {0899-7667},
  url = {http://dx.doi.org/10.1162/089976604323057443},
  doi = {10.1162/089976604323057443},
  number = {7},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2017-09-07},
  date = {2004-07-01},
  pages = {1413--1436},
  author = {Bertschinger, Nils and Natschl{\"a}ger, Thomas},
  file = {/home/fh/lib/articles/Bertschinger2004.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/R2M9DZHZ/089976604323057443.html}
}

@article{Wilting2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.07035},
  primaryClass = {physics, q-bio},
  title = {Branching into the {{Unknown}}: {{Inferring}} Collective Dynamical States from Subsampled Systems},
  url = {http://arxiv.org/abs/1608.07035},
  shorttitle = {Branching into the {{Unknown}}},
  abstract = {When studying the dynamics of complex systems, one can rarely sample the state of all components. We show that this spatial subsampling typically leads to severe underestimation of the risk of instability in systems with propagation of events. We analytically derived a subsampling-invariant estimator and applied it to non-linear network simulations and case reports of various diseases, recovering a close relation between vaccination rate and spreading behavior. The estimator can be particularly useful in countries with unreliable case reports, and promises early warning if e.g. antibiotic resistant bacteria increase their infectiousness. In neuroscience, subsampling has led to contradictory hypotheses about the collective spiking dynamics: asynchronous-irregular or critical. With the novel estimator, we demonstrated for rat, cat and monkey that collective dynamics lives in a narrow subspace between the two. Functionally, this subspace can combine the different computational properties associated with the two states.},
  urldate = {2017-09-07},
  date = {2016-08-25},
  keywords = {Quantitative Biology - Neurons and Cognition,Physics - Data Analysis; Statistics and Probability},
  author = {Wilting, Jens and Priesemann, Viola},
  file = {/home/fh/lib/articles/Wilting2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/M4LNQYKS/1608.html}
}

@article{Shoham2006,
  langid = {english},
  title = {How Silent Is the Brain: Is There a ``Dark Matter'' Problem in Neuroscience?},
  volume = {192},
  issn = {0340-7594, 1432-1351},
  url = {https://link.springer.com/article/10.1007/s00359-006-0117-6},
  doi = {10.1007/s00359-006-0117-6},
  shorttitle = {How Silent Is the Brain},
  abstract = {Evidence from a variety of recording methods suggests that many areas of the brain are far more sparsely active than commonly thought. Here, we review experimental findings pointing to the existence of neurons which fire action potentials rarely or only to very specific stimuli. Because such neurons would be difficult to detect with the most common method of monitoring neural activity in vivo\textemdash{}extracellular electrode recording\textemdash{}they could be referred to as ``dark neurons,'' in analogy to the astrophysical observation that much of the matter in the universe is undetectable, or dark. In addition to discussing the evidence for largely silent neurons, we review technical advances that will ultimately answer the question: how silent is the brain?},
  number = {8},
  journaltitle = {Journal of Comparative Physiology A},
  shortjournal = {J Comp Physiol A},
  urldate = {2017-09-08},
  date = {2006-08-01},
  pages = {777--784},
  author = {Shoham, Shy and O'Connor, Daniel H. and Segev, Ronen},
  file = {/home/fh/lib/articles/Shoham2006.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/36K47PWA/10.html}
}

@article{Tchumatchenko2011,
  title = {Representation of {{Dynamical Stimuli}} in {{Populations}} of {{Threshold Neurons}}},
  volume = {7},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002239},
  doi = {10.1371/journal.pcbi.1002239},
  abstract = {Author Summary Sensory stimuli in our environment are represented in the brain as input current changes to neurons. For example, a periodic bar pattern in the visual field leads to periodic current modulations in the visual cortex. Therefore, models describing the ability of neurons to represent incoming stimuli can offer important clues about how sensory stimuli are processed by the brain. As anyone who has used an old-fashioned radio can attest, there is not just one but multiple ways to encode a signal, e.g. the familiar AM and FM channels. But what are the potential encoding channels in the cortex? A signal could modify the neuronal input current in two distinct ways: it could act either on the mean or the variance of the current. Using a minimal model framework, which can reproduce many features of neuronal activity, we find that both encoding schemes could be equally potent in transmitting slow and fast signals. This allows us to describe how input signals of any functional form give rise to collective firing rate changes in populations of neurons.},
  number = {10},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2017-09-09},
  date = {2011-10-20},
  pages = {e1002239},
  keywords = {Neurons,Action potentials,Single neuron function,Signaling networks,White noise,Dynamic response,Frequency response,Nonlinear dynamics},
  author = {Tchumatchenko, Tatjana and Wolf, Fred},
  file = {/home/fh/lib/articles/Tchumatchenko2011.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/QR2SBQJX/article.html}
}

@article{Kreutz-Delgado2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1501.04032},
  primaryClass = {q-bio},
  title = {Mean {{Time}}-to-{{Fire}} for the {{Noisy LIF Neuron}} - {{A Detailed Derivation}} of the {{Siegert Formula}}},
  url = {http://arxiv.org/abs/1501.04032},
  abstract = {When stimulated by a very large number of Poisson-like presynaptic current input spikes, the temporal dynamics of the soma membrane potential \$V(t)\$ of a leaky integrate-and-fire (LIF) neuron is typically modeled in the diffusion limit and treated as a Ornstein-Uhlenbeck process (OUP). When the potential reaches a threshold value \$$\backslash$theta\$, \$V(t) = $\backslash$theta\$, the LIF neuron fires and the membrane potential is reset to a resting value, \$V\_0 $<$ $\backslash$theta\$, and clamped to this value for a specified (non-stochastic) absolute refractory period \$T\_r $\backslash$ge 0\$, after which the cycle is repeated. The time between firings is given by the random variable \$T\_f = T\_r+ T\$ where \$T\$ is the random time which elapses between the "unpinning" of the membrane potential clamp and the next, subsequent firing of the neuron. The mean time-to-fire, \$$\backslash$widehat\{T\}\_f = $\backslash$text\{E\}(T\_f) = T\_r + $\backslash$text\{E\}(T) = T\_r + $\backslash$widehat\{T\}\$, provides a measure \$$\backslash$rho\$ of the average firing rate of the neuron, $\backslash$[ $\backslash$rho = $\backslash$widehat\{T\}\_f\^\{-1\} = $\backslash$frac\{1\}\{T\_r + $\backslash$widehat\{T\}\} . $\backslash$] This note briefly discusses some aspects of the OUP model and derives the Siegert formula giving the firing rate, \$$\backslash$rho = $\backslash$rho(I\_0)\$ as a function of an injected current, \$I\_0\$. This is a well-known classical result and no claim to originality is made. The derivation of the firing rate given in this report, which closely follows the derivation outlined in the textbook by Gardiner, minimizes the required mathematical background and is done in some pedagogic detail to facilitate study by graduate students and others who are new to the subject. Knowledge of the material presented in the first five chapters of Gardiner should provide an adequate background for following the derivation given in this note.},
  urldate = {2017-09-09},
  date = {2015-01-16},
  keywords = {Quantitative Biology - Neurons and Cognition},
  author = {Kreutz-Delgado, Ken},
  file = {/home/fh/lib/articles/Kreutz-Delgado2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/7LKHS7LM/1501.html}
}

@article{Richardson2007,
  title = {Firing-Rate Response of Linear and Nonlinear Integrate-and-Fire Neurons to Modulated Current-Based and Conductance-Based Synaptic Drive},
  volume = {76},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.76.021919},
  doi = {10.1103/PhysRevE.76.021919},
  abstract = {Integrate-and-fire models are mainstays of the study of single-neuron response properties and emergent states of recurrent networks of spiking neurons. They also provide an analytical base for perturbative approaches that treat important biological details, such as synaptic filtering, synaptic conductance increase, and voltage-activated currents. Steady-state firing rates of both linear and nonlinear integrate-and-fire models, receiving fluctuating synaptic drive, can be calculated from the time-independent Fokker-Planck equation. The dynamic firing-rate response is less easy to extract, even at the first-order level of a weak modulation of the model parameters, but is an important determinant of neuronal response and network stability. For the linear integrate-and-fire model the response to modulations of current-based synaptic drive can be written in terms of hypergeometric functions. For the nonlinear exponential and quadratic models no such analytical forms for the response are available. Here it is demonstrated that a rather simple numerical method can be used to obtain the steady-state and dynamic response for both linear and nonlinear models to parameter modulation in the presence of current-based or conductance-based synaptic fluctuations. To complement the full numerical solution, generalized analytical forms for the high-frequency response are provided. A special case is also identified\textemdash{}time-constant modulation\textemdash{}for which the response to an arbitrarily strong modulation can be calculated exactly.},
  number = {2},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  urldate = {2017-09-09},
  date = {2007-08-20},
  pages = {021919},
  author = {Richardson, Magnus J. E.},
  file = {/home/fh/lib/articles/Richardson2007.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/7FP6ISAU/PhysRevE.76.html}
}

@article{Goedeke2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.01880},
  primaryClass = {nlin, q-bio},
  title = {Noise Dynamically Suppresses Chaos in Random Neural Networks},
  url = {http://arxiv.org/abs/1603.01880},
  abstract = {Noise is ubiquitous in neural systems due to intrinsic stochasticity or external drive. For deterministic dynamics, randomly coupled neural networks display a transition to chaos at a critical coupling strength. Here, we investigate the effect of additive white noise on the onset of chaos. We develop the dynamical mean-field theory yielding the statistics of the activity and the maximum Lyapunov exponent. An exact condition determines the transition from stable to chaotic dynamics. Noise suppresses chaos by a dynamic mechanism, shifting the transition to significantly larger coupling strengths than predicted by local stability analysis. A regime emerges, where expansive dynamics and stable long-term behavior coexist. Furthermore, the time scale of the temporal correlations does not diverge at the transition, but peaks slightly above the critical coupling strength.},
  urldate = {2017-09-10},
  date = {2016-03-06},
  keywords = {Quantitative Biology - Neurons and Cognition,Nonlinear Sciences - Chaotic Dynamics},
  author = {Goedeke, Sven and Schuecker, Jannis and Helias, Moritz},
  file = {/home/fh/lib/articles/Goedeke2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/UBDLTH2N/1603.html}
}

@article{Sompolinsky1988,
  title = {Chaos in {{Random Neural Networks}}},
  volume = {61},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.61.259},
  doi = {10.1103/PhysRevLett.61.259},
  abstract = {A continuous-time dynamic model of a network of N nonlinear elements interacting via random asymmetric couplings is studied. A self-consistent mean-field theory, exact in the N$\rightarrow\infty$ limit, predicts a transition from a stationary phase to a chaotic phase occurring at a critical value of the gain parameter. The autocorrelations of the chaotic flow as well as the maximal Lyapunov exponent are calculated.},
  number = {3},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  urldate = {2017-09-10},
  date = {1988-07-18},
  pages = {259--262},
  author = {Sompolinsky, H. and Crisanti, A. and Sommers, H. J.},
  file = {/home/fh/lib/articles/Sompolinsky1988.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/RR2GQ5I6/PhysRevLett.61.html}
}

@book{Papoulis1991,
  location = {{New York}},
  title = {Probability, Random Variables, and Stochastic Processes},
  edition = {3rd ed},
  isbn = {978-0-07-048477-1},
  pagetotal = {666},
  series = {McGraw-Hill series in electrical engineering},
  publisher = {{McGraw-Hill}},
  date = {1991},
  keywords = {Stochastic processes,Random variables,Probabilities},
  author = {Papoulis, Athanasios},
  file = {/home/fh/lib/books/Papoulis1991_Probability,-random-variables,-and-stochastic-processes.djvu;/home/fh/lib/books/Papoulis1991_solutions_manual.djvu}
}

@article{Nigam2016,
  langid = {english},
  title = {Rich-{{Club Organization}} in {{Effective Connectivity}} among {{Cortical Neurons}}},
  volume = {36},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/36/3/670},
  doi = {10.1523/JNEUROSCI.2177-15.2016},
  abstract = {The performance of complex networks, like the brain, depends on how effectively their elements communicate. Despite the importance of communication, it is virtually unknown how information is transferred in local cortical networks, consisting of hundreds of closely spaced neurons. To address this, it is important to record simultaneously from hundreds of neurons at a spacing that matches typical axonal connection distances, and at a temporal resolution that matches synaptic delays. We used a 512-electrode array (60 $\mu$m spacing) to record spontaneous activity at 20 kHz from up to 500 neurons simultaneously in slice cultures of mouse somatosensory cortex for 1 h at a time. We applied a previously validated version of transfer entropy to quantify information transfer. Similar to in vivo reports, we found an approximately lognormal distribution of firing rates. Pairwise information transfer strengths also were nearly lognormally distributed, similar to reports of synaptic strengths. Some neurons transferred and received much more information than others, which is consistent with previous predictions. Neurons with the highest outgoing and incoming information transfer were more strongly connected to each other than chance, thus forming a ``rich club.'' We found similar results in networks recorded in vivo from rodent cortex, suggesting the generality of these findings. A rich-club structure has been found previously in large-scale human brain networks and is thought to facilitate communication between cortical regions. The discovery of a small, but information-rich, subset of neurons within cortical regions suggests that this population will play a vital role in communication, learning, and memory.
SIGNIFICANCE STATEMENT Many studies have focused on communication networks between cortical brain regions. In contrast, very few studies have examined communication networks within a cortical region. This is the first study to combine such a large number of neurons (several hundred at a time) with such high temporal resolution (so we can know the direction of communication between neurons) for mapping networks within cortex. We found that information was not transferred equally through all neurons. Instead, $\sim$70\% of the information passed through only 20\% of the neurons. Network models suggest that this highly concentrated pattern of information transfer would be both efficient and robust to damage. Therefore, this work may help in understanding how the cortex processes information and responds to neurodegenerative diseases.},
  number = {3},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2017-09-13},
  date = {2016-01-20},
  pages = {670--684},
  keywords = {effective connectivity,information transfer,microcircuits,rich club,transfer entropy},
  author = {Nigam, Sunny and Shimono, Masanori and Ito, Shinya and Yeh, Fang-Chin and Timme, Nicholas and Myroshnychenko, Maxym and Lapish, Christopher C. and Tosi, Zachary and Hottowy, Pawel and Smith, Wesley C. and Masmanidis, Sotiris C. and Litke, Alan M. and Sporns, Olaf and Beggs, John M.},
  file = {/home/fh/lib/articles/Nigam2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/LPCIPJFN/670.html},
  eprinttype = {pmid},
  eprint = {26791200}
}

@article{Rajan2006,
  langid = {english},
  title = {Eigenvalue {{Spectra}} of {{Random Matrices}} for {{Neural Networks}}},
  volume = {97},
  issn = {0031-9007, 1079-7114},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.97.188104},
  doi = {10.1103/PhysRevLett.97.188104},
  number = {18},
  journaltitle = {Physical Review Letters},
  urldate = {2017-09-20},
  date = {2006-11-02},
  author = {Rajan, Kanaka and Abbott, L. F.},
  file = {/home/fh/lib/articles/Rajan2006.pdf}
}

@article{Ross-Hellauer2017,
  langid = {english},
  title = {What Is Open Peer Review? {{A}} Systematic Review},
  volume = {6},
  issn = {2046-1402},
  url = {https://f1000research.com/articles/6-588/v1},
  doi = {10.12688/f1000research.11369.1},
  shorttitle = {What Is Open Peer Review?},
  journaltitle = {F1000Research},
  urldate = {2017-09-21},
  date = {2017-04-27},
  pages = {588},
  author = {Ross-Hellauer, Tony},
  file = {/home/fh/lib/articles/Ross-Hellauer2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/CQJ22AN4/v1.html}
}

@article{Benson2007,
  langid = {english},
  title = {Recurrence of Extreme Events with Power-Law Interarrival Times},
  volume = {34},
  issn = {1944-8007},
  url = {http://onlinelibrary.wiley.com/doi/10.1029/2007GL030767/abstract},
  doi = {10.1029/2007GL030767},
  abstract = {In various geophysical applications, power-law interarrival times are observed between extreme events. Classical extreme value theory is based on exponentially distributed interarrivals and can not be applied to these processes. We solve for the density of the maxima of a sequence of random extreme events with any distribution of random interarrivals by applying a continuous time random max model, similar to a random walk model. The equation is exact when the distributions of the exceedances and the interarrivals are known. If only the tail properties of the exceedances and interarrivals can be estimated, then limiting extreme value distributions governing the maximum observation or exceedance are used. The general extreme value densities are obtained by transforming the classical extreme value distributions via subordination. This new class of extreme value densities can be used to obtain recurrence intervals for extreme events with power-law interarrivals.},
  number = {16},
  journaltitle = {Geophysical Research Letters},
  shortjournal = {Geophys. Res. Lett.},
  urldate = {2017-09-22},
  date = {2007-08-28},
  pages = {L16404},
  keywords = {1817 Extreme events,4468 Probability distributions; heavy and fat-tailed,7223 Earthquake interaction; forecasting; and prediction,extreme values,interarrivals,power law},
  author = {Benson, David A. and Schumer, Rina and Meerschaert, Mark M.},
  file = {/home/fh/lib/articles/Benson2007.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/NG46GDWT/abstract.html}
}

@article{Sussillo2009,
  title = {Generating {{Coherent Patterns}} of {{Activity}} from {{Chaotic Neural Networks}}},
  volume = {63},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627309005479},
  doi = {10.1016/j.neuron.2009.07.018},
  abstract = {Summary
Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated.},
  number = {4},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2017-09-22},
  date = {2009-08-27},
  pages = {544--557},
  keywords = {SYSNEURO},
  author = {Sussillo, David and Abbott, L. F.},
  file = {/home/fh/lib/articles/Sussillo2009.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/57CXVZVJ/S0896627309005479.html}
}

@article{Adesnik2017,
  langid = {english},
  title = {Synaptic {{Mechanisms}} of {{Feature Coding}} in the {{Visual Cortex}} of {{Awake Mice}}},
  volume = {95},
  issn = {08966273},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627317307055},
  doi = {10.1016/j.neuron.2017.08.014},
  number = {5},
  journaltitle = {Neuron},
  urldate = {2017-09-25},
  date = {2017-08},
  pages = {1147--1159.e4},
  author = {Adesnik, Hillel},
  file = {/home/fh/lib/articles/Adesnik2017.pdf}
}

@article{Rubin2015,
  title = {The {{Stabilized Supralinear Network}}: {{A Unifying Circuit Motif Underlying Multi}}-{{Input Integration}} in {{Sensory Cortex}}},
  volume = {85},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627314011350},
  doi = {10.1016/j.neuron.2014.12.026},
  shorttitle = {The {{Stabilized Supralinear Network}}},
  abstract = {Summary
Neurons in sensory cortex integrate multiple influences to parse objects and support perception. Across multiple cortical areas, integration is characterized by two neuronal response properties: (1) surround suppression\textemdash{}modulatory contextual stimuli suppress responses to driving stimuli; and (2) ``normalization''\textemdash{}responses to multiple driving stimuli add sublinearly. These depend on input strength: for weak driving stimuli, contextual influences facilitate or more weakly suppress and summation becomes linear or supralinear. Understanding the circuit operations underlying integration is critical to understanding cortical function and disease. We present a simple, general theory. A wealth of integrative properties, including the above, emerge robustly from four cortical circuit properties: (1) supralinear neuronal input/output functions; (2) sufficiently strong recurrent excitation; (3) feedback inhibition; and (4) simple spatial properties of intracortical connections. Integrative properties emerge dynamically as circuit properties, with excitatory and inhibitory neurons showing similar behaviors. In new recordings in visual cortex, we confirm key model predictions.},
  number = {2},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2017-09-25},
  date = {2015-01-21},
  pages = {402--417},
  author = {Rubin, Daniel B. and Van Hooser, Stephen D. and Miller, Kenneth D.},
  file = {/home/fh/lib/articles/Rubin2015.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/BHNYIKCQ/S0896627314011350.html}
}

@book{InternationalConferenceonTheoryandApplicationinNonlinearDynamics2014,
  langid = {english},
  title = {International {{Conference}} on {{Theory}} and {{Application}} in {{Nonlinear Dynamics}} ({{ICAND}} 2012)},
  isbn = {978-3-319-02925-2 978-3-319-02924-5},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=3093391},
  abstract = {A collection of different lectures presented by experts in the field of nonlinear science provides the reader with contemporary, cutting-edge, research works that bridge the gap between theory and device realizations of nonlinear phenomena. Representative examples of topics covered include: chaos gates, social networks, communication, sensors, lasers, molecular motors, biomedical anomalies, stochastic resonance, nano-oscillators for generating microwave signals and related complex systems. A common theme among these and many other related lectures is to model, study, understand, and exploit the rich behavior exhibited by nonlinear systems to design and fabricate novel technologies with superior characteristics. Consider, for instance, the fact that a sharks sensitivity to electric fields is 400 times more powerful than the most sophisticated electric-field sensor. In spite of significant advances in material properties, in many cases it remains a daunting task to duplicate the superior signal processing capabilities of most animals. Since nonlinear systems tend to be highly sensitive to perturbations when they occur near the onset of a bifurcation, there are also lectures on the general topic of bifurcation theory and on how to exploit such bifurcations for signal enhancements purposes. This manuscript will appeal to researchers interested in both theory and implementations of nonlinear systems.},
  urldate = {2017-09-26},
  date = {2014},
  author = {{International Conference on Theory and Application in Nonlinear Dynamics} and In, Visarath and Palacios, Antonio and Longhini, Patrick},
  file = {/home/fh/lib/books/In2014_International-Conference-on-Theory-and-Application-in-Nonlinear-Dynamics-(ICAND-2012).pdf},
  note = {OCLC: 873948185}
}

@article{Kusmierz2017,
  title = {Learning with Three Factors: Modulating {{Hebbian}} Plasticity with Errors},
  volume = {46},
  issn = {0959-4388},
  url = {http://www.sciencedirect.com/science/article/pii/S0959438817300612},
  doi = {10.1016/j.conb.2017.08.020},
  shorttitle = {Learning with Three Factors},
  abstract = {Synaptic plasticity is a central theme in neuroscience. A framework of three-factor learning rules provides a powerful abstraction, helping to navigate through the abundance of models of synaptic plasticity. It is well-known that the dopamine modulation of learning is related to reward, but theoretical models predict other functional roles of the modulatory third factor; it may encode errors for supervised learning, summary statistics of the population activity for unsupervised learning or attentional feedback. Specialized structures may be needed in order to generate and propagate third factors in the neural network.},
  issue = {Supplement C},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  urldate = {2017-09-27},
  date = {2017-10-01},
  pages = {170--177},
  author = {Ku{\'s}mierz, {\L}ukasz and Isomura, Takuya and Toyoizumi, Taro},
  file = {/home/fh/lib/articles/Kuśmierz2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/NWWVGA69/S0959438817300612.html}
}

@article{Costa2017,
  langid = {english},
  title = {Synaptic {{Transmission Optimization Predicts Expression Loci}} of {{Long}}-{{Term Plasticity}}},
  volume = {96},
  issn = {08966273},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627317308619},
  doi = {10.1016/j.neuron.2017.09.021},
  number = {1},
  journaltitle = {Neuron},
  urldate = {2017-09-29},
  date = {2017-09},
  pages = {177--189.e7},
  author = {Costa, Rui Ponte and Padamsey, Zahid and D'Amour, James A. and Emptage, Nigel J. and Froemke, Robert C. and Vogels, Tim P.},
  file = {/home/fh/lib/articles/Costa3.pdf}
}

@article{Stachenfeld2017,
  langid = {english},
  title = {The Hippocampus as a Predictive Map},
  volume = {advance online publication},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/vaop/ncurrent/full/nn.4650.html?foxtrotcallback=true},
  doi = {10.1038/nn.4650},
  abstract = {A cognitive map has long been the dominant metaphor for hippocampal function, embracing the idea that place cells encode a geometric representation of space. However, evidence for predictive coding, reward sensitivity and policy dependence in place cells suggests that the representation is not purely spatial. We approach this puzzle from a reinforcement learning perspective: what kind of spatial representation is most useful for maximizing future reward? We show that the answer takes the form of a predictive representation. This representation captures many aspects of place cell responses that fall outside the traditional view of a cognitive map. Furthermore, we argue that entorhinal grid cells encode a low-dimensionality basis set for the predictive representation, useful for suppressing noise in predictions and extracting multiscale structure for hierarchical planning.},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2017-10-05},
  date = {2017-10-02},
  keywords = {Hippocampus,Reward,Learning and memory},
  author = {Stachenfeld, Kimberly L. and Botvinick, Matthew M. and Gershman, Samuel J.},
  file = {/home/fh/lib/articles/Stachenfeld2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/EY4XXHIE/nn.4650.html}
}

@misc{zotero-2305,
  title = {Brian2 2.0.2.1},
  file = {/home/fh/lib/manuals/brian2/brian2_2.0.2.1/index.html}
}

@article{Triesch2017,
  langid = {english},
  title = {Competition for a Limited Supply of Synaptic Building Blocks Predicts Multiplicative Synaptic Normalization and Heterosynaptic Plasticity},
  url = {https://www.biorxiv.org/content/early/2017/07/21/166819},
  doi = {10.1101/166819},
  abstract = {We present a mathematical model of synaptic normalization and heterosynaptic plasticity based on competition for limited synaptic resources. In the model, afferent synapses on a part of the dendritic tree of a neuron compete for a limited supply of synaptic building blocks such as AMPA receptors or other postsynaptic components, which are distributed across the dendritic tree. These building blocks form a pool of parts that are ready for incorporation into synapses. Using minimal assumptions, the model produces fast multiplicative normalization behavior and leads to a homeostatic form of heterosynaptic plasticity. It therefore supports the use of such rules in neural network models. Furthermore, the model predicts that the amount of heterosynaptic plasticity is small when many building blocks are available in the pool. The model also suggests that local production and/or assembly of postsynaptic building blocks across the dendritic tree may be necessary to maintain a neuron's proper function, because it facilitates their homogeneous distribution across the dendritic tree. Because of its simplicity and analytical tractability, the model provides a convenient starting point for the development of more detailed models of the molecular mechanisms underlying different forms of synaptic plasticity.},
  journaltitle = {bioRxiv},
  urldate = {2017-10-05},
  date = {2017-07-21},
  pages = {166819},
  author = {Triesch, Jochen and Hafner, Anne-Sophie},
  file = {/home/fh/lib/articles/Triesch2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/MR7KPA3E/166819.html}
}

@article{Pereira2017,
  langid = {english},
  title = {Attractor Dynamics in Networks with Learning Rules Inferred from in Vivo Data},
  url = {https://www.biorxiv.org/content/early/2017/10/06/199521},
  doi = {10.1101/199521},
  abstract = {The attractor neural network scenario is a popular scenario for memory storage in association cortex, but there is still a large gap between models based on this scenario and experimental data. We study a recurrent network model in which both learning rules and distribution of stored patterns are inferred from distributions of visual responses for novel and familiar images in inferior temporal cortex (ITC). Unlike classical attractor neural network models, our model exhibits graded activity in retrieval states, with distributions of firing rates that are close to lognormal. Inferred learning rules are close to maximizing the number of stored patterns within a family of unsupervised Hebbian learning rules, suggesting learning rules in ITC are optimized to store a large number of attractor states. Finally, we show that there exists two types of retrieval states: one in which firing rates are constant in time, another in which firing rates fluctuate chaotically.},
  journaltitle = {bioRxiv},
  urldate = {2017-10-07},
  date = {2017-10-06},
  pages = {199521},
  author = {Pereira, Ulises and Brunel, Nicolas},
  file = {/home/fh/lib/articles/Pereira2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/GF8KZE6D/199521.html}
}

@article{Cohen2013,
  title = {Metabolic {{Turnover}} of {{Synaptic Proteins}}: {{Kinetics}}, {{Interdependencies}} and {{Implications}} for {{Synaptic Maintenance}}},
  volume = {8},
  issn = {1932-6203},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0063191},
  doi = {10.1371/journal.pone.0063191},
  shorttitle = {Metabolic {{Turnover}} of {{Synaptic Proteins}}},
  abstract = {Chemical synapses contain multitudes of proteins, which in common with all proteins, have finite lifetimes and therefore need to be continuously replaced. Given the huge numbers of synaptic connections typical neurons form, the demand to maintain the protein contents of these connections might be expected to place considerable metabolic demands on each neuron. Moreover, synaptic proteostasis might differ according to distance from global protein synthesis sites, the availability of distributed protein synthesis facilities, trafficking rates and synaptic protein dynamics. To date, the turnover kinetics of synaptic proteins have not been studied or analyzed systematically, and thus metabolic demands or the aforementioned relationships remain largely unknown. In the current study we used dynamic Stable Isotope Labeling with Amino acids in Cell culture (SILAC), mass spectrometry (MS), Fluorescent Non\textendash{}Canonical Amino acid Tagging (FUNCAT), quantitative immunohistochemistry and bioinformatics to systematically measure the metabolic half-lives of hundreds of synaptic proteins, examine how these depend on their pre/postsynaptic affiliation or their association with particular molecular complexes, and assess the metabolic load of synaptic proteostasis. We found that nearly all synaptic proteins identified here exhibited half-lifetimes in the range of 2\textendash{}5 days. Unexpectedly, metabolic turnover rates were not significantly different for presynaptic and postsynaptic proteins, or for proteins for which mRNAs are consistently found in dendrites. Some functionally or structurally related proteins exhibited very similar turnover rates, indicating that their biogenesis and degradation might be coupled, a possibility further supported by bioinformatics-based analyses. The relatively low turnover rates measured here ($\sim$0.7\% of synaptic protein content per hour) are in good agreement with imaging-based studies of synaptic protein trafficking, yet indicate that the metabolic load synaptic protein turnover places on individual neurons is very substantial.},
  number = {5},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2017-10-10},
  date = {2013-05-02},
  pages = {e63191},
  keywords = {Neurons,Synapses,Neuronal dendrites,Lysine,Protein metabolism,Protein synthesis,Stable isotope labeling by amino acids in cell culture,Synaptic vesicles},
  author = {Cohen, Laurie D. and Zuchman, Rina and Sorokina, Oksana and M{\"u}ller, Anke and Dieterich, Daniela C. and Armstrong, J. Douglas and Ziv, Tamar and Ziv, Noam E.},
  file = {/home/fh/lib/articles/Cohen2013.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/VPVGJ6HV/article.html}
}

@book{1992,
  langid = {english},
  location = {{Oxford}},
  title = {The {{Oxford}}-{{Duden}} Pictorial {{English}} Dictionary},
  edition = {Repr},
  isbn = {978-0-19-864155-1},
  pagetotal = {820},
  publisher = {{Univ. Press}},
  date = {1992},
  file = {/home/fh/lib/books/1992_The-Oxford-Duden-pictorial-English-dictionary.pdf},
  note = {OCLC: 258013646}
}

@article{Gruning2017,
  langid = {english},
  title = {Practical Computational Reproducibility in the Life Sciences},
  url = {https://www.biorxiv.org/content/early/2017/10/11/200683},
  doi = {10.1101/200683},
  abstract = {Many areas of research suffer from poor reproducibility. This problem is particularly acute in computationally intensive domains where results rely on a series of complex methodological decisions that are not well captured by traditional publication approaches. Various guidelines have emerged for achieving reproducibility, but practical implementation of these practices remains difficult. This is because reproducing published computational analyses requires installing many software tools plus associated libraries, connecting tools together into the complete pipeline, and specifying parameters. Here we present a suite of recently emerged technologies which make computational reproducibility not just possible, but, finally, practical in both time and effort. By combining a system for building highly portable packages of bioinformatics software, containerization and virtualization technologies for isolating reusable execution environments for these packages, and an integrated workflow system that automatically orchestrates the composition of these packages for entire pipelines, an unprecedented level of computational reproducibility can be achieved.$<$/p$>$},
  journaltitle = {bioRxiv},
  urldate = {2017-10-19},
  date = {2017-10-11},
  pages = {200683},
  author = {Gr{\"u}ning, Bj{\"o}rn and Chilton, John and K{\"o}ster, Johannes and Dale, Ryan and Goecks, Jeremy and Backofen, Rolf and Nekrutenko, Anton and Taylor, James},
  file = {/home/fh/lib/articles/Grüning2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/UX3QSBAA/200683.html}
}

@book{Frohlich2016,
  location = {{Amsterdam ; Boston}},
  title = {Network Neuroscience},
  isbn = {978-0-12-801560-5},
  pagetotal = {464},
  publisher = {{Academic Press}},
  date = {2016},
  keywords = {Nerve Net,Neural networks (Neurobiology),Neural circuitry},
  author = {Fr{\"o}hlich, Flavio},
  file = {/home/fh/lib/books/Fröhlich2016_Network-neuroscience.pdf},
  note = {OCLC: ocn969336526}
}

@article{Piccolo2016,
  title = {Tools and Techniques for Computational Reproducibility},
  volume = {5},
  issn = {2047-217X},
  url = {https://doi.org/10.1186/s13742-016-0135-4},
  doi = {10.1186/s13742-016-0135-4},
  abstract = {When reporting research findings, scientists document the steps they followed so that others can verify and build upon the research. When those steps have been described in sufficient detail that others can retrace the steps and obtain similar results, the research is said to be reproducible. Computers play a vital role in many research disciplines and present both opportunities and challenges for reproducibility. Computers can be programmed to execute analysis tasks, and those programs can be repeated and shared with others. The deterministic nature of most computer programs means that the same analysis tasks, applied to the same data, will often produce the same outputs. However, in practice, computational findings often cannot be reproduced because of complexities in how software is packaged, installed, and executed\textemdash{}and because of limitations associated with how scientists document analysis steps. Many tools and techniques are available to help overcome these challenges; here we describe seven such strategies. With a broad scientific audience in mind, we describe the strengths and limitations of each approach, as well as the circumstances under which each might be applied. No single strategy is sufficient for every scenario; thus we emphasize that it is often useful to combine approaches.},
  journaltitle = {GigaScience},
  shortjournal = {GigaScience},
  urldate = {2017-10-24},
  date = {2016-07-11},
  pages = {30},
  keywords = {Computational reproducibility,Literate programming,Practice of science,Software containers,Software frameworks,Virtualization},
  author = {Piccolo, Stephen R. and Frampton, Michael B.},
  file = {/home/fh/lib/articles/Piccolo2016.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/5BKW2A9Z/s13742-016-0135-4.html}
}

@article{Cavallari2014,
  title = {Comparison of the Dynamics of Neural Interactions between Current-Based and Conductance-Based Integrate-and-Fire Recurrent Networks},
  volume = {8},
  issn = {1662-5110},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3943173/},
  doi = {10.3389/fncir.2014.00012},
  abstract = {Models of networks of Leaky Integrate-and-Fire (LIF) neurons are a widely used tool for theoretical investigations of brain function. These models have been used both with current- and conductance-based synapses. However, the differences in the dynamics expressed by these two approaches have been so far mainly studied at the single neuron level. To investigate how these synaptic models affect network activity, we compared the single neuron and neural population dynamics of conductance-based networks (COBNs) and current-based networks (CUBNs) of LIF neurons. These networks were endowed with sparse excitatory and inhibitory recurrent connections, and were tested in conditions including both low- and high-conductance states. We developed a novel procedure to obtain comparable networks by properly tuning the synaptic parameters not shared by the models. The so defined comparable networks displayed an excellent and robust match of first order statistics (average single neuron firing rates and average frequency spectrum of network activity). However, these comparable networks showed profound differences in the second order statistics of neural population interactions and in the modulation of these properties by external inputs. The correlation between inhibitory and excitatory synaptic currents and the cross-neuron correlation between synaptic inputs, membrane potentials and spike trains were stronger and more stimulus-modulated in the COBN. Because of these properties, the spike train correlation carried more information about the strength of the input in the COBN, although the firing rates were equally informative in both network models. Moreover, the network activity of COBN showed stronger synchronization in the gamma band, and spectral information about the input higher and spread over a broader range of frequencies. These results suggest that the second order statistics of network dynamics depend strongly on the choice of synaptic model.},
  journaltitle = {Frontiers in Neural Circuits},
  shortjournal = {Front Neural Circuits},
  urldate = {2017-10-26},
  date = {2014-03-05},
  author = {Cavallari, Stefano and Panzeri, Stefano and Mazzoni, Alberto},
  file = {/home/fh/lib/articles/Cavallari2014.pdf}
}

@report{zotero-2342,
  title = {Dataref},
  file = {/home/fh/lib/manuals/latex/dataref.pdf},
  note = {manuals/latex}
}

@article{Nishimoto2011,
  langid = {english},
  title = {Reconstructing {{Visual Experiences}} from {{Brain Activity Evoked}} by {{Natural Movies}}},
  volume = {21},
  issn = {0960-9822},
  url = {http://www.cell.com/current-biology/abstract/S0960-9822(11)00937-7},
  doi = {10.1016/j.cub.2011.08.031},
  number = {19},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  urldate = {2017-11-15},
  date = {2011-10-11},
  pages = {1641--1646},
  author = {Nishimoto, Shinji and Vu, An T. and Naselaris, Thomas and Benjamini, Yuval and Yu, Bin and Gallant, Jack L.},
  file = {/home/fh/lib/articles/Nishimoto2011.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/XGJRHECM/S0960-9822(11)00937-7.html},
  eprinttype = {pmid},
  eprint = {21945275}
}

@article{Goecks2010,
  title = {Galaxy: A Comprehensive Approach for Supporting Accessible, Reproducible, and Transparent Computational Research in the Life Sciences},
  volume = {11},
  issn = {1474-760X},
  url = {https://doi.org/10.1186/gb-2010-11-8-r86},
  doi = {10.1186/gb-2010-11-8-r86},
  shorttitle = {Galaxy},
  abstract = {Increased reliance on computational approaches in the life sciences has revealed grave concerns about how accessible and reproducible computation-reliant results truly are. Galaxy                   http://usegalaxy.org                                  , an open web-based platform for genomic research, addresses these problems. Galaxy automatically tracks and manages data provenance and provides support for capturing the context and intent of computational methods. Galaxy Pages are interactive, web-based documents that provide users with a medium to communicate a complete computational analysis.},
  journaltitle = {Genome Biology},
  shortjournal = {Genome Biology},
  urldate = {2017-11-16},
  date = {2010-08-25},
  pages = {R86},
  author = {Goecks, Jeremy and Nekrutenko, Anton and Taylor, James},
  file = {/home/fh/lib/articles/Goecks2010.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/5I7MCLSK/gb-2010-11-8-r86.html}
}

@book{Cook2017,
  langid = {english},
  location = {{Berkeley, CA}},
  title = {Docker for {{Data Science Building Scalable}} and {{Extensible Data Infrastructure Around}} the {{Jupyter Notebook Server}}},
  isbn = {978-1-4842-3012-1},
  publisher = {{Apress}},
  date = {2017},
  author = {Cook, Joshua},
  file = {/home/fh/lib/books/Cook2017_Docker-for-Data-Science-Building-Scalable-and-Extensible-Data-Infrastructure-Around-the-Jupyter-Notebook-Server.pdf},
  note = {OCLC: 1006697547}
}

@inproceedings{Hamerly2004,
  title = {Learning the k in K-Means},
  eventtitle = {Advances in Neural Information Processing Systems},
  date = {2004},
  pages = {281--288},
  author = {Hamerly, Greg and Elkan, Charles},
  file = {/home/fh/lib/conferences/Hamerly2004_Learning-the-k-in-k-means.pdf}
}

@article{Berry2017,
  langid = {english},
  title = {Spine {{Dynamics}}: {{Are They All}} the {{Same}}?},
  volume = {96},
  issn = {08966273},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627317306992},
  doi = {10.1016/j.neuron.2017.08.008},
  shorttitle = {Spine {{Dynamics}}},
  number = {1},
  journaltitle = {Neuron},
  urldate = {2017-11-22},
  date = {2017-09},
  pages = {43--55},
  author = {Berry, Kalen P. and Nedivi, Elly},
  file = {/home/fh/lib/articles/Berry2017.pdf}
}

@article{Larkman1992,
  langid = {english},
  title = {Presynaptic Release Probability Influences the Locus of Long-Term Potentiation},
  volume = {360},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/360070a0},
  doi = {10.1038/360070a0},
  abstract = {Presynaptic release probability influences the locus of long-term potentiation},
  number = {6399},
  journaltitle = {Nature},
  urldate = {2017-11-24},
  date = {1992-11-05},
  pages = {70},
  author = {Larkman, Alan and Jack, Julian and Stratford, Ken and Hannay, Timo},
  file = {/home/fh/lib/articles/Larkman1992.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/4R8BWX2P/360070a0.html}
}

@article{Ho2011,
  langid = {english},
  title = {The {{Cell Biology}} of {{Synaptic Plasticity}}},
  volume = {334},
  issn = {0036-8075, 1095-9203},
  url = {http://science.sciencemag.org/content/334/6056/623},
  doi = {10.1126/science.1209236},
  abstract = {Synaptic plasticity is the experience-dependent change in connectivity between neurons that is believed to underlie learning and memory. Here, we discuss the cellular and molecular processes that are altered when a neuron responds to external stimuli, and how these alterations lead to an increase or decrease in synaptic connectivity. Modification of synaptic components and changes in gene expression are necessary for many forms of plasticity. We focus on excitatory neurons in the mammalian hippocampus, one of the best-studied model systems of learning-related plasticity.},
  number = {6056},
  journaltitle = {Science},
  urldate = {2017-11-24},
  date = {2011-11-04},
  pages = {623--628},
  author = {Ho, Victoria M. and Lee, Ji-Ann and Martin, Kelsey C.},
  file = {/home/fh/lib/articles/Ho2011.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/IYQCMT8S/623.html},
  eprinttype = {pmid},
  eprint = {22053042}
}

@article{Edwards2007,
  title = {The {{Neurotransmitter Cycle}} and {{Quantal Size}}},
  volume = {55},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S089662730700668X},
  doi = {10.1016/j.neuron.2007.09.001},
  abstract = {Changes in the response to release of a single synaptic vesicle have generally been attributed to postsynaptic modification of receptor sensitivity, but considerable evidence now demonstrates that alterations in vesicle filling also contribute to changes in quantal size. Receptors are not saturated at many synapses, and changes in the amount of transmitter per vesicle contribute to the physiological regulation of release. On the other hand, the presynaptic factors that determine quantal size remain poorly understood. Aside from regulation of the fusion pore, these mechanisms fall into two general categories: those that affect the accumulation of transmitter inside a vesicle and those that affect vesicle size. This review will summarize current understanding of the neurotransmitter cycle and indicate basic, unanswered questions about the presynaptic regulation of quantal size.},
  number = {6},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2017-11-27},
  date = {2007-09-20},
  pages = {835--858},
  author = {Edwards, Robert H.},
  file = {/home/fh/lib/articles/Edwards2007.pdf}
}

@article{Silver2003,
  langid = {english},
  title = {High-{{Probability Uniquantal Transmission}} at {{Excitatory Synapses}} in {{Barrel Cortex}}},
  volume = {302},
  issn = {0036-8075, 1095-9203},
  url = {http://science.sciencemag.org/content/302/5652/1981},
  doi = {10.1126/science.1087160},
  abstract = {The number of vesicles released at excitatory synapses and the number of release sites per synaptic connection are key determinants of information processing in the cortex, yet they remain uncertain. Here we show that the number of functional release sites and the number of anatomically identified synaptic contacts are equal at connections between spiny stellate and pyramidal cells in rat barrel cortex. Moreover, our results indicate that the amount of transmitter released per synaptic contact is independent of release probability and the intrinsic release probability is high. These properties suggest that connections between layer 4and layer 2/3 are tuned for reliable transmission of spatially distributed, timing-based signals.},
  number = {5652},
  journaltitle = {Science},
  urldate = {2017-11-26},
  date = {2003-12-12},
  pages = {1981--1984},
  author = {Silver, R. Angus and L{\"u}bke, Joachim and Sakmann, Bert and Feldmeyer, Dirk},
  file = {/home/fh/lib/articles/Silver2003.pdf},
  eprinttype = {pmid},
  eprint = {14671309}
}

@article{Sjostrom2007,
  title = {Multiple Forms of Long-Term Plasticity at Unitary Neocortical Layer 5 Synapses},
  volume = {52},
  issn = {0028-3908},
  url = {http://www.sciencedirect.com/science/article/pii/S0028390806002310},
  doi = {10.1016/j.neuropharm.2006.07.021},
  abstract = {Long-term potentiation and depression (LTP and LTD) are cellular plasticity phenomena expressed at a variety of central synapses, and are thought to contribute to learning and developmental changes in circuitry. Recurrent neocortical layer-5 synapses are thought to express a presynaptic form of LTP that influences the short-term plasticity of the synapse. Here we show that changes in synaptic strength elicited by pairing high frequency pre- and postsynaptic firing at this synapse result from a mixture of presynaptic and postsynaptic forms of plasticity, as assessed by the analysis of changes in coefficient of variation, short-term plasticity, and NMDA:AMPA current ratios. Pharmacological dissection of this plasticity revealed that block of presynaptic LTD with an endocannabinoid inhibitor enhanced LTP, while the apparently presynaptic component of LTP could be prevented by induction in the presence of blockers of nitric oxide. These data suggest that correlated high-frequency firing at layer-5 synapses simultaneously induces a mixture of presynaptic LTD, presynaptic LTP, and postsynaptic LTP.},
  number = {1},
  journaltitle = {Neuropharmacology},
  shortjournal = {Neuropharmacology},
  series = {LTP: Forty Unforgettable Years. A Festschrift in Honour of Professor Tim Bliss FRS},
  urldate = {2017-11-28},
  date = {2007-01-01},
  pages = {176--184},
  keywords = {Endocannabinoid,Long-term depression,Long-term potentiation,Neocortical layer 5,Nitric oxide},
  author = {Sj{\"o}str{\"o}m, Per Jesper and Turrigiano, Gina G. and Nelson, Sacha B.},
  file = {/home/fh/lib/articles/Sjöström2007.pdf}
}

@article{Damour2015,
  langid = {english},
  title = {Inhibitory and {{Excitatory Spike}}-{{Timing}}-{{Dependent Plasticity}} in the {{Auditory Cortex}}},
  volume = {86},
  issn = {0896-6273},
  url = {http://www.cell.com/neuron/abstract/S0896-6273(15)00210-X},
  doi = {10.1016/j.neuron.2015.03.014},
  number = {2},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2017-11-27},
  date = {2015-04-22},
  pages = {514--528},
  author = {D'amour, James A. and Froemke, Robert C.},
  file = {/home/fh/lib/articles/D’amour2015.pdf},
  eprinttype = {pmid},
  eprint = {25843405}
}

@article{Elliott2017,
  title = {First {{Passage Time Memory Lifetimes}} for {{Simple}}, {{Multistate Synapses}}},
  volume = {29},
  issn = {0899-7667},
  url = {https://doi.org/10.1162/neco_a_01016},
  doi = {10.1162/neco_a_01016},
  abstract = {Memory models based on synapses with discrete and bounded strengths store new memories by forgetting old ones. Memory lifetimes in such memory systems may be defined in a variety of ways. A mean first passage time (MFPT) definition overcomes much of the arbitrariness and many of the problems associated with the more usual signal-to-noise ratio (SNR) definition. We have previously computed MFPT lifetimes for simple, binary-strength synapses that lack internal, plasticity-related states. In simulation we have also seen that for multistate synapses, optimality conditions based on SNR lifetimes are absent with MFPT lifetimes, suggesting that such conditions may be artifactual. Here we extend our earlier work by computing the entire first passage time (FPT) distribution for simple, multistate synapses, from which all statistics, including the MFPT lifetime, may be extracted. For this, we develop a Fokker-Planck equation using the jump moments for perceptron activation. Two models are considered that satisfy a particular eigenvector condition that this approach requires. In these models, MFPT lifetimes do not exhibit optimality conditions, while in one but not the other, SNR lifetimes do exhibit optimality. Thus, not only are such optimality conditions artifacts of the SNR approach, but they are also strongly model dependent. By examining the variance in the FPT distribution, we may identify regions in which memory storage is subject to high variability, although MFPT lifetimes are nevertheless robustly positive. In such regions, SNR lifetimes are typically (defined to be) zero. FPT-defined memory lifetimes therefore provide an analytically superior approach and also have the virtue of being directly related to a neuron's firing properties.},
  number = {12},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2017-12-04},
  date = {2017-09-28},
  pages = {3219--3259},
  author = {Elliott, Terry},
  file = {/home/fh/lib/articles/Elliott2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/EZHHGD6S/neco_a_01016.html}
}

@article{Eglen2015,
  langid = {english},
  title = {Bivariate Spatial Point Patterns in the Retina: A Reproducible Review},
  url = {https://www.biorxiv.org/content/early/2015/10/16/029348},
  doi = {10.1101/029348},
  shorttitle = {Bivariate Spatial Point Patterns in the Retina},
  abstract = {In this article I present a reproducible review of recent research to investigate the spatial positioning of neurons in the nervous system. In particular, I focus on the relative spatial positioning of pairs of cell types within the retina. I examine three different cases by which two types of neurons might be arranged relative to each other. (1) Cells of different type might be effectively independent of each other. (2) Cells of one type are randomly assigned one of two labels to create two related populations. (3) Interactions between cells of different type generate functional dependencies. I show briefly how spatial statistic techniques can be applied to investigate the nature of spatial interactions between two cell types. Finally, I have termed this article a `reproducible review9 because all the data and computer code are integrated into the manuscript so that others can repeat the analysis presented here. I close the review with a discussion of this concept.$<$/p$>$},
  journaltitle = {bioRxiv},
  urldate = {2017-12-13},
  date = {2015-10-16},
  pages = {029348},
  author = {Eglen, Stephen J.},
  file = {/home/fh/lib/articles/Eglen2015.pdf}
}

@inproceedings{Cito2016,
  langid = {english},
  title = {Using {{Docker Containers}} to {{Improve Reproducibility}} in {{Software}} and {{Web Engineering Research}}},
  isbn = {978-3-319-38790-1 978-3-319-38791-8},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-38791-8_58},
  doi = {10.1007/978-3-319-38791-8_58},
  abstract = {The ability to replicate and reproduce scientific results has become an increasingly important topic for many academic disciplines. In computer science and, more specifically, software and web engineering, contributions of scientific work rely on developed algorithms, tools and prototypes, quantitative evaluations, and other computational analyses. Published code and data come with many undocumented assumptions, dependencies, and configurations that are internal knowledge and make reproducibility hard to achieve. This tutorial presents how Docker containers can overcome these issues and aid the reproducibility of research artifacts in software and web engineering and discusses their applications in the field.},
  eventtitle = {International {{Conference}} on {{Web Engineering}}},
  booktitle = {Web {{Engineering}}},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Cham}},
  urldate = {2017-12-12},
  date = {2016-06-06},
  pages = {609--612},
  author = {Cito, J{\"u}rgen and Ferme, Vincenzo and Gall, Harald C.},
  file = {/home/fh/lib/conferences/Cito2016_Using-Docker-Containers-to-Improve-Reproducibility-in-Software-and-Web-Engineering-Research.pdf}
}

@inproceedings{Gadea2016,
  title = {A Microservices Architecture for Collaborative Document Editing Enhanced with Face Recognition},
  doi = {10.1109/SACI.2016.7507409},
  abstract = {Modern web applications can now provide rich and dynamic user experiences, such as allowing multiple users to collaboratively edit rich-text documents in real-time from multiple devices. Application architectures are evolving to support the development and deployment of such interactive functionality by decoupling software components into microservices. This paper introduces the architecture and the implementation of a collaborative rich-text editor that makes use of microservices to enable and enhance its scalable co-editing functionality. This includes microservices for synchronizing unstructured text using operational transformations, for chat functionality, and for detecting and recognizing faces in images added to the editor. The architecture makes use of Docker to allow for the development and testing of individual services as separate containers enabling seamless deployment across the available network of computers and other computing devices. The system will be demonstrated by showing how microservices make it possible for multiple users to co-edit a document where images containing faces are added and recognized as part of the document content, thereby supporting the document creation process.},
  eventtitle = {2016 {{IEEE}} 11th {{International Symposium}} on {{Applied Computational Intelligence}} and {{Informatics}} ({{SACI}})},
  booktitle = {2016 {{IEEE}} 11th {{International Symposium}} on {{Applied Computational Intelligence}} and {{Informatics}} ({{SACI}})},
  date = {2016-05},
  pages = {441--446},
  keywords = {chat functionality,Collaboration,collaborative document editing,collaborative editing,Computer architecture,Databases,docker,Docker,document creation process,document image processing,Face,face detection,face recognition,Face recognition,Google,groupware,Internet,microservices,microservices architecture,operational transformation,Real-time systems,real-time web,rich-text document editing,software architecture,text analysis,user experience,Web applications},
  author = {Gadea, C. and Trifan, M. and Ionescu, D. and Cordea, M. and Ionescu, B.},
  file = {/home/fh/lib/conferences/Gadea2016_A-microservices-architecture-for-collaborative-document-editing-enhanced-with-face-recognition.pdf}
}

@article{Gleeson2017,
  title = {A {{Commitment}} to {{Open Source}} in {{Neuroscience}}},
  volume = {96},
  issn = {0896-6273},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627317309819},
  doi = {10.1016/j.neuron.2017.10.013},
  abstract = {Modern neuroscience increasingly relies on custom-developed software, but much of this is not being made available to the wider community. A group of researchers are pledging to make code they produce for data analysis and modeling open source, and are actively encouraging their colleagues to follow suit.},
  number = {5},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2017-12-12},
  date = {2017-12-06},
  pages = {964--965},
  author = {Gleeson, Padraig and Davison, Andrew P. and Silver, R. Angus and Ascoli, Giorgio A.},
  file = {/home/fh/lib/articles/Gleeson2017.pdf}
}

@article{Clarkson2015,
  title = {The Archaeology, Chronology and Stratigraphy of {{Madjedbebe}} ({{Malakunanja II}}): {{A}} Site in Northern {{Australia}} with Early Occupation},
  volume = {83},
  issn = {0047-2484},
  url = {http://www.sciencedirect.com/science/article/pii/S0047248415000846},
  doi = {10.1016/j.jhevol.2015.03.014},
  shorttitle = {The Archaeology, Chronology and Stratigraphy of {{Madjedbebe}} ({{Malakunanja II}})},
  abstract = {Published ages of $>$50~ka for occupation at Madjedbebe (Malakunanja II) in Australia's north have kept the site prominent in discussions about the colonisation of Sahul. The site also contains one of the largest stone artefact assemblages in Sahul for this early period. However, the stone artefacts and other important archaeological components of the site have never been described in detail, leading to persistent doubts about its stratigraphic integrity. We report on our analysis of the stone artefacts and faunal and other materials recovered during the 1989 excavations, as well as the stratigraphy and depositional history recorded by the original excavators. We demonstrate that the technology and raw materials of the early assemblage are distinctive from those in the overlying layers. Silcrete and quartzite artefacts are common in the early assemblage, which also includes edge-ground axe fragments and ground haematite. The lower flaked stone assemblage is distinctive, comprising a mix of long convergent flakes, some radial flakes with faceted platforms, and many small thin silcrete flakes that we interpret as thinning flakes. Residue and use-wear analysis indicate occasional grinding of haematite and woodworking, as well as frequent abrading of platform edges on thinning flakes. We conclude that previous claims of extensive displacement of artefacts and post-depositional disturbance may have been overstated. The stone artefacts and stratigraphic details support previous claims for human occupation 50\textendash{}60~ka and show that human occupation during this time differed from later periods. We discuss the implications of these new data for understanding the first human colonisation of Sahul.},
  issue = {Supplement C},
  journaltitle = {Journal of Human Evolution},
  shortjournal = {Journal of Human Evolution},
  urldate = {2017-12-11},
  date = {2015-06-01},
  pages = {46--64},
  keywords = {Australia,Chronology,Colonisation,Grindstones,Lithic technology,Stone axes},
  author = {Clarkson, Chris and Smith, Mike and Marwick, Ben and Fullagar, Richard and Wallis, Lynley A. and Faulkner, Patrick and Manne, Tiina and Hayes, Elspeth and Roberts, Richard G. and Jacobs, Zenobia and Carah, Xavier and Lowe, Kelsey M. and Matthews, Jacqueline and Florin, S. Anna},
  file = {/home/fh/lib/articles/Clarkson2015.pdf}
}

@article{Bremges2015,
  title = {Deeply Sequenced Metagenome and Metatranscriptome of a Biogas-Producing Microbial Community from an Agricultural Production-Scale Biogas Plant},
  volume = {4},
  issn = {2047-217X},
  url = {https://doi.org/10.1186/s13742-015-0073-6},
  doi = {10.1186/s13742-015-0073-6},
  abstract = {The production of biogas takes place under anaerobic conditions and involves microbial decomposition of organic matter. Most of the participating microbes are still unknown and non-cultivable. Accordingly, shotgun metagenome sequencing currently is the method of choice to obtain insights into community composition and the genetic repertoire.},
  journaltitle = {GigaScience},
  shortjournal = {GigaScience},
  urldate = {2017-12-11},
  date = {2015-07-30},
  pages = {33},
  keywords = {Anaerobic digestion,Assembly,Biogas,Metagenomics,Metatranscriptomics,Methanogenesis,Sequencing,Wet fermentation},
  author = {Bremges, Andreas and Maus, Irena and Belmann, Peter and Eikmeyer, Felix and Winkler, Anika and Albersmeier, Andreas and P{\"u}hler, Alfred and Schl{\"u}ter, Andreas and Sczyrba, Alexander},
  file = {/home/fh/lib/articles/Bremges2015.pdf}
}

@article{Wong2010,
  title = {Points of View: {{Design}} of Data Figures},
  volume = {7},
  issn = {1548-7091, 1548-7105},
  url = {http://www.nature.com/doifinder/10.1038/nmeth0910-665},
  doi = {10.1038/nmeth0910-665},
  shorttitle = {Points of View},
  number = {9},
  journaltitle = {Nature Methods},
  urldate = {2017-12-13},
  date = {2010-09},
  pages = {665--665},
  author = {Wong, Bang},
  file = {/home/fh/lib/articles/Wong2010.pdf}
}

@article{Ghosh2017,
  langid = {english},
  title = {A Very Simple, Re-Executable Neuroimaging Publication},
  volume = {6},
  issn = {2046-1402},
  url = {https://f1000research.com/articles/6-124/v2},
  doi = {10.12688/f1000research.10783.2},
  journaltitle = {F1000Research},
  urldate = {2017-12-13},
  date = {2017-06-15},
  pages = {124},
  author = {Ghosh, Satrajit S. and Poline, Jean-Baptiste and Keator, David B. and Halchenko, Yaroslav O. and Thomas, Adam G. and Kessler, Daniel A. and Kennedy, David N.},
  file = {/home/fh/lib/articles/Ghosh2017.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/KYUG89FA/v2.html}
}

@article{Waskom2014,
  langid = {english},
  title = {Frontoparietal {{Representations}} of {{Task Context Support}} the {{Flexible Control}} of {{Goal}}-{{Directed Cognition}}},
  volume = {34},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/34/32/10743},
  doi = {10.1523/JNEUROSCI.5282-13.2014},
  abstract = {Cognitive control allows stimulus-response processing to be aligned with internal goals and is thus central to intelligent, purposeful behavior. Control is thought to depend in part on the active representation of task information in prefrontal cortex (PFC), which provides a source of contextual bias on perception, decision making, and action. In the present study, we investigated the organization, influences, and consequences of context representation as human subjects performed a cued sorting task that required them to flexibly judge the relationship between pairs of multivalent stimuli. Using a connectivity-based parcellation of PFC and multivariate decoding analyses, we determined that context is specifically and transiently represented in a region spanning the inferior frontal sulcus during context-dependent decision making. We also found strong evidence that decision context is represented within the intraparietal sulcus, an area previously shown to be functionally networked with the inferior frontal sulcus at rest and during task performance. Rule-guided allocation of attention to different stimulus dimensions produced discriminable patterns of activation in visual cortex, providing a signature of top-down bias over perception. Furthermore, demands on cognitive control arising from the task structure modulated context representation, which was found to be strongest after a shift in task rules. When context representation in frontoparietal areas increased in strength, as measured by the discriminability of high-dimensional activation patterns, the bias on attended stimulus features was enhanced. These results provide novel evidence that illuminates the mechanisms by which humans flexibly guide behavior in complex environments.},
  number = {32},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2017-12-13},
  date = {2014-08-06},
  pages = {10743--10755},
  keywords = {decision making,attention,cognitive control,prefrontal cortex},
  author = {Waskom, Michael L. and Kumaran, Dharshan and Gordon, Alan M. and Rissman, Jesse and Wagner, Anthony D.},
  file = {/home/fh/lib/articles/Waskom2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/IVU9PLLE/10743.html},
  eprinttype = {pmid},
  eprint = {25100605}
}

@article{Spoerer2017,
  langid = {english},
  title = {Recurrent {{Convolutional Neural Networks}}: {{A Better Model}} of {{Biological Object Recognition}}},
  volume = {8},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2017.01551/full},
  doi = {10.3389/fpsyg.2017.01551},
  shorttitle = {Recurrent {{Convolutional Neural Networks}}},
  abstract = {Feedforward neural networks provide the dominant model of how the brain performs visual object recognition. However, these networks lack the lateral and feedback connections, and the resulting recurrent neuronal dynamics, of the ventral visual pathway in the human and nonhuman primate brain. Here we investigate recurrent convolutional neural networks with bottom-up (B), lateral (L), and top-down (T) connections. Combining these types of connections yields four architectures (B, BT, BL, and BLT), which we systematically test and compare. We hypothesized that recurrent dynamics might improve recognition performance in the challenging scenario of partial occlusion. We introduce two novel occluded object recognition tasks to test the efficacy of the models, $\backslash$emph\{digit clutter\} (where multiple target digits occlude one another) and $\backslash$emph\{digit debris\} (where target digits are occluded by digit fragments). We find that recurrent neural networks outperform feedforward control models (approximately matched in parametric complexity) at recognising objects, both in the absence of occlusion and in all occlusion conditions. Recurrent networks were also found to be more robust to the inclusion of additive Gaussian noise. Recurrent neural networks are better in two respects: (1) they are more neurobiologically realistic than their feedforward counterparts; (2) they are better in terms of their ability to recognise objects, especially under challenging conditions. This work shows that computer vision can benefit from using recurrent convolutional architectures and suggests that the ubiquitous recurrent connections in biological brains are essential for task performance.},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  urldate = {2017-12-14},
  date = {2017},
  keywords = {Convolutional Neural Network,object recognition,occlusion,recurrent neural network,top-down processing},
  author = {Spoerer, Courtney J. and McClure, Patrick and Kriegeskorte, Nikolaus},
  file = {/home/fh/lib/articles/Spoerer2017.pdf}
}

@article{Wernle2017,
  langid = {english},
  title = {Integration of Grid Maps in Merged Environments},
  issn = {1546-1726},
  url = {https://www.nature.com/articles/s41593-017-0036-6},
  doi = {10.1038/s41593-017-0036-6},
  abstract = {The authors investigate grid cell dynamics after removal of a border between two environments. Near the transition between environments, grid fields changed location, resulting in local spatial periodicity and continuity between the original maps.$<$/p$>$},
  journaltitle = {Nature Neuroscience},
  urldate = {2017-12-14},
  date = {2017-12-11},
  pages = {1},
  author = {Wernle, Tanja and Waaga, Torgeir and M{\o}rreaunet, Maria and Treves, Alessandro and Moser, May-Britt and Moser, Edvard I.},
  file = {/home/fh/lib/articles/Wernle2017.pdf}
}

@book{Ben-Israel2003,
  location = {{New York}},
  title = {Generalized Inverses: Theory and Applications},
  edition = {2nd ed},
  isbn = {978-0-387-00293-4},
  shorttitle = {Generalized Inverses},
  pagetotal = {420},
  number = {15},
  series = {CMS books in mathematics},
  publisher = {{Springer}},
  date = {2003},
  keywords = {Matrix inversion},
  author = {Ben-Israel, Adi and Greville, T. N. E.},
  file = {/home/fh/lib/books/Ben-Israel2003_Generalized-inverses-theory-and-applications.pdf}
}

@article{Penrose1955,
  langid = {english},
  title = {A Generalized Inverse for Matrices},
  volume = {51},
  issn = {1469-8064, 0305-0041},
  url = {https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/generalized-inverse-for-matrices/5F4516D6B9989BB6563A4B267CC7D615},
  doi = {10.1017/S0305004100030401},
  abstract = {This paper describes a generalization of the inverse of a non-singular matrix, as the unique solution of a certain set of equations. This generalized inverse exists for any (possibly rectangular) matrix whatsoever with complex elements. It is used here for solving linear matrix equations, and among other applications for finding an expression for the principal idempotent elements of a matrix. Also a new type of spectral decomposition is given.},
  number = {3},
  journaltitle = {Mathematical Proceedings of the Cambridge Philosophical Society},
  urldate = {2018-04-01},
  date = {1955-07},
  pages = {406--413},
  author = {Penrose, R.},
  file = {/home/fh/lib/articles/Penrose1955.pdf}
}

@inproceedings{Nallapu2016,
  title = {Dynamics of {{Reward Based Decision Making}}: {{A Computational Study}}},
  booktitle = {International {{Conference}} on {{Artificial Neural Networks}}},
  publisher = {{Springer}},
  date = {2016},
  pages = {322--329},
  author = {Nallapu, Bhargav Teja and Rougier, Nicolas P},
  file = {/home/fh/lib/conferences/Nallapu2016_Dynamics-of-Reward-Based-Decision-Making-A-Computational-Study.pdf}
}

@article{Stern2018,
  langid = {english},
  title = {A Transformation from Temporal to Ensemble Coding in a Model of Piriform Cortex},
  volume = {7},
  issn = {2050-084X},
  url = {https://elifesciences.org/articles/34831},
  doi = {10.7554/eLife.34831},
  journaltitle = {eLife},
  shortjournal = {eLife Sciences},
  urldate = {2018-03-30},
  date = {2018-03-29},
  pages = {e34831},
  author = {Stern, Merav and Bolding, Kevin A. and Abbott, Larry F. and Franks, Kevin M.},
  file = {/home/fh/lib/articles/Stern2018.pdf}
}

@article{Ha2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.10122},
  primaryClass = {cs, stat},
  title = {World {{Models}}},
  url = {http://arxiv.org/abs/1803.10122},
  doi = {10.5281/zenodo.1207631},
  abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at \$$\backslash$href\{https://worldmodels.github.io/\}\{$\backslash$mathtt\{https://worldmodels.github.io\}\}\$.},
  urldate = {2018-03-28},
  date = {2018-03-27},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  file = {/home/fh/lib/articles/Ha2018.pdf}
}

@article{Scheler2017,
  langid = {english},
  title = {Logarithmic Distributions Prove That Intrinsic Learning Is {{Hebbian}}},
  volume = {6},
  issn = {2046-1402},
  url = {https://f1000research.com/articles/6-1222/v2},
  doi = {10.12688/f1000research.12130.2},
  journaltitle = {F1000Research},
  urldate = {2018-03-26},
  date = {2017-10-11},
  pages = {1222},
  author = {Scheler, Gabriele},
  file = {/home/fh/lib/articles/Scheler2017.pdf}
}

@article{Hosoda2011,
  title = {Origin of Lognormal-like Distributions with a Common Width in a Growth and Division Process},
  volume = {83},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.83.031118},
  doi = {10.1103/PhysRevE.83.031118},
  abstract = {Lognormal statistical distributions are observed in a variety of scientific fields. The widths of these distributions in the log scale are often similar, but the underlying mechanism that maintains these widths within a small range has not been well explained. We show that a stochastic process of halving followed by addition can yield a stationary distribution that resembles the universal lognormal distribution with a certain width. The mechanism that we propose here would provide insight into the essence of why lognormal-like distributions in many systems have a common width.},
  number = {3},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  urldate = {2018-03-26},
  date = {2011-03-16},
  pages = {031118},
  author = {Hosoda, Kazufumi and Matsuura, Tomoaki and Suzuki, Hiroaki and Yomo, Tetsuya},
  file = {/home/fh/lib/articles/Hosoda2011.pdf}
}

@article{Cutts2014,
  langid = {english},
  title = {Detecting {{Pairwise Correlations}} in {{Spike Trains}}: {{An Objective Comparison}} of {{Methods}} and {{Application}} to the {{Study}} of {{Retinal Waves}}},
  volume = {34},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/34/43/14288},
  doi = {10.1523/JNEUROSCI.2767-14.2014},
  shorttitle = {Detecting {{Pairwise Correlations}} in {{Spike Trains}}},
  abstract = {Correlations in neuronal spike times are thought to be key to processing in many neural systems. Many measures have been proposed to summarize these correlations and of these the correlation index is widely used and is the standard in studies of spontaneous retinal activity. We show that this measure has two undesirable properties: it is unbounded above and confounded by firing rate. We list properties needed for a measure to fairly quantify and compare correlations and we propose a novel measure of correlation\textemdash{}the spike time tiling coefficient. This coefficient, the correlation index, and 33 other measures of correlation of spike times are blindly tested for the required properties on synthetic and experimental data. Based on this, we propose a measure (the spike time tiling coefficient) to replace the correlation index. To demonstrate the benefits of this measure, we reanalyze data from seven key studies, which previously used the correlation index to investigate the nature of spontaneous activity. We reanalyze data from $\beta$2(KO) and $\beta$2(TG) mutants, mutants lacking connexin isoforms, and also the age-dependent changes in wild-type and $\beta$2(KO) correlations. Reanalysis of the data using the proposed measure can significantly change the conclusions. It leads to better quantification of correlations and therefore better inference from the data. We hope that the proposed measure will have wide applications, and will help clarify the role of activity in retinotopic map formation.},
  number = {43},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2018-03-26},
  date = {2014-10-22},
  pages = {14288--14303},
  keywords = {correlations,activity,development,retina,retinotopic map,spike times},
  author = {Cutts, Catherine S. and Eglen, Stephen J.},
  file = {/home/fh/lib/articles/Cutts2014.pdf},
  eprinttype = {pmid},
  eprint = {25339742}
}

@article{Bloss2018,
  langid = {english},
  title = {Single Excitatory Axons Form Clustered Synapses onto {{CA1}} Pyramidal Cell Dendrites},
  volume = {21},
  issn = {1546-1726},
  url = {https://www.nature.com/articles/s41593-018-0084-6},
  doi = {10.1038/s41593-018-0084-6},
  abstract = {Bloss et al. show single axons form clustered inputs onto the dendrites of hippocampal pyramidal cells in a projection-specific manner. The spatial and temporal features inherent in these connections efficiently drive dendritic depolarization.},
  number = {3},
  journaltitle = {Nature Neuroscience},
  urldate = {2018-03-26},
  date = {2018-03},
  pages = {353--363},
  author = {Bloss, Erik B. and Cembrowski, Mark S. and Karsh, Bill and Colonell, Jennifer and Fetter, Richard D. and Spruston, Nelson},
  file = {/home/fh/lib/articles/Bloss2018.pdf}
}

@article{Tran2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.03585},
  primaryClass = {cs},
  title = {The {{Importance}} of {{Being Recurrent}} for {{Modeling Hierarchical Structure}}},
  url = {http://arxiv.org/abs/1803.03585},
  abstract = {Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks such as language modeling (Linzen et al., 2016) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures---recurrent versus non-recurrent---with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose.},
  urldate = {2018-03-21},
  date = {2018-03-09},
  keywords = {Computer Science - Computation and Language},
  author = {Tran, Ke and Bisazza, Arianna and Monz, Christof},
  file = {/home/fh/lib/articles/Tran2018.pdf}
}

@article{Forster2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.08448},
  primaryClass = {cs, stat},
  title = {Neural {{Simpletrons}} - {{Minimalistic Directed Generative Networks}} for {{Learning}} with {{Few Labels}}},
  url = {http://arxiv.org/abs/1506.08448},
  abstract = {Classifiers for the semi-supervised setting often combine strong supervised models with additional learning objectives to make use of unlabeled data. This results in powerful though very complex models that are hard to train and that demand additional labels for optimal parameter tuning, which are often not given when labeled data is very sparse. We here study a minimalistic multi-layer generative neural network for semi-supervised learning in a form and setting as similar to standard discriminative networks as possible. Based on normalized Poisson mixtures, we derive compact and local learning and neural activation rules. Learning and inference in the network can be scaled using standard deep learning tools for parallelized GPU implementation. With the single objective of likelihood optimization, both labeled and unlabeled data are naturally incorporated into learning. Empirical evaluations on standard benchmarks show, that for datasets with few labels the derived minimalistic network improves on all classical deep learning approaches and is competitive with their recent variants without the need of additional labels for parameter tuning. Furthermore, we find that the studied network is the best performing monolithic (`non-hybrid') system for few labels, and that it can be applied in the limit of very few labels, where no other system has been reported to operate so far.},
  urldate = {2018-03-21},
  date = {2015-06-28},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Forster, Dennis and Sheikh, Abdul-Saboor and L{\"u}cke, J{\"o}rg},
  file = {/home/fh/lib/articles/Forster2015.pdf}
}

@article{Diehl2015,
  langid = {english},
  title = {Unsupervised Learning of Digit Recognition Using Spike-Timing-Dependent Plasticity},
  volume = {9},
  issn = {1662-5188},
  url = {https://www.frontiersin.org/articles/10.3389/fncom.2015.00099/full},
  doi = {10.3389/fncom.2015.00099},
  abstract = {In order to understand how the mammalian neocortex is performing computations, two things are necessary; we need to have a good understanding of the available neuronal processing units and mechanisms, and we need to gain a better understanding of how those mechanisms are combined to build functioning systems. Therefore, in recent years there is an increasing interest in how spiking neural networks (SNN) can be used to perform complex computations or solve pattern recognition tasks. However, it remains a challenging task to design SNNs which use biologically plausible mechanisms (especially for learning new patterns), since most of such SNN architectures rely on training in a rate-based network and subsequent conversion to a SNN. We present a SNN for digit recognition which is based on mechanisms with increased biological plausibility, i.e. conductance-based instead of current-based synapses, spike-timing-dependent plasticity with time-dependent weight change, lateral inhibition, and an adaptive spiking threshold. Unlike most other systems, we do not use a teaching signal and do not present any class labels to the network. Using this unsupervised learning scheme, our architecture achieves 95\% accuracy on the MNIST benchmark, which is better than previous SNN implementations without supervision. The fact that we used no domain-specific knowledge points toward the general applicability of our network design. Also, the performance of our network scales well with the number of neurons used and shows similar performance for four different learning rules, indicating robustness of the full combination of mechanisms, which suggests applicability in heterogeneous biological neural networks.},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci.},
  urldate = {2018-03-21},
  date = {2015},
  keywords = {STDP,unsupervised learning,Classification,Digit recognition,Spiking Neural network},
  author = {Diehl, Peter U. and Cook, Matthew},
  file = {/home/fh/lib/articles/Diehl2015.pdf}
}

@article{Davies2018,
  title = {Loihi: {{A Neuromorphic Manycore Processor}} with {{On}}-{{Chip Learning}}},
  volume = {38},
  issn = {0272-1732},
  doi = {10.1109/MM.2018.112130359},
  shorttitle = {Loihi},
  abstract = {Loihi is a 60-mm2 chip fabricated in Intels 14-nm process that advances the state-of-the-art modeling of spiking neural networks in silicon. It integrates a wide range of novel features for the field, such as hierarchical connectivity, dendritic compartments, synaptic delays, and, most importantly, programmable synaptic learning rules. Running a spiking convolutional form of the Locally Competitive Algorithm, Loihi can solve LASSO optimization problems with over three orders of magnitude superior energy-delay-product compared to conventional solvers running on a CPU iso-process/voltage/area. This provides an unambiguous example of spike-based computation, outperforming all known conventional solutions.},
  number = {1},
  journaltitle = {IEEE Micro},
  date = {2018-01},
  pages = {82--99},
  keywords = {Neurons,_tablet,Biological neural networks,artificial intelligence,Computer architecture,Algorithm design and analysis,Computational modeling,machine learning,neuromorphic computing,Neuromorphics},
  author = {Davies, M. and Srinivasa, N. and Lin, T. H. and Chinya, G. and Cao, Y. and Choday, S. H. and Dimou, G. and Joshi, P. and Imam, N. and Jain, S. and Liao, Y. and Lin, C. K. and Lines, A. and Liu, R. and Mathaikutty, D. and McCoy, S. and Paul, A. and Tse, J. and Venkataramanan, G. and Weng, Y. H. and Wild, A. and Yang, Y. and Wang, H.},
  file = {/home/fh/lib/articles/Davies2018.pdf}
}

@article{Himmelstein2018,
  langid = {english},
  title = {Research: {{Sci}}-{{Hub}} Provides Access to Nearly All Scholarly Literature},
  volume = {7},
  issn = {2050-084X},
  url = {https://elifesciences.org/articles/32822},
  doi = {10.7554/eLife.32822},
  shorttitle = {Research},
  abstract = {The availability of almost all articles from toll access journals in the Sci-Hub repository will disrupt scholarly publishing towards more open models.},
  journaltitle = {eLife},
  shortjournal = {eLife Sciences},
  urldate = {2018-03-20},
  date = {2018-02-09},
  pages = {e32822},
  author = {Himmelstein, Daniel S. and Romero, Ariel Rodriguez and Levernier, Jacob G. and Munro, Thomas Anthony and McLaughlin, Stephen Reid and Tzovaras, Bastian Greshake and Greene, Casey S.},
  file = {/home/fh/lib/articles/Himmelstein2018.pdf}
}

@article{Heeger2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.06288},
  primaryClass = {cs, q-bio},
  title = {{{ORGaNICs}}: {{A Theory}} of {{Working Memory}} in {{Brains}} and {{Machines}}},
  url = {http://arxiv.org/abs/1803.06288},
  shorttitle = {{{ORGaNICs}}},
  abstract = {Working memory is a cognitive process that is responsible for temporarily holding and manipulating information. Most of the empirical neuroscience research on working memory has focused on measuring sustained activity in prefrontal cortex (PFC) and/or parietal cortex during simple delayed-response tasks, and most of the models of working memory have been based on neural integrators. But working memory means much more than just holding a piece of information online. We describe a new theory of working memory, based on a recurrent neural circuit that we call ORGaNICs (Oscillatory Recurrent GAted Neural Integrator Circuits). ORGaNICs are a variety of Long Short Term Memory units (LSTMs), imported from machine learning and artificial intelligence. ORGaNICs can be used to explain the complex dynamics of delay-period activity in prefrontal cortex (PFC) during a working memory task. The theory is analytically tractable so that we can characterize the dynamics, and the theory provides a means for reading out information from the dynamically varying responses at any point in time, in spite of the complex dynamics. ORGaNICs can be implemented with a biophysical (electrical circuit) model of pyramidal cells, combined with shunting inhibition via a thalamocortical loop. Although introduced as a computational theory of working memory, ORGaNICs are also applicable to models of sensory processing, motor preparation and motor control. ORGaNICs offer computational advantages compared to other varieties of LSTMs that are commonly used in AI applications. Consequently, ORGaNICs are a framework for canonical computation in brains and machines.},
  urldate = {2018-03-19},
  date = {2018-03-16},
  keywords = {Quantitative Biology - Neurons and Cognition,Computer Science - Artificial Intelligence},
  author = {Heeger, David J. and Mackey, Wayne E.},
  file = {/home/fh/lib/articles/Heeger2018.pdf}
}

@article{Mallat1993,
  title = {Matching Pursuits with Time-Frequency Dictionaries},
  volume = {41},
  issn = {1053-587X},
  doi = {10.1109/78.258082},
  abstract = {The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992)},
  number = {12},
  journaltitle = {IEEE Transactions on Signal Processing},
  date = {1993-12},
  pages = {3397--3415},
  keywords = {adaptive signal representations,adaptive time-frequency transform,Dictionaries,Fourier transforms,Gabor functions,Interference,linear waveform expansion,matching pursuit algorithm,Matching pursuit algorithms,matching pursuit decomposition,Natural languages,noisy signals,optimized wavepacket orthonormal basis,pattern extraction,Pursuit algorithms,signal energy distribution,signal expansion,signal processing,Signal processing algorithms,Signal representations,signal structures,Time frequency analysis,time-frequency analysis,time-frequency dictionaries,time-frequency plane,Vocabulary,wavelet transforms},
  author = {Mallat, S. G. and Zhang, Zhifeng},
  file = {/home/fh/lib/articles/Mallat1993.pdf}
}

@article{Vu2018,
  langid = {english},
  title = {A {{Shared Vision}} for {{Machine Learning}} in {{Neuroscience}}},
  volume = {38},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/38/7/1601},
  doi = {10.1523/JNEUROSCI.0508-17.2018},
  abstract = {With ever-increasing advancements in technology, neuroscientists are able to collect data in greater volumes and with finer resolution. The bottleneck in understanding how the brain works is consequently shifting away from the amount and type of data we can collect and toward what we actually do with the data. There has been a growing interest in leveraging this vast volume of data across levels of analysis, measurement techniques, and experimental paradigms to gain more insight into brain function. Such efforts are visible at an international scale, with the emergence of big data neuroscience initiatives, such as the BRAIN initiative (Bargmann et al., 2014), the Human Brain Project, the Human Connectome Project, and the National Institute of Mental Health's Research Domain Criteria initiative. With these large-scale projects, much thought has been given to data-sharing across groups (Poldrack and Gorgolewski, 2014; Sejnowski et al., 2014); however, even with such data-sharing initiatives, funding mechanisms, and infrastructure, there still exists the challenge of how to cohesively integrate all the data. At multiple stages and levels of neuroscience investigation, machine learning holds great promise as an addition to the arsenal of analysis tools for discovering how the brain works.},
  number = {7},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2018-03-19},
  date = {2018-02-14},
  pages = {1601--1607},
  keywords = {machine learning,explainable artificial intelligence,reinforcement learning},
  author = {Vu, Mai-Anh T. and Adal{\i}, T{\"u}lay and Ba, Demba and Buzs{\'a}ki, Gy{\"o}rgy and Carlson, David and Heller, Katherine and Liston, Conor and Rudin, Cynthia and Sohal, Vikaas S. and Widge, Alik S. and Mayberg, Helen S. and Sapiro, Guillermo and Dzirasa, Kafui},
  file = {/home/fh/lib/articles/Vu2018.pdf},
  eprinttype = {pmid},
  eprint = {29374138}
}

@article{Kietzmann2017,
  title = {Deep {{Neural Networks In Computational Neuroscience}}},
  url = {http://biorxiv.org/lookup/doi/10.1101/133504},
  doi = {10.1101/133504},
  abstract = {The goal of computational neuroscience is to find mechanistic explanations of how the nervous system processes information to support cognitive function and behaviour. At the heart of the field are its models, i.e. mathematical and computational descriptions of the system being studied. These models typically map sensory stimuli to neural responses and/or neural to behavioural responses and range for simple to complex. Recently, deep neural networks (DNNs), using either feedforward and recurrent architectures, have come to dominate several domains of artificial intelligence (AI). As the term ``neural network'' suggests, these models are inspired by biological brains. However, current DNN models abstract from many details of biological neural networks. Their abstractions contribute to their computational efficiency, enabling to perform complex feats of intelligence, ranging from perceptual tasks (e.g. visual object and auditory speech recognition) to cognitive tasks (e.g. machine translation), and on to motor control tasks (e.g. playing computer games or controlling a robot arm). In addition to their ability to model complex intelligent behaviours, DNNs have been shown to predict neural responses to novel sensory stimuli that cannot be predicted with any other currently available type of model. DNNs can have millions of parameters (connection strengths), which are required to capture the domain knowledge needed for task performance. These parameters are often set by task training using stochastic gradient descent. The computational properties of the units are the result of four directly manipulable elements: input statistics, network structure, functional objective, and learning algorithm. The advances with neural nets in engineering provide the technological basis for building task-performing models of varying degrees of biological realism that promise substantial insights for computational neuroscience.},
  urldate = {2018-03-17},
  date = {2017-05-04},
  pages = {--},
  author = {Kietzmann, Tim Christian and McClure, Patrick and Kriegeskorte, Nikolaus},
  file = {/home/fh/lib/articles/Kietzmann2017.pdf}
}

@article{Maass2002,
  title = {Real-{{Time Computing Without Stable States}}: {{A New Framework}} for {{Neural Computation Based}} on {{Perturbations}}},
  volume = {14},
  issn = {0899-7667},
  url = {https://doi.org/10.1162/089976602760407955},
  doi = {10.1162/089976602760407955},
  shorttitle = {Real-{{Time Computing Without Stable States}}},
  abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
  number = {11},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2018-03-16},
  date = {2002-11-01},
  pages = {2531--2560},
  author = {Maass, Wolfgang and Natschl{\"a}ger, Thomas and Markram, Henry},
  file = {/home/fh/lib/articles/Maass2002.pdf}
}

@article{Lukosevicius2009a,
  title = {Reservoir Computing Approaches to Recurrent Neural Network Training},
  volume = {3},
  issn = {1574-0137},
  url = {http://www.sciencedirect.com/science/article/pii/S1574013709000173},
  doi = {10.1016/j.cosrev.2009.03.005},
  abstract = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current ``brand-names'' of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed ``map'' of it.},
  number = {3},
  journaltitle = {Computer Science Review},
  shortjournal = {Computer Science Review},
  urldate = {2018-03-16},
  date = {2009-08-01},
  pages = {127--149},
  author = {Luko{\v s}evi{\v c}ius, Mantas and Jaeger, Herbert},
  file = {/home/fh/lib/articles/Lukoševičius2009.pdf}
}

@article{Stodden2018,
  langid = {english},
  title = {An Empirical Analysis of Journal Policy Effectiveness for Computational Reproducibility},
  volume = {115},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/115/11/2584},
  doi = {10.1073/pnas.1708290115},
  abstract = {A key component of scientific communication is sufficient information for other researchers in the field to reproduce published findings. For computational and data-enabled research, this has often been interpreted to mean making available the raw data from which results were generated, the computer code that generated the findings, and any additional information needed such as workflows and input parameters. Many journals are revising author guidelines to include data and code availability. This work evaluates the effectiveness of journal policy that requires the data and code necessary for reproducibility be made available postpublication by the authors upon request. We assess the effectiveness of such a policy by (i) requesting data and code from authors and (ii) attempting replication of the published findings. We chose a random sample of 204 scientific papers published in the journal Science after the implementation of their policy in February 2011. We found that we were able to obtain artifacts from 44\% of our sample and were able to reproduce the findings for 26\%. We find this policy\textemdash{}author remission of data and code postpublication upon request\textemdash{}an improvement over no policy, but currently insufficient for reproducibility.},
  number = {11},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  urldate = {2018-03-15},
  date = {2018-03-13},
  pages = {2584--2589},
  keywords = {code access,data access,open science,reproducibility policy,reproducible research},
  author = {Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun},
  file = {/home/fh/lib/articles/Stodden2018.pdf},
  eprinttype = {pmid},
  eprint = {29531050}
}

@article{Savin2010,
  langid = {english},
  title = {Independent {{Component Analysis}} in {{Spiking Neurons}}},
  volume = {6},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000757},
  doi = {10.1371/journal.pcbi.1000757},
  abstract = {Although models based on independent component analysis (ICA) have been successful in explaining various properties of sensory coding in the cortex, it remains unclear how networks of spiking neurons using realistic plasticity rules can realize such computation. Here, we propose a biologically plausible mechanism for ICA-like learning with spiking neurons. Our model combines spike-timing dependent plasticity and synaptic scaling with an intrinsic plasticity rule that regulates neuronal excitability to maximize information transmission. We show that a stochastically spiking neuron learns one independent component for inputs encoded either as rates or using spike-spike correlations. Furthermore, different independent components can be recovered, when the activity of different neurons is decorrelated by adaptive lateral inhibition.},
  number = {4},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2018-03-15},
  date = {2010-04-22},
  pages = {e1000757},
  keywords = {Neurons,Action potentials,Neuronal plasticity,Synapses,Coding mechanisms,Neural networks,Synaptic plasticity,Transfer functions},
  author = {Savin, Cristina and Joshi, Prashant and Triesch, Jochen},
  file = {/home/fh/lib/articles/Savin2010.pdf}
}

@article{Nessler2013,
  langid = {english},
  title = {Bayesian {{Computation Emerges}} in {{Generic Cortical Microcircuits}} through {{Spike}}-{{Timing}}-{{Dependent Plasticity}}},
  volume = {9},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003037},
  doi = {10.1371/journal.pcbi.1003037},
  abstract = {The principles by which networks of neurons compute, and how spike-timing dependent plasticity (STDP) of synaptic weights generates and maintains their computational function, are unknown. Preceding work has shown that soft winner-take-all (WTA) circuits, where pyramidal neurons inhibit each other via interneurons, are a common motif of cortical microcircuits. We show through theoretical analysis and computer simulations that Bayesian computation is induced in these network motifs through STDP in combination with activity-dependent changes in the excitability of neurons. The fundamental components of this emergent Bayesian computation are priors that result from adaptation of neuronal excitability and implicit generative models for hidden causes that are created in the synaptic weights through STDP. In fact, a surprising result is that STDP is able to approximate a powerful principle for fitting such implicit generative models to high-dimensional spike inputs: Expectation Maximization. Our results suggest that the experimentally observed spontaneous activity and trial-to-trial variability of cortical neurons are essential features of their information processing capability, since their functional role is to represent probability distributions rather than static neural codes. Furthermore it suggests networks of Bayesian computation modules as a new model for distributed information processing in the cortex.},
  number = {4},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2018-03-15},
  date = {2013-04-25},
  pages = {e1003037},
  keywords = {Neurons,Action potentials,Neuronal plasticity,Synapses,Machine learning,Probability distribution,Neural networks,Learning},
  author = {Nessler, Bernhard and Pfeiffer, Michael and Buesing, Lars and Maass, Wolfgang},
  file = {/home/fh/lib/articles/Nessler2013.pdf}
}

@article{Zhu2017,
  langid = {english},
  title = {Joint {{Learning}} of {{Binocularly Driven Saccades}} and {{Vergence}} by {{Active Efficient Coding}}},
  volume = {11},
  issn = {1662-5218},
  url = {https://www.frontiersin.org/articles/10.3389/fnbot.2017.00058/full},
  doi = {10.3389/fnbot.2017.00058},
  abstract = {This paper investigates two types of eye movements: vergence and saccades. Vergence eye movements are responsible for bringing the images of the two eyes into correspondence whereas saccades drive gaze to interesting regions in the scene. Control of both vergence and saccades develop during early infancy. To date, these two types of eye movements have been studied separately. Here we propose a computational model of an active vision system that integrates these two types of eye movements. We hypothesize that incorporating a saccade strategy driven by bottom-up attention will benefit the development of vergence control. The integrated system is based on the Active Efficient Coding framework, which describes the joint development of sensory processing and eye movement control to jointly optimize the coding efficiency of the sensory system. In the integrated system, we propose a binocular saliency model to drive saccades based on learned binocular feature extractors, which simultaneously encode both depth and texture information. Saliency in our model also depends on the current fixation point. This extends prior work, which focused on monocular images and saliency measures that are independent of the current fixation. Our results show that the proposed saliency driven saccades lead to better vergence performance and faster learning in the overall system than random saccades. Faster learning is significant because it indicates that the system actively selects inputs for the most effective learning. This work suggests that saliency driven saccades provide a scaffold for the development of vergence control during infancy.},
  journaltitle = {Frontiers in Neurorobotics},
  shortjournal = {Front. Neurorobot.},
  urldate = {2018-03-15},
  date = {2017},
  keywords = {reinforcement learning,Active Efficient Coding,binocular saliency map,GASSOM,Saccades,vergence},
  author = {Zhu, Qingpeng and Triesch, Jochen and Shi, Bertram E.},
  file = {/home/fh/lib/articles/Zhu2017.pdf}
}

@article{Brette2017,
  langid = {english},
  title = {Is Coding a Relevant Metaphor for the Brain?},
  url = {https://www.biorxiv.org/content/early/2017/07/31/168237},
  doi = {10.1101/168237},
  abstract = {"Neural coding" is a popular metaphor in neuroscience, where objective properties of the world are communicated to the brain in the form of spikes. Here I argue that this metaphor is often inappropriate and misleading. First, when neurons are said to encode experimental parameters, the implied communication channel consists of both the experimental and biological system. Thus, the terms "neural code" are used inappropriately when "neuroexperimental code" would be more accurate, although less insightful. Second, the brain cannot be presumed to decode neural messages into objective properties of the world, since it never gets to observe those properties. To avoid dualism, codes must relate not to external properties but to internal sensorimotor models. Because this requires structured representations, neural assemblies cannot be the basis of such codes. Third, a message is informative to the extent that the reader understands its language. But the neural code is private to the encoder since only the message is communicated: each neuron speaks its own language. It follows that in the neural coding metaphor, the brain is a Tower of Babel. Finally, the relation between input signals and actions is circular; that inputs do not preexist to outputs makes the coding paradigm problematic. I conclude that the view that spikes are messages is generally not tenable. An alternative proposition is that action potentials are actions on other neurons and the environment, and neurons interact with each other rather than exchange messages.},
  journaltitle = {bioRxiv},
  urldate = {2018-03-15},
  date = {2017-07-31},
  pages = {168237},
  author = {Brette, Romain},
  file = {/home/fh/lib/articles/Brette2017.pdf}
}

@book{Press2007,
  langid = {english},
  location = {{Cambridge, UK; New York}},
  title = {Numerical Recipes: The Art of Scientific Computing},
  isbn = {978-0-511-33555-6},
  shorttitle = {Numerical Recipes},
  publisher = {{Cambridge University Press}},
  date = {2007},
  author = {Press, William H},
  file = {/home/fh/lib/books/Press2007_Numerical-recipes-the-art-of-scientific-computing.pdf},
  note = {OCLC: 212427139}
}

@article{Clauset2009a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0706.1062},
  title = {Power-Law Distributions in Empirical Data},
  volume = {51},
  issn = {0036-1445, 1095-7200},
  url = {http://arxiv.org/abs/0706.1062},
  doi = {10.1137/070710111},
  abstract = {Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution -- the part of the distribution representing large but rare events -- and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out.},
  number = {4},
  journaltitle = {SIAM Review},
  urldate = {2018-03-13},
  date = {2009-11-04},
  pages = {661--703},
  keywords = {Physics - Data Analysis; Statistics and Probability,Condensed Matter - Disordered Systems and Neural Networks,Statistics - Applications,Statistics - Methodology},
  author = {Clauset, Aaron and Shalizi, Cosma Rohilla and Newman, M. E. J.},
  file = {/home/fh/lib/articles/Clauset22.pdf}
}

@article{Klaus2011a,
  langid = {english},
  title = {Statistical {{Analyses Support Power Law Distributions Found}} in {{Neuronal Avalanches}}},
  volume = {6},
  issn = {1932-6203},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0019779},
  doi = {10.1371/journal.pone.0019779},
  abstract = {The size distribution of neuronal avalanches in cortical networks has been reported to follow a power law distribution with exponent close to -1.5, which is a reflection of long-range spatial correlations in spontaneous neuronal activity. However, identifying power law scaling in empirical data can be difficult and sometimes controversial. In the present study, we tested the power law hypothesis for neuronal avalanches by using more stringent statistical analyses. In particular, we performed the following steps: (i) analysis of finite-size scaling to identify scale-free dynamics in neuronal avalanches, (ii) model parameter estimation to determine the specific exponent of the power law, and (iii) comparison of the power law to alternative model distributions. Consistent with critical state dynamics, avalanche size distributions exhibited robust scaling behavior in which the maximum avalanche size was limited only by the spatial extent of sampling (``finite size'' effect). This scale-free dynamics suggests the power law as a model for the distribution of avalanche sizes. Using both the Kolmogorov-Smirnov statistic and a maximum likelihood approach, we found the slope to be close to -1.5, which is in line with previous reports. Finally, the power law model for neuronal avalanches was compared to the exponential and to various heavy-tail distributions based on the Kolmogorov-Smirnov distance and by using a log-likelihood ratio test. Both the power law distribution without and with exponential cut-off provided significantly better fits to the cluster size distributions in neuronal avalanches than the exponential, the lognormal and the gamma distribution. In summary, our findings strongly support the power law scaling in neuronal avalanches, providing further evidence for critical state dynamics in superficial layers of cortex.},
  number = {5},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2018-03-13},
  date = {2011-05-26},
  pages = {e19779},
  keywords = {Probability distribution,Statistical distributions,Statistical models,Anesthesia,Electrode recording,Microelectrodes,Monkeys,Test statistics},
  author = {Klaus, Andreas and Yu, Shan and Plenz, Dietmar},
  file = {/home/fh/lib/articles/Klaus22.pdf}
}

@article{Alstott2014a,
  langid = {english},
  title = {Powerlaw: {{A Python Package}} for {{Analysis}} of {{Heavy}}-{{Tailed Distributions}}},
  volume = {9},
  issn = {1932-6203},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0085777},
  doi = {10.1371/journal.pone.0085777},
  shorttitle = {Powerlaw},
  abstract = {Power laws are theoretically interesting probability distributions that are also frequently used to describe empirical data. In recent years, effective statistical methods for fitting power laws have been developed, but appropriate use of these techniques requires significant programming and statistical insight. In order to greatly decrease the barriers to using good statistical methods for fitting power law distributions, we developed the powerlaw Python package. This software package provides easy commands for basic fitting and statistical analysis of distributions. Notably, it also seeks to support a variety of user needs by being exhaustive in the options available to the user. The source code is publicly available and easily extensible.},
  number = {1},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2018-03-13},
  date = {2014-01-29},
  pages = {e85777},
  keywords = {Neurons,Probability distribution,Statistical distributions,Data visualization,Source code,Software tools,Semantics,Simulation and modeling},
  author = {Alstott, Jeff and Bullmore, Ed and Plenz, Dietmar},
  file = {/home/fh/lib/articles/Alstott22.pdf}
}

@article{Collberg2016,
  title = {Repeatability in {{Computer Systems Research}}},
  volume = {59},
  issn = {0001-0782},
  url = {http://doi.acm.org/10.1145/2812803},
  doi = {10.1145/2812803},
  abstract = {To encourage repeatable research, fund repeatability engineering and reward commitments to sharing research artifacts.},
  number = {3},
  journaltitle = {Commun. ACM},
  urldate = {2018-03-12},
  date = {2016-02},
  pages = {62--69},
  author = {Collberg, Christian and Proebsting, Todd A.},
  file = {/home/fh/lib/articles/Collberg2016.pdf}
}

@article{Collberg2015,
  title = {Repeatability and {{Benefaction}} in {{Computer Systems Research}}},
  abstract = {We describe a study into the extent to which Computer Systems researchers share their code and data and the extent to which such code builds. Starting with 601 papers from ACM conferences and journals, we examine 402 papers whose results were backed by code. For 32.3\% of these papers we were able to obtain the code and build it within 30 minutes; for 48.3\% of the papers we managed to build the code, but it may have required extra effort; for 54.0\% of the papers either we managed to build the code or the authors stated the code would build with reasonable effort. We also propose a novel sharing specification scheme that requires researchers to specify the level of sharing that reviewers and readers can assume from a paper.},
  date = {2015-02-27},
  author = {Collberg, Christian and Proebsting, Todd and Warren, Alex M},
  file = {/home/fh/lib/articles/Collberg2015.pdf}
}

@article{Collberg2013,
  title = {Measuring {{Reproducibility}} in {{Computer Systems Research}}},
  abstract = {We describe a study into the willingness of Computer Systems researchers to share their code and data. We find that . . . . We also propose a novel sharing specification scheme that will require researchers to specify the level of reproducibility that reviewers and readers can assume from a paper either submitted for publication, or published.},
  date = {2013-12-10},
  author = {Collberg, Christian and Proebsting, Todd and Moraila, Gina and Shankaran, Akash and Zuoming, Shi and Warren, Alex M},
  file = {/home/fh/lib/articles/Collberg2013.pdf}
}

@article{Balleine2010,
  langid = {english},
  title = {Human and {{Rodent Homologies}} in {{Action Control}}: {{Corticostriatal Determinants}} of {{Goal}}-{{Directed}} and {{Habitual Action}}},
  volume = {35},
  issn = {1740-634X},
  url = {https://www.nature.com/articles/npp2009131},
  doi = {10.1038/npp.2009.131},
  shorttitle = {Human and {{Rodent Homologies}} in {{Action Control}}},
  abstract = {Recent behavioral studies in both humans and rodents have found evidence that performance in decision-making tasks depends on two different learning processes; one encoding the relationship between actions and their consequences and a second involving the formation of stimulus\textendash{}response associations. These learning processes are thought to govern goal-directed and habitual actions, respectively, and have been found to depend on homologous corticostriatal networks in these species. Thus, recent research using comparable behavioral tasks in both humans and rats has implicated homologous regions of cortex (medial prefrontal cortex/medial orbital cortex in humans and prelimbic cortex in rats) and of dorsal striatum (anterior caudate in humans and dorsomedial striatum in rats) in goal-directed action and in the control of habitual actions (posterior lateral putamen in humans and dorsolateral striatum in rats). These learning processes have been argued to be antagonistic or competing because their control over performance appears to be all or none. Nevertheless, evidence has started to accumulate suggesting that they may at times compete and at others cooperate in the selection and subsequent evaluation of actions necessary for normal choice performance. It appears likely that cooperation or competition between these sources of action control depends not only on local interactions in dorsal striatum but also on the cortico-basal ganglia network within which the striatum is embedded and that mediates the integration of learning with basic motivational and emotional processes. The neural basis of the integration of learning and motivation in choice and decision-making is still controversial and we review some recent hypotheses relating to this issue.},
  number = {1},
  journaltitle = {Neuropsychopharmacology},
  urldate = {2018-03-11},
  date = {2010-01},
  pages = {48--69},
  author = {Balleine, Bernard W. and O'Doherty, John P.},
  file = {/home/fh/lib/articles/Balleine2010.pdf}
}

@article{Eglen2016,
  langid = {english},
  title = {Bivariate Spatial Point Patterns in the Retina: A Reproducible Review},
  url = {https://www.repository.cam.ac.uk/handle/1810/254988},
  shorttitle = {Bivariate Spatial Point Patterns in the Retina},
  abstract = {In this article I present a reproducible review of recent research to investigate the spatial positioning of neurons in the nervous system. In particular, I focus on the relative spatial positioning of pairs of cell types within the retina. I examine three different cases by which two types of neurons might be arranged relative to each other. (1) Cells of different type might be effectively independent of each other. (2) Cells of one type are randomly assigned one of two labels to create two related populations. (3) Interactions between cells of different type generate functional dependencies. I show briefly how spatial statistic techniques can be applied to investigate the nature of spatial interactions between two cell types. Finally, I have termed this article a `reproducible review' because all the data and computer code are integrated into the manuscript so that others can repeat the analysis presented here. I close the review with a discussion of this concept.},
  urldate = {2018-03-10},
  date = {2016-04-08},
  author = {Eglen, Stephen J.},
  file = {/home/fh/lib/articles/Eglen2016.pdf}
}

@article{Mesnard2017,
  title = {Reproducible and {{Replicable Computational Fluid Dynamics}}: {{It}}'s {{Harder Than You Think}}},
  volume = {19},
  issn = {1521-9615},
  url = {https://aip.scitation.org/doi/abs/10.1109/MCSE.2017.3151254},
  doi = {10.1109/MCSE.2017.3151254},
  shorttitle = {Reproducible and {{Replicable Computational Fluid Dynamics}}},
  number = {4},
  journaltitle = {Computing in Science \& Engineering},
  shortjournal = {Computing in Science \& Engineering},
  urldate = {2018-03-09},
  date = {2017-07-01},
  pages = {44--55},
  author = {Mesnard, Olivier and Barba, Lorena A.},
  file = {/home/fh/lib/articles/Mesnard2017.pdf}
}

@article{Vicente-Saez2018,
  title = {Open {{Science}} Now: {{A}} Systematic Literature Review for an Integrated Definition},
  issn = {0148-2963},
  url = {https://www.sciencedirect.com/science/article/pii/S0148296317305441},
  doi = {10.1016/j.jbusres.2017.12.043},
  shorttitle = {Open {{Science}} Now},
  abstract = {Open Science is a disruptive phenomenon that is emerging around the world and especially in Europe. Open Science brings about socio-cultural and technological change, based on openness and connectivity, on how research is designed, performed, captured, and assessed. Several studies show that there is a lack of awareness about what Open Science is, mainly due to the fact that there is no formal definition of Open Science. The purpose of this paper is to build a rigorous, integrated, and up-to-date definition of the Open Science phenomenon through a systematic literature review. The resulting definition ``Open Science is transparent and accessible knowledge that is shared and developed through collaborative networks'' helps the scientific community, the business world, political actors, and citizens to have a common and clear understanding about what Open Science is, and stimulates an open debate about the social, economic, and human added value of this phenomenon.},
  journaltitle = {Journal of Business Research},
  shortjournal = {Journal of Business Research},
  urldate = {2018-01-22},
  date = {2018},
  keywords = {Definition,Open access,Open innovation,Open science,Research and innovation management,Responsible research and innovation},
  author = {Vicente-Saez, Ruben and Martinez-Fuentes, Clara},
  file = {/home/fh/lib/articles/Vicente-Saez2018.pdf}
}

@article{Benureau2018,
  title = {Re-Run, {{Repeat}}, {{Reproduce}}, {{Reuse}}, {{Replicate}}: {{Transforming Code}} into {{Scientific Contributions}}},
  volume = {11},
  issn = {1662-5196},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5758530/},
  doi = {10.3389/fninf.2017.00069},
  shorttitle = {Re-Run, {{Repeat}}, {{Reproduce}}, {{Reuse}}, {{Replicate}}},
  abstract = {Scientific code is different from production software. Scientific code, by producing results that are then analyzed and interpreted, participates in the elaboration of scientific conclusions. This imposes specific constraints on the code that are often overlooked in practice. We articulate, with a small example, five characteristics that a scientific code in computational science should possess: re-runnable, repeatable, reproducible, reusable, and replicable. The code should be executable (re-runnable) and produce the same result more than once (repeatable); it should allow an investigator to reobtain the published results (reproducible) while being easy to use, understand and modify (reusable), and it should act as an available reference for any ambiguity in the algorithmic descriptions of the article (replicable).},
  journaltitle = {Frontiers in Neuroinformatics},
  shortjournal = {Front Neuroinform},
  urldate = {2018-03-09},
  date = {2018-01-04},
  author = {Benureau, Fabien C. Y. and Rougier, Nicolas P.},
  file = {/home/fh/lib/articles/Benureau2018.pdf}
}

@article{Mikaitis2018,
  langid = {english},
  title = {Neuromodulated {{Synaptic Plasticity}} on the {{SpiNNaker Neuromorphic System}}},
  volume = {12},
  issn = {1662-453X},
  url = {https://www.frontiersin.org/articles/10.3389/fnins.2018.00105/full},
  doi = {10.3389/fnins.2018.00105},
  abstract = {SpiNNaker is a digital neuromorphic architecture, designed specifically for the low power simulation of large-scale spiking neural networks at speeds close to biological real-time. Unlike other neuromorphic systems, SpiNNaker allows users to develop their own neuron and synapse models as well as specify arbitrary connectivity. As a result SpiNNaker has proved to be a powerful tool for studying different neuron models as well as synaptic plasticity -- believed to be one of the main mechanisms behind learning and memory in the brain. A number of Spike-Timing-Dependent-Plasticity(STDP) rules have already been implemented on SpiNNaker and have been shown to be capable of solving various learning tasks in real-time. However, while STDP is an important biological theory of learning, it is a form of Hebbian or unsupervised learning and therefore does not explain behaviours that depend on feedback from the environement. Instead, learning rules based on neuromodulated STDP (three-factor learning rules) have been shown to be capable of solving reinforcement learning tasks in a biologically plausible manner. In this paper we demonstrate for the first time how a model of three-factor STDP, with the third-factor representing spikes from dopaminergic neurons, can be implemented on the SpiNNaker neuromorphic system. Using this learning rule we first show how reward and punishment signals can be delivered to a single synapse before going on to demonstrate it in a larger network which solves the credit assignment problem in a Pavlovian conditioning experiment. Because of its extra complexity, we find that our three-factor learning rule requires approximately 2x as much processing time as the existing SpiNNaker STDP learning rules. However, we show that it is still possible to run our Pavlovian conditioning model with up to 10000 neurons in real-time, opening up new research opportunities for modelling behavioural learning on SpiNNaker.},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front. Neurosci.},
  urldate = {2018-03-09},
  date = {2018},
  keywords = {STDP,reinforcement learning,behavioural learning,Neuromodulation,SpiNNaker,three-factor learning rules},
  author = {Mikaitis, Mantas and Pineda Garc{\'\i}a, Garibaldi and Knight, James C. and Furber, Steve B.},
  file = {/home/fh/lib/articles/Mikaitis2018.pdf}
}

@article{Bannach-Brown2018,
  langid = {english},
  title = {The Use of Text-Mining and Machine Learning Algorithms in Systematic Reviews: Reducing Workload in Preclinical Biomedical Sciences and Reducing Human Screening Error},
  url = {https://www.biorxiv.org/content/early/2018/01/31/255760},
  doi = {10.1101/255760},
  shorttitle = {The Use of Text-Mining and Machine Learning Algorithms in Systematic Reviews},
  abstract = {Background: In this paper we outline a method of applying machine learning (ML) algorithms to aid citation screening in an on-going broad and shallow systematic review, with the aim of achieving a high performing algorithm comparable to human screening. Methods: We tested a range of machine learning algorithms. We applied ML algorithms to incremental numbers of training records and recorded the performance on sensitivity and specificity on an unseen validation set of papers. The performance of these algorithms was assessed on measures of recall, specificity, and accuracy. The classification results of the best performing algorithm was taken forward and applied to the remaining unseen records in the dataset and will be taken forward to the next stage of systematic review. ML was used to identify potential human errors during screening by analysing the training and validation datasets against the machine-ranked score. Results: We found that ML algorithms perform at a desirable level. Classifiers reached 98.7\% sensitivity based on learning from a training set of 5749 records, with an inclusion prevalence of 13.2\%. The highest level of specificity reached was 86\%. Human errors in the training and validation set were successfully identified using ML scores to highlight discrepancies. Training the ML algorithm on the corrected dataset improved the specificity of the algorithm without compromising sensitivity. Error analysis sees a 3\% increase or change in sensitivity and specificity, which increases precision and accuracy of the ML algorithm. Conclusions: The technique of using ML to identify human error needs to be investigated in more depth, however this pilot shows a promising approach to integrating human decisions and automation in systematic review methodology.},
  journaltitle = {bioRxiv},
  urldate = {2018-03-08},
  date = {2018-01-31},
  pages = {255760},
  author = {Bannach-Brown, Alexandra and Przyby{\l}a, Piotr and Thomas, James and Rice, Andrew S. C. and Ananiadou, Sophia and Liao, Jing and Macleod, Malcolm Robert},
  file = {/home/fh/lib/articles/Bannach-Brown2018.pdf}
}

@article{Gao2017,
  langid = {english},
  title = {A Theory of Multineuronal Dimensionality, Dynamics and Measurement},
  url = {https://www.biorxiv.org/content/early/2017/11/12/214262},
  doi = {10.1101/214262},
  abstract = {In many experiments, neuroscientists tightly control behavior, record many trials, and obtain trial-averaged firing rates from hundreds of neurons in circuits containing billions of behaviorally relevant neurons. Dimensionality reduction methods reveal a striking simplicity underlying such multi-neuronal data: they can be reduced to a low-dimensional space, and the resulting neural trajectories in this space yield a remarkably insightful dynamical portrait of circuit computation. This simplicity raises profound and timely conceptual questions. What are its origins and its implications for the complexity of neural dynamics? How would the situation change if we recorded more neurons? When, if at all, can we trust dynamical portraits obtained from measuring an infinitesimal fraction of task relevant neurons? We present a theory that answers these questions, and test it using physiological recordings from reaching monkeys. This theory reveals conceptual insights into how task complexity governs both neural dimensionality and accurate recovery of dynamic portraits, thereby providing quantitative guidelines for future large-scale experimental design.},
  journaltitle = {bioRxiv},
  urldate = {2018-03-05},
  date = {2017-11-12},
  pages = {214262},
  author = {Gao, Peiran and Trautmann, Eric and Yu, Byron M. and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna and Ganguli, Surya},
  file = {/home/fh/lib/articles/Gao2017.pdf}
}

@article{Benureau2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.08205},
  primaryClass = {cs},
  title = {Re-Run, {{Repeat}}, {{Reproduce}}, {{Reuse}}, {{Replicate}}: {{Transforming Code}} into {{Scientific Contributions}}},
  url = {http://arxiv.org/abs/1708.08205},
  shorttitle = {Re-Run, {{Repeat}}, {{Reproduce}}, {{Reuse}}, {{Replicate}}},
  abstract = {Scientific code is not production software. Scientific code participates in the evaluation of a scientific hypothesis. This imposes specific constraints on the code that are often overlooked in practice. We articulate, with a small example, five characteristics that a scientific code in computational science should possess: re-runnable, repeatable, reproducible, reusable and replicable.},
  urldate = {2018-03-05},
  date = {2017-08-28},
  keywords = {Computer Science - Computers and Society,Computer Science - General Literature},
  author = {Benureau, Fabien and Rougier, Nicolas},
  file = {/home/fh/lib/articles/Benureau2017.pdf}
}

@article{Berkowitz2018,
  langid = {english},
  title = {Decoding Neural Responses with Minimal Information Loss},
  url = {https://www.biorxiv.org/content/early/2018/02/28/273854},
  doi = {10.1101/273854},
  abstract = {Cortical tissue has a circuit motif termed the cortical column, which is thought to represent its basic computational unit but whose function remains unclear. Here we propose, and show quantitative evidence, that the cortical column performs computations necessary to decode neural activity with minimal information loss. The cortical decoder achieves higher accuracy compared to simpler decoders found in invertebrate and subcortical circuits by incorporating specific recurrent network dynamics. This recurrent dynamics also makes it possible to choose between alternative stimulus categories. The structure of cortical decoder predicts quadratic dependence of cortex size relative to subcortical parts of the brain. We quantitatively verify this relationship using anatomical data across mammals. The results offer a new perspective on the evolution and computational function of cortical columns.},
  journaltitle = {bioRxiv},
  urldate = {2018-03-01},
  date = {2018-02-28},
  pages = {273854},
  author = {Berkowitz, John and Sharpee, Tatyana},
  file = {/home/fh/lib/articles/Berkowitz2018.pdf}
}

@article{Marwick2017,
  langid = {english},
  title = {Computational {{Reproducibility}} in {{Archaeological Research}}: {{Basic Principles}} and a {{Case Study}} of {{Their Implementation}}},
  volume = {24},
  issn = {1072-5369, 1573-7764},
  url = {https://link.springer.com/article/10.1007/s10816-015-9272-9},
  doi = {10.1007/s10816-015-9272-9},
  shorttitle = {Computational {{Reproducibility}} in {{Archaeological Research}}},
  abstract = {The use of computers and complex software is pervasive in archaeology, yet their role in the analytical pipeline is rarely exposed for other researchers to inspect or reuse. This limits the progress of archaeology because researchers cannot easily reproduce each other's work to verify or extend it. Four general principles of reproducible research that have emerged in other fields are presented. An archaeological case study is described that shows how each principle can be implemented using freely available software. The costs and benefits of implementing reproducible research are assessed. The primary benefit, of sharing data in particular, is increased impact via an increased number of citations. The primary cost is the additional time required to enhance reproducibility, although the exact amount is difficult to quantify.},
  number = {2},
  journaltitle = {Journal of Archaeological Method and Theory},
  shortjournal = {J Archaeol Method Theory},
  urldate = {2018-03-01},
  date = {2017-06-01},
  pages = {424--450},
  author = {Marwick, Ben},
  file = {/home/fh/lib/articles/Marwick2017.pdf}
}

@article{Tannenbaum2017,
  title = {Theory of Nonstationary {{Hawkes}} Processes},
  volume = {96},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.96.062314},
  doi = {10.1103/PhysRevE.96.062314},
  abstract = {We expand the theory of Hawkes processes to the nonstationary case, in which the mutually exciting point processes receive time-dependent inputs. We derive an analytical expression for the time-dependent correlations, which can be applied to networks with arbitrary connectivity, and inputs with arbitrary statistics. The expression shows how the network correlations are determined by the interplay between the network topology, the transfer functions relating units within the network, and the pattern and statistics of the external inputs. We illustrate the correlation structure using several examples in which neural network dynamics are modeled as a Hawkes process. In particular, we focus on the interplay between internally and externally generated oscillations and their signatures in the spike and rate correlation functions.},
  number = {6},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  urldate = {2018-02-28},
  date = {2017-12-26},
  pages = {062314},
  author = {Tannenbaum, Neta Ravid and Burak, Yoram},
  file = {/home/fh/lib/articles/Tannenbaum2017.pdf}
}

@article{Kaiser2011a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1105.4705},
  title = {A {{Tutorial}} in {{Connectome Analysis}}: {{Topological}} and {{Spatial Features}} of {{Brain Networks}}},
  volume = {57},
  issn = {10538119},
  url = {http://arxiv.org/abs/1105.4705},
  doi = {10.1016/j.neuroimage.2011.05.025},
  shorttitle = {A {{Tutorial}} in {{Connectome Analysis}}},
  abstract = {High-throughput methods for yielding the set of connections in a neural system, the connectome, are now being developed. This tutorial describes ways to analyze the topological and spatial organization of the connectome at the macroscopic level of connectivity between brain regions as well as the microscopic level of connectivity between neurons. We will describe topological features at three different levels: the local scale of individual nodes, the regional scale of sets of nodes, and the global scale of the complete set of nodes in a network. Such features can be used to characterize components of a network and to compare different networks, e.g. the connectome of patients and control subjects for clinical studies. At the global scale, different types of networks can be distinguished and we will describe Erd$\backslash$"os-R$\backslash$'enyi random, scale-free, small-world, modular, and hierarchical archetypes of networks. Finally, the connectome also has a spatial organization and we describe methods for analyzing wiring lengths of neural systems. As an introduction for new researchers in the field of connectome analysis, we discuss the benefits and limitations of each analysis approach.},
  number = {3},
  journaltitle = {NeuroImage},
  urldate = {2018-02-28},
  date = {2011-08},
  pages = {892--907},
  keywords = {Quantitative Biology - Neurons and Cognition,Computer Science - Social and Information Networks,Physics - Physics and Society},
  author = {Kaiser, Marcus},
  file = {/home/fh/lib/articles/Kaiser2011.pdf}
}

@article{Braganza2018,
  langid = {english},
  title = {The {{Circuit Motif}} as a {{Conceptual Tool}} for {{Multilevel Neuroscience}}},
  volume = {41},
  issn = {0166-2236, 1878-108X},
  url = {http://www.cell.com/trends/neurosciences/abstract/S0166-2236(18)30002-X},
  doi = {10.1016/j.tins.2018.01.002},
  number = {3},
  journaltitle = {Trends in Neurosciences},
  shortjournal = {Trends in Neurosciences},
  urldate = {2018-02-28},
  date = {2018-03-01},
  pages = {128--136},
  keywords = {inhibition,behavior,circuit motif,high-dimensional research,multilevel neuroscience,optogenetics},
  author = {Braganza, Oliver and Beck, Heinz},
  file = {/home/fh/lib/articles/Braganza2018.pdf},
  eprinttype = {pmid},
  eprint = {29397990}
}

@article{Rabinowitz2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.07740},
  primaryClass = {cs},
  title = {Machine {{Theory}} of {{Mind}}},
  url = {http://arxiv.org/abs/1802.07740},
  abstract = {Theory of mind (ToM; Premack \& Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test (Wimmer \& Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.},
  urldate = {2018-02-26},
  date = {2018-02-21},
  keywords = {Computer Science - Artificial Intelligence},
  author = {Rabinowitz, Neil C. and Perbet, Frank and Song, H. Francis and Zhang, Chiyuan and Eslami, S. M. Ali and Botvinick, Matthew},
  file = {/home/fh/lib/articles/Rabinowitz2018.pdf}
}

@article{Bon2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.08008},
  primaryClass = {cs},
  title = {Novel Processes and Metrics for a Scientific Evaluation Rooted in the Principles of Science - {{Version}} 1},
  url = {http://arxiv.org/abs/1701.08008},
  abstract = {Scientific evaluation is a determinant of how scientists, institutions and funders behave, and as such is a key element in the making of science. In this article, we propose an alternative to the current norm of evaluating research with journal rank. Following a well-defined notion of scientific value, we introduce qualitative processes that can also be quantified and give rise to meaningful and easy-to-use article-level metrics. In our approach, the goal of a scientist is transformed from convincing an editorial board through a vertical process to convincing peers through an horizontal one. We argue that such an evaluation system naturally provides the incentives and logic needed to constantly promote quality, reproducibility, openness and collaboration in science. The system is legally and technically feasible and can gradually lead to the self-organized reappropriation of the scientific process by the scholarly community and its institutions. We propose an implementation of our evaluation system with the platform "the Self-Journals of Science" (www.sjscience.org).},
  urldate = {2018-02-16},
  date = {2017-01-27},
  keywords = {Computer Science - Digital Libraries,68-02,H.3.7; H.5.3},
  author = {Bon, Micha{\"e}l and Taylor, Michael and McDowell, Gary S.},
  file = {/home/fh/lib/articles/Bon2017.pdf}
}

@article{Hutson2018,
  langid = {english},
  title = {Artificial Intelligence Faces Reproducibility Crisis},
  volume = {359},
  issn = {0036-8075, 1095-9203},
  url = {http://science.sciencemag.org/content/359/6377/725},
  doi = {10.1126/science.359.6377.725},
  abstract = {The booming field of artificial intelligence (AI) is grappling with a replication crisis, much like the ones that have afflicted psychology, medicine, and other fields over the past decade. Just because algorithms are based on code doesn't mean experiments are easily replicated. Far from it. Unpublished codes and a sensitivity to training conditions have made it difficult for AI researchers to reproduce many key results. That is leading to a new conscientiousness about research methods and publication protocols. Last week, at a meeting of the Association for the Advancement of Artificial Intelligence in New Orleans, Louisiana, reproducibility was on the agenda, with some teams diagnosing the problem\textemdash{}and one laying out tools to mitigate it.
Unpublished code and sensitivity to training conditions make many claims hard to verify.
Unpublished code and sensitivity to training conditions make many claims hard to verify.},
  number = {6377},
  journaltitle = {Science},
  urldate = {2018-02-15},
  date = {2018-02-16},
  pages = {725--726},
  author = {Hutson, Matthew},
  file = {/home/fh/lib/articles/Hutson2018.pdf}
}

@article{Ross-Hellauer2017a,
  langid = {english},
  title = {Survey on Open Peer Review: {{Attitudes}} and Experience amongst Editors, Authors and Reviewers},
  volume = {12},
  issn = {1932-6203},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0189311},
  doi = {10.1371/journal.pone.0189311},
  shorttitle = {Survey on Open Peer Review},
  abstract = {Open peer review (OPR) is a cornerstone of the emergent Open Science agenda. Yet to date no large-scale survey of attitudes towards OPR amongst academic editors, authors, reviewers and publishers has been undertaken. This paper presents the findings of an online survey, conducted for the OpenAIRE2020 project during September and October 2016, that sought to bridge this information gap in order to aid the development of appropriate OPR approaches by providing evidence about attitudes towards and levels of experience with OPR. The results of this cross-disciplinary survey, which received 3,062 full responses, show the majority (60.3\%) of respondents to be believe that OPR as a general concept should be mainstream scholarly practice (although attitudes to individual traits varied, and open identities peer review was not generally favoured). Respondents were also in favour of other areas of Open Science, like Open Access (88.2\%) and Open Data (80.3\%). Among respondents we observed high levels of experience with OPR, with three out of four (76.2\%) reporting having taken part in an OPR process as author, reviewer or editor. There were also high levels of support for most of the traits of OPR, particularly open interaction, open reports and final-version commenting. Respondents were against opening reviewer identities to authors, however, with more than half believing it would make peer review worse. Overall satisfaction with the peer review system used by scholarly journals seems to strongly vary across disciplines. Taken together, these findings are very encouraging for OPR's prospects for moving mainstream but indicate that due care must be taken to avoid a ``one-size fits all'' solution and to tailor such systems to differing (especially disciplinary) contexts. OPR is an evolving phenomenon and hence future studies are to be encouraged, especially to further explore differences between disciplines and monitor the evolution of attitudes.},
  number = {12},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2018-02-14},
  date = {2017-12-13},
  pages = {e0189311},
  keywords = {Open science,Agriculture,Ecology and environmental sciences,Open access publishing,Peer review,Scientific publishing,Social sciences,Surveys},
  author = {Ross-Hellauer, Tony and Deppe, Arvid and Schmidt, Birgit},
  file = {/home/fh/lib/articles/Ross-Hellauer22.pdf}
}

@article{Kaufman2012,
  langid = {english},
  title = {Long-Term {{Relationships}} between {{Cholinergic Tone}}, {{Synchronous Bursting}} and {{Synaptic Remodeling}}},
  volume = {7},
  issn = {1932-6203},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0040980},
  doi = {10.1371/journal.pone.0040980},
  abstract = {Cholinergic neuromodulation plays key roles in the regulation of neuronal excitability, network activity, arousal, and behavior. On longer time scales, cholinergic systems play essential roles in cortical development, maturation, and plasticity. Presumably, these processes are associated with substantial synaptic remodeling, yet to date, long-term relationships between cholinergic tone and synaptic remodeling remain largely unknown. Here we used automated microscopy combined with multielectrode array recordings to study long-term relationships between cholinergic tone, excitatory synapse remodeling, and network activity characteristics in networks of cortical neurons grown on multielectrode array substrates. Experimental elevations of cholinergic tone led to the abrupt suppression of episodic synchronous bursting activity (but not of general activity), followed by a gradual growth of excitatory synapses over hours. Subsequent blockage of cholinergic receptors led to an immediate restoration of synchronous bursting and the gradual reversal of synaptic growth. Neither synaptic growth nor downsizing was governed by multiplicative scaling rules. Instead, these occurred in a subset of synapses, irrespective of initial synaptic size. Synaptic growth seemed to depend on intrinsic network activity, but not on the degree to which bursting was suppressed. Intriguingly, sustained elevations of cholinergic tone were associated with a gradual recovery of synchronous bursting but not with a reversal of synaptic growth. These findings show that cholinergic tone can strongly affect synaptic remodeling and synchronous bursting activity, but do not support a strict coupling between the two. Finally, the reemergence of synchronous bursting in the presence of elevated cholinergic tone indicates that the capacity of cholinergic neuromodulation to indefinitely suppress synchronous bursting might be inherently limited.},
  number = {7},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2018-02-14},
  date = {2012-07-23},
  pages = {e40980},
  keywords = {Neurons,Action potentials,Synapses,Neuronal dendrites,Network analysis,Neural networks,Cholinergics,Sleep},
  author = {Kaufman, Maya and Corner, Michael A. and Ziv, Noam E.},
  file = {/home/fh/lib/articles/Kaufman2012.pdf}
}

@article{Sammons2018,
  langid = {english},
  title = {Size-{{Dependent Axonal Bouton Dynamics}} Following {{Visual Deprivation In Vivo}}},
  volume = {22},
  issn = {2211-1247},
  url = {http://www.cell.com/cell-reports/abstract/S2211-1247(17)31902-2},
  doi = {10.1016/j.celrep.2017.12.065},
  number = {3},
  journaltitle = {Cell Reports},
  shortjournal = {Cell Reports},
  urldate = {2018-02-03},
  date = {2018-01-16},
  pages = {576--584},
  keywords = {LTP,plasticity,visual cortex,network,homeostasis,axonal bouton,GCaMP,population coupling,presynaptic,sensory deprivation},
  author = {Sammons, Rosanna P. and Clopath, Claudia and Barnes, Samuel J.},
  file = {/home/fh/lib/articles/Sammons2018.pdf},
  eprinttype = {pmid},
  eprint = {29346758}
}

@article{Ziv2018,
  langid = {english},
  title = {Synaptic {{Tenacity}} or {{Lack Thereof}}: {{Spontaneous Remodeling}} of {{Synapses}}},
  volume = {41},
  issn = {0166-2236, 1878-108X},
  url = {http://www.cell.com/trends/neurosciences/abstract/S0166-2236(17)30237-0},
  doi = {10.1016/j.tins.2017.12.003},
  shorttitle = {Synaptic {{Tenacity}} or {{Lack Thereof}}},
  number = {2},
  journaltitle = {Trends in Neurosciences},
  shortjournal = {Trends in Neurosciences},
  urldate = {2018-02-03},
  date = {2018-02-01},
  pages = {89--99},
  keywords = {synaptic plasticity,stochastic processes,synaptic remodeling,synaptic tenacity},
  author = {Ziv, Noam E. and Brenner, Naama},
  file = {/home/fh/lib/articles/Ziv2018.pdf},
  eprinttype = {pmid},
  eprint = {29275902}
}

@article{Kappel2015,
  langid = {english},
  title = {Network {{Plasticity}} as {{Bayesian Inference}}},
  volume = {11},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004485},
  doi = {10.1371/journal.pcbi.1004485},
  abstract = {Author Summary Synaptic connectivity between neurons in the brain and the efficacies (``weights'') of these synaptic connections are thought to encode the long-term memory of an organism. But a closer look at their molecular implementation, as well as imaging experiments over longer periods of time, have shown that synaptic connections are subject to numerous stochastic processes. We propose that this seeming unreliability of synaptic connections is not a bug, but an important feature. It endows networks of neurons with an important experimentally observed but theoretically not understood capability: Automatic compensation for internal and external changes. This perspective of network plasticity requires a new conceptual and mathematical framework, which is provided by this article. Stochasticity of synapses is seen here not as noise of an inherently deterministic system, but as an inherent property, similarly as Brownian motion of particles in a physical system cannot be abstracted away if one wants to understand certain properties of a physical system. In fact, we find that this underlying stochasticity of synaptic connections enables a network of neurons to continuously try out new network configurations while maintaining its functionality.},
  number = {11},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2018-02-03},
  date = {2015-11-06},
  pages = {e1004485},
  keywords = {Neurons,Action potentials,Neuronal plasticity,Synapses,Neuronal tuning,Network analysis,Neural networks,Synaptic plasticity},
  author = {Kappel, David and Habenschuss, Stefan and Legenstein, Robert and Maass, Wolfgang},
  file = {/home/fh/lib/articles/Kappel2015.pdf}
}

@article{Frank2018,
  langid = {english},
  title = {Hotspots of Dendritic Spine Turnover Facilitate Clustered Spine Addition and Learning and Memory},
  volume = {9},
  issn = {2041-1723},
  url = {https://www.nature.com/articles/s41467-017-02751-2},
  doi = {10.1038/s41467-017-02751-2},
  abstract = {Structural remodeling of dendritic spines is thought to be a mechanism of memory storage. Here, the authors look at how spine turnover and clustering predict future learning and memory performance, and see that a genetically modified mouse with enhanced spine turnover has enhanced learning.$<$/p$>$},
  number = {1},
  journaltitle = {Nature Communications},
  urldate = {2018-01-31},
  date = {2018-01-29},
  pages = {422},
  author = {Frank, Adam C. and Huang, Shan and Zhou, Miou and Gdalyahu, Amos and Kastellakis, George and Silva, Tawnie K. and Lu, Elaine and Wen, Ximiao and Poirazi, Panayiota and Trachtenberg, Joshua T. and Silva, Alcino J.},
  file = {/home/fh/lib/articles/Frank2018.pdf}
}

@book{Minsky1969,
  title = {Perceptrons},
  publisher = {{MIT Press}},
  date = {1969},
  author = {Minsky, Marvin and Papert, Seymour},
  file = {/home/fh/lib/books/Minsky1969_Perceptrons.djvu}
}

@report{Novikoff1963,
  title = {On Convergence Proofs for Perceptrons},
  institution = {{STANFORD RESEARCH INST MENLO PARK CALIF}},
  date = {1963},
  author = {Novikoff, Albert BJ},
  file = {/home/fh/lib/undefined/Novikoff1963on_convergence_proofs_for_perceptrons.pdf}
}

@article{Block1962,
  title = {The {{Perceptron}}: {{A Model}} for {{Brain Functioning}}. {{I}}},
  volume = {34},
  url = {https://link.aps.org/doi/10.1103/RevModPhys.34.123},
  doi = {10.1103/RevModPhys.34.123},
  shorttitle = {The {{Perceptron}}},
  abstract = {DOI:https://doi.org/10.1103/RevModPhys.34.123},
  number = {1},
  journaltitle = {Reviews of Modern Physics},
  shortjournal = {Rev. Mod. Phys.},
  urldate = {2018-01-15},
  date = {1962-01-01},
  pages = {123--135},
  author = {Block, H. D.},
  file = {/home/fh/lib/articles/Block1962.pdf}
}

@article{Daw2011,
  title = {Model-{{Based Influences}} on {{Humans}}' {{Choices}} and {{Striatal Prediction Errors}}},
  volume = {69},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627311001255},
  doi = {10.1016/j.neuron.2011.02.027},
  abstract = {Summary
The mesostriatal dopamine system is prominently implicated in model-free reinforcement learning, with fMRI BOLD signals in ventral striatum notably covarying with model-free prediction errors. However, latent learning and devaluation studies show that behavior also shows hallmarks of model-based planning, and the interaction between model-based and model-free values, prediction errors, and preferences is underexplored. We designed a multistep decision task in which model-based and model-free influences on human choice behavior could be distinguished. By showing that choices reflected both influences we could then test the purity of the ventral striatal BOLD signal as a model-free report. Contrary to expectations, the signal reflected both model-free and model-based predictions in proportions matching those that best explained choice behavior. These results challenge the notion of a separate model-free learner and suggest a more integrated computational architecture for high-level human decision-making.},
  number = {6},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2018-01-12},
  date = {2011-03-24},
  pages = {1204--1215},
  author = {Daw, Nathaniel D. and Gershman, Samuel J. and Seymour, Ben and Dayan, Peter and Dolan, Raymond J.},
  file = {/home/fh/lib/articles/Daw2011.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/JVZCY25D/S0896627311001255.html}
}

@article{Dayan2014,
  langid = {english},
  title = {Model-Based and Model-Free {{Pavlovian}} Reward Learning: {{Revaluation}}, Revision, and Revelation},
  volume = {14},
  issn = {1530-7026, 1531-135X},
  url = {https://link.springer.com/article/10.3758/s13415-014-0277-8},
  doi = {10.3758/s13415-014-0277-8},
  shorttitle = {Model-Based and Model-Free {{Pavlovian}} Reward Learning},
  abstract = {Evidence supports at least two methods for learning about reward and punishment and making predictions for guiding actions. One method, called model-free, progressively acquires cached estimates of the long-run values of circumstances and actions from retrospective experience. The other method, called model-based, uses representations of the environment, expectations, and prospective calculations to make cognitive predictions of future value. Extensive attention has been paid to both methods in computational analyses of instrumental learning. By contrast, although a full computational analysis has been lacking, Pavlovian learning and prediction has typically been presumed to be solely model-free. Here, we revise that presumption and review compelling evidence from Pavlovian revaluation experiments showing that Pavlovian predictions can involve their own form of model-based evaluation. In model-based Pavlovian evaluation, prevailing states of the body and brain influence value computations, and thereby produce powerful incentive motivations that can sometimes be quite new. We consider the consequences of this revised Pavlovian view for the computational landscape of prediction, response, and choice. We also revisit differences between Pavlovian and instrumental learning in the control of incentive motivation.},
  number = {2},
  journaltitle = {Cognitive, Affective, \& Behavioral Neuroscience},
  shortjournal = {Cogn Affect Behav Neurosci},
  urldate = {2018-01-12},
  date = {2014-06-01},
  pages = {473--492},
  author = {Dayan, Peter and Berridge, Kent C.},
  file = {/home/fh/lib/articles/Dayan2014.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/XZCVUBW7/10.html}
}

@article{Dayan2008,
  title = {Reinforcement Learning: {{The Good}}, {{The Bad}} and {{The Ugly}}},
  volume = {18},
  issn = {0959-4388},
  url = {http://www.sciencedirect.com/science/article/pii/S0959438808000767},
  doi = {10.1016/j.conb.2008.08.003},
  shorttitle = {Reinforcement Learning},
  abstract = {Reinforcement learning provides both qualitative and quantitative frameworks for understanding and modeling adaptive decision-making in the face of rewards and punishments. Here we review the latest dispatches from the forefront of this field, and map out some of the territories where lie monsters.},
  number = {2},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  series = {Cognitive neuroscience},
  urldate = {2018-01-12},
  date = {2008-04-01},
  pages = {185--196},
  author = {Dayan, Peter and Niv, Yael},
  file = {/home/fh/lib/articles/Dayan2008.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/Y2ZAMNKF/S0959438808000767.html}
}

@article{Chalk2018,
  langid = {english},
  title = {Toward a Unified Theory of Efficient, Predictive, and Sparse Coding},
  volume = {115},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1711114115},
  doi = {10.1073/pnas.1711114115},
  number = {1},
  journaltitle = {Proceedings of the National Academy of Sciences},
  urldate = {2018-01-10},
  date = {2018-01-02},
  pages = {186--191},
  author = {Chalk, Matthew and Marre, Olivier and Tka{\v c}ik, Ga{\v s}per},
  file = {/home/fh/lib/articles/Chalk2018.pdf}
}

@article{Knott2006,
  langid = {english},
  title = {Spine Growth Precedes Synapse Formation in the Adult Neocortex in Vivo},
  volume = {9},
  issn = {1546-1726},
  url = {https://www.nature.com/articles/nn1747},
  doi = {10.1038/nn1747},
  abstract = {Spine growth precedes synapse formation in the adult neocortex \emph{in vivo}},
  number = {9},
  journaltitle = {Nature Neuroscience},
  urldate = {2018-01-10},
  date = {2006-09},
  pages = {1117},
  author = {Knott, Graham W. and Holtmaat, Anthony and Wilbrecht, Linda and Welker, Egbert and Svoboda, Karel},
  file = {/home/fh/lib/articles/Knott2006.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/2KU5A59B/nn1747.html}
}

@article{Harris1989,
  langid = {english},
  title = {Dendritic Spines of {{CA}} 1 Pyramidal Cells in the Rat Hippocampus: Serial Electron Microscopy with Reference to Their Biophysical Characteristics},
  volume = {9},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/9/8/2982},
  shorttitle = {Dendritic Spines of {{CA}} 1 Pyramidal Cells in the Rat Hippocampus},
  abstract = {Serial electron microscopy and 3-D reconstructions of dendritic spines from hippocampal area CA 1 dendrites were obtained to evaluate 2 questions about relationships between spine geometry and synaptic efficacy. First, under what biophysical conditions are the spine necks likely to reduce the magnitude of charge transferred from the synapses on the spine heads to the recipient dendrite? Simulation software provided by Charles Wilson (1984) was used to determine that if synaptic conductance is 1 nS or less, only 1\% of the hippocampal spine necks are sufficiently thin and long to reduce charge transfer by more than 10\%. If synaptic conductance approaches 5 nS, however, 33\% of the hippocampal spine necks are sufficiently thin and long to reduce charge transfer by more than 10\%. Second, is spine geometry associated with other anatomical indicators of synaptic efficacy, including the area of the postsynaptic density and the number of vesicles in the presynaptic axon? Reconstructed spines were graphically edited into head and neck compartments, and their dimensions were measured, the areas of the postsynaptic densities (PSD) were measured, and all of the vesicles in the presynaptic axonal varicosities were counted. The dimensions of the spine head were well correlated with the area of PSD and the number of vesicles in the presynaptic axonal varicosity. Spine neck diameter and length were not correlated with PSD area, head volume, or the number of vesicles. These results suggest that the dimensions of the spine head, but not the spine neck, reflect differences in synaptic efficacy. We suggest that the constricted necks of hippocampal dendritic spines might reduce diffusion of activated molecules to neighboring synapses, thereby attributing specificity to activated or potentiated synapses.},
  number = {8},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2018-01-10},
  date = {1989-08-01},
  pages = {2982--2997},
  author = {Harris, K. M. and Stevens, J. K.},
  file = {/home/fh/lib/articles/Harris1989.pdf;/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/RN7UAMPK/2982.html},
  eprinttype = {pmid},
  eprint = {2769375}
}

@article{Marcus2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.00631},
  primaryClass = {cs, stat},
  title = {Deep {{Learning}}: {{A Critical Appraisal}}},
  url = {http://arxiv.org/abs/1801.00631},
  shorttitle = {Deep {{Learning}}},
  abstract = {Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.},
  urldate = {2018-01-09},
  date = {2018-01-02},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,97R40,I.2.0,I.2.6},
  author = {Marcus, Gary},
  file = {/home/fh/lib/articles/Marcus2018.pdf}
}

@book{Abu-Mostafa2012,
  langid = {english},
  location = {{S.l.}},
  title = {Learning from Data: A Short Course},
  isbn = {978-1-60049-006-4},
  shorttitle = {Learning from Data},
  pagetotal = {201},
  publisher = {{AMLbook.com}},
  date = {2012},
  author = {Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik and Lin, Hsuan-Tien},
  file = {/home/fh/lib/books/Abu-Mostafa2012_Learning-from-data-a-short-course.pdf},
  note = {OCLC: 808441289}
}

@article{Duarte2017,
  langid = {english},
  title = {Leveraging Heterogeneity for Neural Computation with Fading Memory in Layer 2/3 Cortical Microcircuits},
  url = {https://www.biorxiv.org/content/early/2017/12/11/230821},
  doi = {10.1101/230821},
  abstract = {Complexity and heterogeneity are intrinsic to neurobiological systems, manifest in every process, at every scale, and are inextricably linked to the systems' emergent collective behaviours and function. However, the majority of studies addressing the dynamics and computational properties of biologically inspired cortical microcircuits tend to assume (often for the sake of analytical tractability) a great degree of homogeneity in both neuronal and synaptic/connectivity parameters. While simplification and reductionism are necessary to understand the brain's functional principles, disregarding the existence of the multiple heterogeneities in the cortical composition, which may be at the core of its computational proficiency, will inevitably fail to account for important phenomena and limit the scope and generalizability of cortical models. We address these issues by studying the individual and composite functional roles of heterogeneities in neuronal, synaptic and structural properties in a biophysically plausible layer 2/3 microcircuit model, built and constrained by multiple sources of empirical data. This approach was made possible by the emergence of large-scale, well curated databases, as well as the substantial improvements in experimental methodologies achieved over the last few years. Our results show that variability in single neuron parameters is the dominant source of functional specialization, leading to highly proficient microcircuits with much higher computational power than their homogeneous counterparts. We further show that fully heterogeneous circuits, which are closest to the biophysical reality, owe their response properties to the differential contribution of different sources of heterogeneity.},
  journaltitle = {bioRxiv},
  urldate = {2017-12-26},
  date = {2017-12-11},
  pages = {230821},
  author = {Duarte, Renato and Morrison, Abigail},
  file = {/home/fh/lib/articles/Duarte2017.pdf}
}

@book{Chernick2008,
  location = {{Hoboken, N.J}},
  title = {Bootstrap Methods: A Guide for Practitioners and Researchers},
  edition = {2nd ed},
  isbn = {978-0-471-75621-7},
  shorttitle = {Bootstrap Methods},
  pagetotal = {369},
  series = {Wiley series in probability and statistics},
  publisher = {{Wiley-Interscience}},
  date = {2008},
  keywords = {Bootstrap (Statistics)},
  author = {Chernick, Michael R.},
  file = {/home/fh/lib/books/Chernick2008_Bootstrap-methods-a-guide-for-practitioners-and-researchers.pdf},
  note = {OCLC: ocn156785095}
}

@book{Diggle2011,
  langid = {english},
  location = {{Oxford}},
  title = {Statistics and Scientific Method: An Introduction for Students and Researchers},
  isbn = {978-0-19-954318-2 978-0-19-954319-9},
  shorttitle = {Statistics and Scientific Method},
  abstract = {"Most introductory statistics text-books are written either in a highly mathematical style for an intended readership of mathematics undergraduate students, or in a recipe-book style for an intended audience of non-mathematically inclined undergraduate or postgraduate students, typically in a single discipline; hence, "statistics for biologists", "statistics for psychologists", and so on. An antidote to technique-oriented service courses, Statistics and Scientific Method is different. It studiously avoids the recipe-book style and keeps algebraic details of specific statistical methods to the minimum extent necessary to understand the underlying concepts. Instead, the text aims to give the reader a clear understanding of how core statistical ideas of experimental design, modelling and data analysis are integral to the scientific method. Aimed primarily at beginning postgraduate students across a range of scientific disciplines (albeit with a bias towards the biological, environmental and health sciences), it therefore assumes some maturity of understanding of scientific method, but does not require any prior knowledge of statistics, or any mathematical knowledge beyond basic algebra and a willingness to come to terms with mathematical notation. Any statistical analysis of a realistically sized data-set requires the use of specially written computer software. An Appendix introduces the reader to our open-source software of choice, R, whilst the book's web-page includes downloadable data and R code that enables the reader to reproduce all of the analyses in the book and, with easy modifications, to adapt the code to analyse their own data if they wish. However, the book is not intended to be a textbook on statistical computing, and all of the material in the book can be understood without using either R or any other computer software"--},
  pagetotal = {172},
  publisher = {{Oxford Univ. Press}},
  date = {2011},
  author = {Diggle, Peter and Chetwynd, Amanda},
  file = {/home/fh/lib/books/Diggle2011_Statistics-and-scientific-method-an-introduction-for-students-and-researchers.pdf},
  note = {OCLC: 753366927}
}

@book{Karris2007,
  langid = {english},
  location = {{Fremont, CA}},
  title = {Mathematics for Business, Science, and Technology: With {{MATLAB}} and Spreadsheet Applications},
  isbn = {978-1-934404-01-0},
  shorttitle = {Mathematics for Business, Science, and Technology},
  publisher = {{Orchard Publications}},
  date = {2007},
  author = {Karris, Steven T},
  file = {/home/fh/lib/books/Karris2007_Mathematics-for-business,-science,-and-technology-with-MATLAB-and-spreadsheet-applications.pdf},
  note = {OCLC: 896815975}
}

@article{Mengiste2017,
  langid = {english},
  title = {Computational {{Approaches}} to the {{Degeneration}} of {{Brain Networks}} and {{Other Complex Networks}}},
  url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-213729},
  abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 47 universities and research institutions.},
  journaltitle = {DIVA},
  urldate = {2017-12-26},
  date = {2017},
  author = {Mengiste, Simachew Abebe},
  file = {/home/fh/lib/articles/Mengiste2017.pdf}
}

@article{Cowan2012,
  title = {Nodal {{Dynamics}}, {{Not Degree Distributions}}, {{Determine}} the {{Structural Controllability}} of {{Complex Networks}}},
  volume = {7},
  issn = {1932-6203},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0038398},
  doi = {10.1371/journal.pone.0038398},
  abstract = {Structural controllability has been proposed as an analytical framework for making predictions regarding the control of complex networks across myriad disciplines in the physical and life sciences (Liu et al., Nature:473(7346):167\textendash{}173, 2011). Although the integration of control theory and network analysis is important, we argue that the application of the structural controllability framework to most if not all real-world networks leads to the conclusion that a single control input, applied to the power dominating set, is all that is needed for structural controllability. This result is consistent with the well-known fact that controllability and its dual observability are generic properties of systems. We argue that more important than issues of structural controllability are the questions of whether a system is almost uncontrollable, whether it is almost unobservable, and whether it possesses almost pole-zero cancellations.},
  number = {6},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2017-12-26},
  date = {2012-06-22},
  pages = {e38398},
  keywords = {Signal processing,Dynamical systems,Eigenvalues,Transfer functions,Control theory,Directed graphs,Engineering and technology,Food web structure},
  author = {Cowan, Noah J. and Chastain, Erick J. and Vilhena, Daril A. and Freudenberg, James S. and Bergstrom, Carl T.},
  file = {/home/fh/lib/articles/Cowan2012.pdf}
}

@article{Mulugeta2018,
  langid = {english},
  title = {Credibility, {{Replicability}}, and {{Reproducibility}} in {{Simulation}} for {{Biomedicine}} and {{Clinical Applications}} in {{Neuroscience}}},
  volume = {12},
  issn = {1662-5196},
  url = {https://www.frontiersin.org/articles/10.3389/fninf.2018.00018/abstract},
  doi = {10.3389/fninf.2018.00018},
  abstract = {Modeling and simulation in computational neuroscience is currently a research enterprise to better understand neural systems. It is not yet directly applicable to the problems of patients with brain disease. To be used for clinical applications, there must not only be considerable progress in the field but also a concerted effort to use best practices in order to demonstrate model credibility to regulatory bodies, to clinics and hospitals, to doctors, and to patients. In doing this for neuroscience, we can learn lessons from long-standing practices in other areas of simulation (aircraft, computer chips), from software engineering, and from other biomedical disciplines. In this manuscript, we introduce some basic concepts that will be important in the development of credible clinical neuroscience models: reproducibility and replicability; verification and validation; model configuration; and procedures and processes for credible mechanistic multiscale modeling. We also discuss how garnering strong community involvement can promote model credibility. Finally, in addition to direct usage with patients, we note the potential for simulation usage in the area of Simulation-Based Medical Education, an area which to date has been primarily reliant on physical models (mannequins) and scenario-based simulations rather than on numerical simulations.},
  journaltitle = {Frontiers in Neuroinformatics},
  shortjournal = {Front. Neuroinform.},
  urldate = {2018-04-04},
  date = {2018},
  keywords = {Neurology,computational neuroscience,computational model,Version control,clinical translation,computational modeling,context of interests,credibility,good practice,Intended use,mathematical modeling,mechanistic explanation,Mechanistic explanations,mechanistic modelling,mechanistic models,model sharing,model testing,model validation,Model verification,Modeling and simulations,multiscale modeling,Neurosurgery,Psychiatry,Reproducibility of Results,simulation models,Simulation-based medical education,uncertainty quantification,verification and validation},
  author = {Mulugeta, Lealem and Drach, Andrew and Erdemir, Ahmet and Hunt, C. Anthony and Horner, Marc and Ku, Joy P. and Myers, Jerry G. Jr and Vadigepalli, Rajanikanth and Lytton, William W.}
}

@article{Gerlai2018,
  title = {Reproducibility and Replicability in Zebrafish Behavioral Neuroscience Research},
  issn = {0091-3057},
  url = {http://www.sciencedirect.com/science/article/pii/S0091305717306081},
  doi = {10.1016/j.pbb.2018.02.005},
  abstract = {Reproducibility and replicability are fundamentally important aspects of the scientific method. From time to time the discussion about whether scientific findings are replicable enough flares up. In fact, some recent publications claim we are witnessing a replication crisis. This is a particularly important problem in laboratory organisms that are relatively new, i.e., for which only limited amount of information is available, and for which only a limited number of methods have been developed. The zebrafish is a relative newcomer in behavioral neuroscience. This review considers four distinct reasons as possibly underlying reproducibility issues in behavioral neuroscience studies using the zebrafish. One, publication bias for positive results. Two, statistical issues that surround the question of how to address type 1 and type 2 errors, and how to make statistical inference. Three, inappropriate control of factors that are known to potentially influence results. And four, methodological issues stemming from insufficient understanding of factors that may influence experimental results. The review will mainly focus on experimental issues and solutions, i.e. the latter two reasons listed above. It is not intended to be comprehensive, and its examples are drawn mainly from the author's own studies and experience with zebrafish. Nevertheless, most issues discussed are not unique to his laboratory, to the zebrafish, or even to behavioral neuroscience.},
  journaltitle = {Pharmacology Biochemistry and Behavior},
  shortjournal = {Pharmacology Biochemistry and Behavior},
  urldate = {2018-04-04},
  date = {2018-02-23},
  keywords = {Behavior,Brain function,Husbandry,Replicability,Reproducibility,Standardization,Zebrafish},
  author = {Gerlai, Robert},
  file = {/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/I4AXQFFA/S0091305717306081.html}
}

@article{Baker2016,
  langid = {english},
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  volume = {533},
  url = {http://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970},
  doi = {10.1038/533452a},
  abstract = {Survey sheds light on the `crisis' rocking research.},
  number = {7604},
  journaltitle = {Nature News},
  urldate = {2018-04-04},
  date = {2016-05-26},
  pages = {452},
  author = {Baker, Monya},
  file = {/home/fh/.mozilla/firefox/lbe84bnd.default/zotero/storage/67B4773Q/Baker - 2016 - 1,500 scientists lift the lid on reproducibility.pdf;/home/fh/lib/articles/Baker2016.pdf}
}


